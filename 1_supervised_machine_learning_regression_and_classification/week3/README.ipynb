{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c481a6b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-with-logistic-regression\" data-toc-modified-id=\"Classification-with-logistic-regression-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Classification with logistic regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Motivations\" data-toc-modified-id=\"Motivations-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Motivations</a></span></li><li><span><a href=\"#Optional-lab:-Classification\" data-toc-modified-id=\"Optional-lab:-Classification-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Optional lab: Classification</a></span></li><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Logistic regression</a></span></li><li><span><a href=\"#Optional-lab:-Sigmoid-function-and-logistic-regression\" data-toc-modified-id=\"Optional-lab:-Sigmoid-function-and-logistic-regression-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Optional lab: Sigmoid function and logistic regression</a></span></li><li><span><a href=\"#Decision-boundary\" data-toc-modified-id=\"Decision-boundary-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>Decision boundary</a></span></li><li><span><a href=\"#Optional-lab:-Decision-boundary\" data-toc-modified-id=\"Optional-lab:-Decision-boundary-1.1.6\"><span class=\"toc-item-num\">1.1.6&nbsp;&nbsp;</span>Optional lab: Decision boundary</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Classification-with-logistic-regression\" data-toc-modified-id=\"Practice-quiz:-Classification-with-logistic-regression-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Practice quiz: Classification with logistic regression</a></span></li><li><span><a href=\"#Cost-function-for-logistic-regression\" data-toc-modified-id=\"Cost-function-for-logistic-regression-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Cost function for logistic regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cost-function-for-logistic-regression\" data-toc-modified-id=\"Cost-function-for-logistic-regression-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Cost function for logistic regression</a></span></li><li><span><a href=\"#Optional-lab:-Logistic-loss\" data-toc-modified-id=\"Optional-lab:-Logistic-loss-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Optional lab: Logistic loss</a></span></li><li><span><a href=\"#Simplified-Cost-Function-for-Logistic-Regression\" data-toc-modified-id=\"Simplified-Cost-Function-for-Logistic-Regression-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Simplified Cost Function for Logistic Regression</a></span></li><li><span><a href=\"#Optional-lab:-Cost-function-for-logistic-regression\" data-toc-modified-id=\"Optional-lab:-Cost-function-for-logistic-regression-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Optional lab: Cost function for logistic regression</a></span></li></ul></li><li><span><a href=\"#Practice-Quiz:-Cost-function-for-logistic-regression\" data-toc-modified-id=\"Practice-Quiz:-Cost-function-for-logistic-regression-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Practice Quiz: Cost function for logistic regression</a></span></li><li><span><a href=\"#Gradient-descent-for-logistic-regression\" data-toc-modified-id=\"Gradient-descent-for-logistic-regression-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Gradient descent for logistic regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gradient-Descent-Implementation\" data-toc-modified-id=\"Gradient-Descent-Implementation-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Gradient Descent Implementation</a></span></li><li><span><a href=\"#Optional-lab:-Gradient-descent-for-logistic-regression\" data-toc-modified-id=\"Optional-lab:-Gradient-descent-for-logistic-regression-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Optional lab: Gradient descent for logistic regression</a></span></li><li><span><a href=\"#Optional-lab:-Logistic-regression-with-scikit-learn\" data-toc-modified-id=\"Optional-lab:-Logistic-regression-with-scikit-learn-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Optional lab: Logistic regression with scikit-learn</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Gradient-descent-for-logistic-regression\" data-toc-modified-id=\"Practice-quiz:-Gradient-descent-for-logistic-regression-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Practice quiz: Gradient descent for logistic regression</a></span></li><li><span><a href=\"#The-problem-of-overfitting\" data-toc-modified-id=\"The-problem-of-overfitting-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>The problem of overfitting</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-problem-of-overfitting\" data-toc-modified-id=\"The-problem-of-overfitting-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>The problem of overfitting</a></span></li><li><span><a href=\"#Addressing-overfitting\" data-toc-modified-id=\"Addressing-overfitting-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>Addressing overfitting</a></span></li><li><span><a href=\"#Optional-lab:-Overfitting\" data-toc-modified-id=\"Optional-lab:-Overfitting-1.7.3\"><span class=\"toc-item-num\">1.7.3&nbsp;&nbsp;</span>Optional lab: Overfitting</a></span></li><li><span><a href=\"#Cost-function-with-regularization\" data-toc-modified-id=\"Cost-function-with-regularization-1.7.4\"><span class=\"toc-item-num\">1.7.4&nbsp;&nbsp;</span>Cost function with regularization</a></span></li><li><span><a href=\"#Regularized-linear-regression\" data-toc-modified-id=\"Regularized-linear-regression-1.7.5\"><span class=\"toc-item-num\">1.7.5&nbsp;&nbsp;</span>Regularized linear regression</a></span></li><li><span><a href=\"#Optional-lab:-Regularization\" data-toc-modified-id=\"Optional-lab:-Regularization-1.7.6\"><span class=\"toc-item-num\">1.7.6&nbsp;&nbsp;</span>Optional lab: Regularization</a></span></li></ul></li><li><span><a href=\"#Practice-Quiz:-The-problem-of-overfitting\" data-toc-modified-id=\"Practice-Quiz:-The-problem-of-overfitting-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Practice Quiz: The problem of overfitting</a></span></li><li><span><a href=\"#Week-3-practice-lab:-Logistic-regression\" data-toc-modified-id=\"Week-3-practice-lab:-Logistic-regression-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Week 3 practice lab: Logistic regression</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a56be1",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e09abb",
   "metadata": {},
   "source": [
    "## Classification with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb77326",
   "metadata": {},
   "source": [
    "### Motivations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad62ff00",
   "metadata": {},
   "source": [
    "In **classification**, our output variable $y$ can take on only one of a small handful of possible values instead of any number in an infinite range of numbers.\n",
    "- Classification problem with two possible classes or categories is called **binary classification**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e90d3",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/logistic_regression_motivation_1.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d6af4",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/logistic_regression_motivation_2.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "    <img src=\"attachments/logistic_regression_motivation_3.png\" alt=\"Drawing\" style=\"width: 50%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b65f6f",
   "metadata": {},
   "source": [
    "In the above tumor example, \n",
    "- class 1 (+) => Malignant\n",
    "- class 0 (-) => Non-malignant (Benign)\n",
    "\n",
    "For first example, we can try applying linear regression to fit straight line to the data. Linear regression predicts not just 0 and 1, but all numbers between 0 and 1, and outside 0 and 1. However, we want to predict categories. One way to convert the output of linear regression to categories is to choose a **threshold of 0.5** such that \n",
    "- if $f_{w,b}(x)=wx+b \\lt 0.5 \\rightarrow \\hat{y} = 0$\n",
    "- if $f_{w,b}(x)=wx+b \\ge 0.5 \\rightarrow \\hat{y} = 1$\n",
    "For this first example, it looks like linear regression could do something reasonable in predicting the two classes.\n",
    "\n",
    "However, in the second example, we add one more training example that looks more like an outlier. Ideally, the new training example should not change the way we classify the data points. <span style=\"color:red\">The best fit line now shifts over, and if we continue using the <b>threshold of 0.5</b>, the dividing line (a.k.a the <b>decision boundary</b>) shift over to the right, thus many malignant tumours are now misclassified as non-malginant as it now learns a much worse function.</span>\n",
    "\n",
    "> In a nutshell, the linear model is insufficient to model categorical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e774e",
   "metadata": {},
   "source": [
    "### Optional lab: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d071ea09",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Optional%20Labs/C1_W3_Lab01_Classification_Soln.ipynb) solutions and [my practice](greyhatguy007/Optional%20Labs/C1_W3_Lab01_Classification_Practice_TP.ipynb) for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd4aca",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b254f32",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/logistic_regression_1.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0a34e1",
   "metadata": {},
   "source": [
    "In previous example, we saw that linear regression is not a good algorithm for above problem.\n",
    "\n",
    "In contrast to fitting a straight line in linear regression, the logistic regression fits a *S*-shaped curve to the data.\n",
    "\n",
    "<span style=\"color:blue\">To build out to logistic regression algo, we need to learn about the <b><u>Sigmoid function (a.k.a. Logistic function).</u></b></span> The sigmoid function $g(z)$ offers outputs between $0$ and $1$.\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}} \\;\\;\\; 0<g(z)<1$$\n",
    "\n",
    "where $e$ is a mathematical constant with value of $2.71...$\n",
    "\n",
    "- If $z$ is very large +ve number (e.g., $z=100$), then $g(z) = \\frac{1}{1+e^{-100}} \\approx \\frac{1}{1+\\text{ very tiny number}} \\approx 1$\n",
    "- If $z$ is very large -ve number (e.g., $z=-100$), then $g(z) = \\frac{1}{1+e^{100}} \\approx \\frac{1}{1+\\text{ very large number}} \\approx 0$\n",
    "- If $z$ is zero (e.g., $z=0$), then $g(z) = \\frac{1}{1+e^{0}} = \\frac{1}{1+1} = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482a261",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/logistic_regression_2.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06abde92",
   "metadata": {},
   "source": [
    "For linear regression,\n",
    "$$f_{w,b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x} + b$$\n",
    "\n",
    "For logistic regression,\n",
    "$$f_{w,b}(\\vec{x}) = g(\\vec{w}\\cdot\\vec{x} + b) = g(z) = \\frac{1}{1+e^{-(\\vec{w}\\cdot\\vec{x}+b)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cf6caf",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/logistic_regression_3.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b7dbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T23:27:28.591464Z",
     "start_time": "2023-03-17T23:27:28.572360Z"
    }
   },
   "source": [
    "<span style=\"font-size:+22px\">$f_{w,b}(\\vec{x}) = \\frac{1}{1+e^{-(\\vec{w}\\cdot\\vec{x}+b)}}$</span> \n",
    "\n",
    "The output of logistic regression (above) can be interepreted as probability that the output belongs to class $1$ given input $x$.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Since $y$ has to be either $0$ or $1$,\n",
    "$$P(y=0) + P(y=1)=1$$\n",
    "\n",
    "For e.g., $P(y=1)=70\\%$, it implies $P(y=0)=30\\%$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "It can also be expressed as \n",
    "$$f_{w,b}(\\vec{x}) = P(y=1|{x};\\vec{w},b)$$\n",
    "which can be interpreted as probability of $y$ being equal to $1$ given the input feature $x$ with parameters $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8551e2",
   "metadata": {},
   "source": [
    "### Optional lab: Sigmoid function and logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895482fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T23:49:17.848757Z",
     "start_time": "2023-03-17T23:49:17.833037Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/lab_sigmoid_function_and_logistic_regression_1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "    <img src=\"attachments/lab_sigmoid_function_and_logistic_regression_2_edited.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8a0b05",
   "metadata": {},
   "source": [
    "Note:\n",
    "- In the above figure, the orange line is $z$ or $w\\cdot x^{(i)}+b$\n",
    "    - Unlike linear regression which would have wrongly classified two of malignant cases as benign in the second example, the logistic regression model correctly predicts the two classes despite having some extreme cases\n",
    "- In a nutshell, <span style=\"color:blue\"><b>the logistic regression model applies sigmoid to the output of linear regression model.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c1dcc1",
   "metadata": {},
   "source": [
    "### Decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ed768",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ee95c22",
   "metadata": {},
   "source": [
    "### Optional lab: Decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab4108",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8423c927",
   "metadata": {},
   "source": [
    "## Practice quiz: Classification with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0aea2",
   "metadata": {},
   "source": [
    "## Cost function for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd41b5",
   "metadata": {},
   "source": [
    "### Cost function for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e319226",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c11dce6c",
   "metadata": {},
   "source": [
    "### Optional lab: Logistic loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c926425a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c10c541d",
   "metadata": {},
   "source": [
    "### Simplified Cost Function for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af29b333",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01cdea9e",
   "metadata": {},
   "source": [
    "### Optional lab: Cost function for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbbff6d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c247d0cd",
   "metadata": {},
   "source": [
    "## Practice Quiz: Cost function for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf3c34",
   "metadata": {},
   "source": [
    "## Gradient descent for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e58198",
   "metadata": {},
   "source": [
    "### Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c231d3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5495181a",
   "metadata": {},
   "source": [
    "### Optional lab: Gradient descent for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0836b45",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a54b246",
   "metadata": {},
   "source": [
    "### Optional lab: Logistic regression with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6032e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aedcfeb0",
   "metadata": {},
   "source": [
    "## Practice quiz: Gradient descent for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b096fe7",
   "metadata": {},
   "source": [
    "## The problem of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce2d4d",
   "metadata": {},
   "source": [
    "### The problem of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84a740",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e84e200",
   "metadata": {},
   "source": [
    "### Addressing overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f430f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb31edb5",
   "metadata": {},
   "source": [
    "### Optional lab: Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc14819",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81487c94",
   "metadata": {},
   "source": [
    "### Cost function with regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebd7af1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b98e4a04",
   "metadata": {},
   "source": [
    "### Regularized linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d2fd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "865f5920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T05:05:14.896887Z",
     "start_time": "2023-03-17T05:05:14.877936Z"
    }
   },
   "source": [
    "### Optional lab: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d5b0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abb3c69f",
   "metadata": {},
   "source": [
    "## Practice Quiz: The problem of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b0309",
   "metadata": {},
   "source": [
    "## Week 3 practice lab: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b690a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4b5f58d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b76f76a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac30be0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcda13bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ca274b6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ga]",
   "language": "python",
   "name": "conda-env-ga-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "781px",
    "left": "22px",
    "top": "111.125px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
