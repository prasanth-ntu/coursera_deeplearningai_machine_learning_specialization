{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c481a6b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-with-logistic-regression\" data-toc-modified-id=\"Classification-with-logistic-regression-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Classification with logistic regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Motivations\" data-toc-modified-id=\"Motivations-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Motivations</a></span></li><li><span><a href=\"#Optional-lab:-Classification\" data-toc-modified-id=\"Optional-lab:-Classification-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Optional lab: Classification</a></span></li><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Logistic regression</a></span></li><li><span><a href=\"#Optional-lab:-Sigmoid-function-and-logistic-regression\" data-toc-modified-id=\"Optional-lab:-Sigmoid-function-and-logistic-regression-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Optional lab: Sigmoid function and logistic regression</a></span></li><li><span><a href=\"#Decision-boundary\" data-toc-modified-id=\"Decision-boundary-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>Decision boundary</a></span></li><li><span><a href=\"#Optional-lab:-Decision-boundary\" data-toc-modified-id=\"Optional-lab:-Decision-boundary-1.1.6\"><span class=\"toc-item-num\">1.1.6&nbsp;&nbsp;</span>Optional lab: Decision boundary</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Classification-with-logistic-regression\" data-toc-modified-id=\"Practice-quiz:-Classification-with-logistic-regression-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Practice quiz: Classification with logistic regression</a></span></li><li><span><a href=\"#Cost-function-for-logistic-regression\" data-toc-modified-id=\"Cost-function-for-logistic-regression-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Cost function for logistic regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cost-function-for-logistic-regression\" data-toc-modified-id=\"Cost-function-for-logistic-regression-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Cost function for logistic regression</a></span></li><li><span><a href=\"#Optional-lab:-Logistic-loss\" data-toc-modified-id=\"Optional-lab:-Logistic-loss-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Optional lab: Logistic loss</a></span></li><li><span><a href=\"#Simplified-Cost-Function-for-Logistic-Regression\" data-toc-modified-id=\"Simplified-Cost-Function-for-Logistic-Regression-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Simplified Cost Function for Logistic Regression</a></span></li><li><span><a href=\"#Optional-lab:-Cost-function-for-logistic-regression\" data-toc-modified-id=\"Optional-lab:-Cost-function-for-logistic-regression-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Optional lab: Cost function for logistic regression</a></span></li></ul></li><li><span><a href=\"#Practice-Quiz:-Cost-function-for-logistic-regression\" data-toc-modified-id=\"Practice-Quiz:-Cost-function-for-logistic-regression-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Practice Quiz: Cost function for logistic regression</a></span></li><li><span><a href=\"#Gradient-descent-for-logistic-regression\" data-toc-modified-id=\"Gradient-descent-for-logistic-regression-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Gradient descent for logistic regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gradient-Descent-Implementation\" data-toc-modified-id=\"Gradient-Descent-Implementation-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Gradient Descent Implementation</a></span></li><li><span><a href=\"#Optional-lab:-Gradient-descent-for-logistic-regression\" data-toc-modified-id=\"Optional-lab:-Gradient-descent-for-logistic-regression-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Optional lab: Gradient descent for logistic regression</a></span></li><li><span><a href=\"#Optional-lab:-Logistic-regression-with-scikit-learn\" data-toc-modified-id=\"Optional-lab:-Logistic-regression-with-scikit-learn-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Optional lab: Logistic regression with scikit-learn</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Gradient-descent-for-logistic-regression\" data-toc-modified-id=\"Practice-quiz:-Gradient-descent-for-logistic-regression-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Practice quiz: Gradient descent for logistic regression</a></span></li><li><span><a href=\"#The-problem-of-overfitting\" data-toc-modified-id=\"The-problem-of-overfitting-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>The problem of overfitting</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-problem-of-overfitting\" data-toc-modified-id=\"The-problem-of-overfitting-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>The problem of overfitting</a></span></li><li><span><a href=\"#Addressing-overfitting\" data-toc-modified-id=\"Addressing-overfitting-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>Addressing overfitting</a></span></li><li><span><a href=\"#Optional-lab:-Overfitting\" data-toc-modified-id=\"Optional-lab:-Overfitting-1.7.3\"><span class=\"toc-item-num\">1.7.3&nbsp;&nbsp;</span>Optional lab: Overfitting</a></span></li><li><span><a href=\"#Cost-function-with-regularization\" data-toc-modified-id=\"Cost-function-with-regularization-1.7.4\"><span class=\"toc-item-num\">1.7.4&nbsp;&nbsp;</span>Cost function with regularization</a></span></li><li><span><a href=\"#Regularized-linear-regression\" data-toc-modified-id=\"Regularized-linear-regression-1.7.5\"><span class=\"toc-item-num\">1.7.5&nbsp;&nbsp;</span>Regularized linear regression</a></span></li><li><span><a href=\"#Optional-lab:-Regularization\" data-toc-modified-id=\"Optional-lab:-Regularization-1.7.6\"><span class=\"toc-item-num\">1.7.6&nbsp;&nbsp;</span>Optional lab: Regularization</a></span></li></ul></li><li><span><a href=\"#Practice-Quiz:-The-problem-of-overfitting\" data-toc-modified-id=\"Practice-Quiz:-The-problem-of-overfitting-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Practice Quiz: The problem of overfitting</a></span></li><li><span><a href=\"#Week-3-practice-lab:-Logistic-regression\" data-toc-modified-id=\"Week-3-practice-lab:-Logistic-regression-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Week 3 practice lab: Logistic regression</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a56be1",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e09abb",
   "metadata": {},
   "source": [
    "## Classification with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb77326",
   "metadata": {},
   "source": [
    "### Motivations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0a94c",
   "metadata": {},
   "source": [
    "In **classification**, our output variable $y$ can take on only one of a small handful of possible values instead of any number in an infinite range of numbers.\n",
    "- Classification problem with two possible classes or categories is called **binary classification**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4763df1",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/logistic_regression_motivation_1.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14deced0",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/logistic_regression_motivation_2.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "    <img src=\"attachments/logistic_regression_motivation_3.png\" alt=\"Drawing\" style=\"width: 50%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64066673",
   "metadata": {},
   "source": [
    "In the above tumor example, \n",
    "- class 1 (+) => Malignant\n",
    "- class 0 (-) => Non-malignant (Benign)\n",
    "\n",
    "For first example, we can try applying linear regression to fit straight line to the data. Linear regression predicts not just 0 and 1, but all numbers between 0 and 1, and outside 0 and 1. However, we want to predict categories. One way to convert the output of linear regression to categories is to choose a **threshold of 0.5** such that \n",
    "- if $f_{w,b}(x)=wx+b \\lt 0.5 \\rightarrow \\hat{y} = 0$\n",
    "- if $f_{w,b}(x)=wx+b \\ge 0.5 \\rightarrow \\hat{y} = 1$\n",
    "For this first example, it looks like linear regression could do something reasonable in predicting the two classes.\n",
    "\n",
    "However, in the second example, we add one more training example that looks more like an outlier. Ideally, the new training example should not change the way we classify the data points. <span style=\"color:red\">The best fit line now shifts over, and if we continue using the <b>threshold of 0.5</b>, the dividing line (a.k.a the <b>decision boundary</b>) shift over to the right, thus many malignant tumours are now misclassified as non-malginant as it now learns a much worse function.</span>\n",
    "\n",
    "> In a nutshell, the linear model is insufficient to model categorical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e774e",
   "metadata": {},
   "source": [
    "### Optional lab: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac4563",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Optional%20Labs/C1_W3_Lab01_Classification_Soln.ipynb) solutions and [my practice](greyhatguy007/Optional%20Labs/C1_W3_Lab01_Classification_Practice_TP.ipynb) for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd4aca",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae84464",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/logistic_regression_1.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471db436",
   "metadata": {},
   "source": [
    "In previous example, we saw that linear regression is not a good algorithm for above problem.\n",
    "\n",
    "In contrast to fitting a straight line in linear regression, the logistic regression fits a *S*-shaped curve to the data.\n",
    "\n",
    "<span style=\"color:blue\">To build out to logistic regression algo, we need to learn about the <b><u>Sigmoid function (a.k.a. Logistic function).</u></b></span> The sigmoid function $g(z)$ offers outputs between $0$ and $1$.\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}} \\;\\;\\; 0<g(z)<1$$\n",
    "\n",
    "where $e$ is a mathematical constant with value of $2.71...$\n",
    "\n",
    "- If $z$ is very large +ve number (e.g., $z=100$), then $g(z) = \\frac{1}{1+e^{-100}} \\approx \\frac{1}{1+\\text{ very tiny number}} \\approx 1$\n",
    "- If $z$ is very large -ve number (e.g., $z=-100$), then $g(z) = \\frac{1}{1+e^{100}} \\approx \\frac{1}{1+\\text{ very large number}} \\approx 0$\n",
    "- If $z$ is zero (e.g., $z=0$), then $g(z) = \\frac{1}{1+e^{0}} = \\frac{1}{1+1} = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d72293",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/logistic_regression_2.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925989dc",
   "metadata": {},
   "source": [
    "For linear regression,\n",
    "$$f_{w,b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x} + b$$\n",
    "\n",
    "For logistic regression,\n",
    "$$f_{w,b}(\\vec{x}) = g(\\vec{w}\\cdot\\vec{x} + b) = g(z) = \\frac{1}{1+e^{-(\\vec{w}\\cdot\\vec{x}+b)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca5b0ac",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/logistic_regression_3.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da4afc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T23:27:28.591464Z",
     "start_time": "2023-03-17T23:27:28.572360Z"
    }
   },
   "source": [
    "<span style=\"font-size:+22px\">$f_{w,b}(\\vec{x}) = \\frac{1}{1+e^{-(\\vec{w}\\cdot\\vec{x}+b)}}$</span> \n",
    "\n",
    "The output of logistic regression (above) can be interepreted as probability that the output belongs to class $1$ given input $x$.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Since $y$ has to be either $0$ or $1$,\n",
    "$$P(y=0) + P(y=1)=1$$\n",
    "\n",
    "For e.g., $P(y=1)=70\\%$, it implies $P(y=0)=30\\%$\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "It can also be expressed as \n",
    "$$f_{w,b}(\\vec{x}) = P(y=1|{x};\\vec{w},b)$$\n",
    "which can be interpreted as probability of $y$ being equal to $1$ given the input feature $x$ with parameters $w$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8551e2",
   "metadata": {},
   "source": [
    "### Optional lab: Sigmoid function and logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc20e74",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Optional%20Labs/C1_W3_Lab02_Sigmoid_function_Soln.ipynb) solutions and [my practice](greyhatguy007/Optional%20Labs/C1_W3_Lab02_Sigmoid_function_Practice_TP.ipynb) for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5da8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T23:49:17.848757Z",
     "start_time": "2023-03-17T23:49:17.833037Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/lab_sigmoid_function_and_logistic_regression_1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "    <img src=\"attachments/lab_sigmoid_function_and_logistic_regression_2_edited.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7212c03",
   "metadata": {},
   "source": [
    "Note:\n",
    "- In the above figure, the orange line is $z$ or $w\\cdot x^{(i)}+b$\n",
    "    - Unlike linear regression which would have wrongly classified two of malignant cases as benign in the second example, the logistic regression model correctly predicts the two classes despite having some extreme cases.\n",
    "- In a nutshell, <span style=\"color:blue\"><b>the logistic regression model applies sigmoid to the output of linear regression model.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c1dcc1",
   "metadata": {},
   "source": [
    "### Decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b0ec9",
   "metadata": {},
   "source": [
    "Let's look at decision boundary to get a better sense of how logistic regression is computing these predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d035bc3",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/decision_boundary_1.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b16622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T01:10:34.648898Z",
     "start_time": "2023-03-18T01:10:34.626231Z"
    }
   },
   "source": [
    "Recap: \n",
    "- Logistic regression can be computed step by step as follows\n",
    "    - Step 1: $$z=\\vec{w}\\cdot\\vec{x}+b$$\n",
    "    - Step 2: $$f_{w,b}{(\\vec{x})} = g(\\vec{w}\\cdot\\vec{x}+b) = \\frac{1}{1+e^{-wx+b}} \\\\ \\; \n",
    "    \\Rightarrow \\; f_{w,b}{(\\vec{x})} = g(z) = \\frac{1}{1+e^{-z}} \\\\\n",
    "    \\Rightarrow \\; f_{w,b}{(\\vec{x})} = P(y=1|x;\\vec{w},b)$$\n",
    "    - Step 3: $$\\hat{y} = \\left\\{\\begin{matrix}\n",
    "        1 & \\text{ if } f_{\\vec{w},b}(\\vec{x}) \\ge 0.5 & \\rightarrow g(z) \\ge 0.5 \\rightarrow z \\ge 0 \\rightarrow \\vec{w}\n",
    "        \\cdot\\vec{x}+b\\ge 0\\\\ \n",
    "        0 & \\text{otherwise} & \n",
    "        \\end{matrix}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0ab106",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/decision_boundary_2.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db1ebb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T01:30:16.948758Z",
     "start_time": "2023-03-18T01:30:16.930878Z"
    }
   },
   "source": [
    "Let's look at an example with 2 features, $x_1$ and $x_2$ and assuming the decision boundary is linear\n",
    "\n",
    "$$f_{\\vec{w},b}(\\vec{x}) = g(z) = g(\\vec{w}\\cdot\\vec{x} + b) = g(w_1x_1+w_2x_2+b)$$\n",
    "\n",
    "If $w_1=1$, $w_2=1$, and $b=-3$, then **linear decision boundary** line of logistic regression will be \n",
    "$$z = 0  \\\\\\Rightarrow \\vec{w}\\cdot\\vec{x} + b = 0 \\\\\\Rightarrow w_1x_1+w_2x_2+b = 0 \\\\\\Rightarrow x_1 + x_2 -3 = 0 \\\\\\Rightarrow x_1 + x_2 = 3$$\n",
    "- The data points with features above this line, logistic regression will predict it as $\\hat{y}=1$, and for the points below the line, $\\hat{y}=0$\n",
    "- If we have different choice of parameters, then the decision boundary will be different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc626e05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T01:38:23.460237Z",
     "start_time": "2023-03-18T01:38:23.444022Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/decision_boundary_3.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b8362",
   "metadata": {},
   "source": [
    "Let's look at another example where decision boundary is no longer a straight line (e.g, polynomial)\n",
    "\n",
    "$$f_{\\vec{w},b}(\\vec{x}) = g(z) = g(\\vec{w}\\cdot\\vec{x} + b) = g(w_1x_1^2+w_2x_2^2+b)$$\n",
    "\n",
    "If $w_1=1$, $w_2=1$, and $b=-1$, then **non-linear decision boundary** line of logistic regression will be \n",
    "$$z = 0  \\\\\\Rightarrow \\vec{w}\\cdot\\vec{x} + b = 0 \\\\\\Rightarrow w_1x_1^2+w_2x_2^2+b = 0 \\\\\\Rightarrow x_1^2 + x_2^2 -1 = 0 \\\\\\Rightarrow x_1^2 + x_2^2 = 1$$\n",
    "- The data points with features outside this circle $x_1^2 + x_2^2 \\ge 1$, logistic regression will predict it as $\\hat{y}=1$, and for the points inside this circle $x_1^2 + x_2^2 \\lt 1$, $\\hat{y}=0$\n",
    "- If we have different choice of parameters, then the decision boundary will be different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1b052b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/decision_boundary_4.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db8cb3",
   "metadata": {},
   "source": [
    "We can even produce **non-linear complex decision boundaries** by including combinations of higher order polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ef7174",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/decision_boundary_5.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee95c22",
   "metadata": {},
   "source": [
    "### Optional lab: Decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d2839",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Optional%20Labs/C1_W3_Lab03_Decision_Boundary_Soln.ipynb) solutions and [my practice](greyhatguy007/Optional%20Labs/C1_W3_Lab03_Decision_Boundary_Practice_TP.ipynb) for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8423c927",
   "metadata": {},
   "source": [
    "## Practice quiz: Classification with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0aea2",
   "metadata": {},
   "source": [
    "## Cost function for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd41b5",
   "metadata": {},
   "source": [
    "### Cost function for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bd7f05",
   "metadata": {},
   "source": [
    "Training set for logistic regression for cancer prediction might look like follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7410491",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/cost_function_for_logistic_regression_1.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ccff4",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/cost_function_for_logistic_regression_2.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb3ee6",
   "metadata": {},
   "source": [
    "Cost function helps us to measure how well a specific set of parameters fit the training data, thereby allowing us to choose better parameters.\n",
    "\n",
    "| Details | Linear regression | Logistic regression |\n",
    "| :------- | :----------------- | :------------------- |\n",
    "| **cost function**, $J(\\vec{w},b)$ | **Squared error** works <br><br> Is convex $\\Rightarrow$  Works well | **Squared error** <span style=\"color:red\">does not work</span> <br><br> Is non-convex $\\Rightarrow$ <span style=\"color:red\">Gradient descent might get end up in sub-optimal local minima </span>| "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6406216",
   "metadata": {},
   "source": [
    "Cost function of linear regression, but doesn't work well for logistic regression\n",
    "$$J(\\vec{w},b) = \\frac{1}{2m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)})^2 = \\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{\\frac{1}{2}(f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)})^2}$$\n",
    "\n",
    "So, we **need to build a new cost function that works for logistic regression**. \n",
    "- In order to build a new cost function for logistic regression, we will change the definition of cost function of the linear regression a little bit such that the new cost function will be a convex function.\n",
    "\n",
    "Let's denote the **loss on a single training example** as $L$,\n",
    "$$L\\mathbf{(f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)})}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa409cf",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/cost_function_for_logistic_regression_3.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523fbdaf",
   "metadata": {},
   "source": [
    "**Logistic loss function**\n",
    "$$L(f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)}) = \\left\\{\\begin{matrix}\n",
    "        \\phantom{1- }-log\\left(f_{\\vec{w},b}(\\vec{x}^{(i)})\\right ) & \\text{ if } y^{(i)}=1 \\\\ \n",
    "        -log\\left(1- f_{\\vec{w},b}(\\vec{x}^{(i)})\\right ) & \\text{ if } y^{(i)}=0 & \n",
    "        \\end{matrix}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc741dd",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/cost_function_for_logistic_regression_4.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd127db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e8e2489",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/cost_function_for_logistic_regression_5.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0df48b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6113d2dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98bbf677",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c11dce6c",
   "metadata": {},
   "source": [
    "### Optional lab: Logistic loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e13b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c10c541d",
   "metadata": {},
   "source": [
    "### Simplified Cost Function for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fbc45c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01cdea9e",
   "metadata": {},
   "source": [
    "### Optional lab: Cost function for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f85e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c247d0cd",
   "metadata": {},
   "source": [
    "## Practice Quiz: Cost function for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf3c34",
   "metadata": {},
   "source": [
    "## Gradient descent for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e58198",
   "metadata": {},
   "source": [
    "### Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249eb873",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5495181a",
   "metadata": {},
   "source": [
    "### Optional lab: Gradient descent for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea78c2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a54b246",
   "metadata": {},
   "source": [
    "### Optional lab: Logistic regression with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8851d4c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aedcfeb0",
   "metadata": {},
   "source": [
    "## Practice quiz: Gradient descent for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b096fe7",
   "metadata": {},
   "source": [
    "## The problem of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce2d4d",
   "metadata": {},
   "source": [
    "### The problem of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd2cd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e84e200",
   "metadata": {},
   "source": [
    "### Addressing overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116602c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb31edb5",
   "metadata": {},
   "source": [
    "### Optional lab: Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9000bf1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81487c94",
   "metadata": {},
   "source": [
    "### Cost function with regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a137a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b98e4a04",
   "metadata": {},
   "source": [
    "### Regularized linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee977b6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "865f5920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T05:05:14.896887Z",
     "start_time": "2023-03-17T05:05:14.877936Z"
    }
   },
   "source": [
    "### Optional lab: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248beb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abb3c69f",
   "metadata": {},
   "source": [
    "## Practice Quiz: The problem of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b0309",
   "metadata": {},
   "source": [
    "## Week 3 practice lab: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b690a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4b5f58d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b76f76a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac30be0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcda13bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ca274b6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ga]",
   "language": "python",
   "name": "conda-env-ga-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "781px",
    "left": "22px",
    "top": "111.125px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
