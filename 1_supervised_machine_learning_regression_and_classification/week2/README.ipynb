{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ed828b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Course-1-Week-2:-Regression-with-multiple-input-variable\" data-toc-modified-id=\"Course-1-Week-2:-Regression-with-multiple-input-variable-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Course 1 Week 2: Regression with multiple input variable</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multiple-linear-regression\" data-toc-modified-id=\"Multiple-linear-regression-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Multiple linear regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multiple-features\" data-toc-modified-id=\"Multiple-features-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Multiple features</a></span></li><li><span><a href=\"#Vectorization-part-1\" data-toc-modified-id=\"Vectorization-part-1-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Vectorization part 1</a></span></li><li><span><a href=\"#Vectorization-part-2\" data-toc-modified-id=\"Vectorization-part-2-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Vectorization part 2</a></span></li><li><span><a href=\"#Optional-lab:-Python,-Numpy-and-vectorization\" data-toc-modified-id=\"Optional-lab:-Python,-Numpy-and-vectorization-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Optional lab: Python, Numpy and vectorization</a></span></li><li><span><a href=\"#Gradient-descent-for-multiple-linear-regression\" data-toc-modified-id=\"Gradient-descent-for-multiple-linear-regression-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>Gradient descent for multiple linear regression</a></span></li><li><span><a href=\"#Optional-lab:-Multiple-linear-regression\" data-toc-modified-id=\"Optional-lab:-Multiple-linear-regression-1.1.6\"><span class=\"toc-item-num\">1.1.6&nbsp;&nbsp;</span>Optional lab: Multiple linear regression</a></span></li></ul></li><li><span><a href=\"#Practice-Quiz:-Multiple-linear-regression\" data-toc-modified-id=\"Practice-Quiz:-Multiple-linear-regression-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Practice Quiz: Multiple linear regression</a></span></li><li><span><a href=\"#Gradient-descent-in-practice\" data-toc-modified-id=\"Gradient-descent-in-practice-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Gradient descent in practice</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-scaling-part-1\" data-toc-modified-id=\"Feature-scaling-part-1-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Feature scaling part 1</a></span></li><li><span><a href=\"#Feature-scaling-part-2\" data-toc-modified-id=\"Feature-scaling-part-2-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Feature scaling part 2</a></span></li><li><span><a href=\"#Checking-gradient-descent-for-convergence\" data-toc-modified-id=\"Checking-gradient-descent-for-convergence-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Checking gradient descent for convergence</a></span></li><li><span><a href=\"#Choosing-the-learning-rate\" data-toc-modified-id=\"Choosing-the-learning-rate-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Choosing the learning rate</a></span></li><li><span><a href=\"#Optional-lab:-Feature-scaling-and-learning-rate\" data-toc-modified-id=\"Optional-lab:-Feature-scaling-and-learning-rate-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Optional lab: Feature scaling and learning rate</a></span></li><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-1.3.6\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>Feature engineering</a></span></li><li><span><a href=\"#Polynomial-regression\" data-toc-modified-id=\"Polynomial-regression-1.3.7\"><span class=\"toc-item-num\">1.3.7&nbsp;&nbsp;</span>Polynomial regression</a></span></li><li><span><a href=\"#Optional-lab:-Feature-engineering-and-Polynomial-regression\" data-toc-modified-id=\"Optional-lab:-Feature-engineering-and-Polynomial-regression-1.3.8\"><span class=\"toc-item-num\">1.3.8&nbsp;&nbsp;</span>Optional lab: Feature engineering and Polynomial regression</a></span></li><li><span><a href=\"#Optional-lab:-Linear-regression-with-scikit-learn\" data-toc-modified-id=\"Optional-lab:-Linear-regression-with-scikit-learn-1.3.9\"><span class=\"toc-item-num\">1.3.9&nbsp;&nbsp;</span>Optional lab: Linear regression with scikit-learn</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Gradient-descent-in-practice\" data-toc-modified-id=\"Practice-quiz:-Gradient-descent-in-practice-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Practice quiz: Gradient descent in practice</a></span></li><li><span><a href=\"#Week-2-practice-lab:-Linear-regression\" data-toc-modified-id=\"Week-2-practice-lab:-Linear-regression-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Week 2 practice lab: Linear regression</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e533bbe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T22:40:15.173425Z",
     "start_time": "2023-03-12T22:40:15.157345Z"
    }
   },
   "source": [
    "# Course 1 Week 2: Regression with multiple input variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851d017",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T22:40:48.421724Z",
     "start_time": "2023-03-12T22:40:48.406390Z"
    }
   },
   "source": [
    "## Multiple linear regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22646f0b",
   "metadata": {},
   "source": [
    "### Multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2398df",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/multiple_features_1.png\" alt=\"Drawing\" style=\"width: 40%;\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef5ce01",
   "metadata": {},
   "source": [
    "In our original version of our linear regression, we had a single feature $x$, the size of the house, and we are able to predict $y$, the price of the house. So, the model was\n",
    "$$f_{w,b}(x) = wx+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d5e8f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T23:20:22.904354Z",
     "start_time": "2023-03-12T23:20:22.895787Z"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"attachments/multiple_features_2.png\" alt=\"Drawing\" style=\"width: 45%;\"/></div>\n",
    "Now, we have more features that gives us lot more info to predict the price of our house."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a95d014",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T23:30:11.771368Z",
     "start_time": "2023-03-12T23:30:11.749780Z"
    }
   },
   "source": [
    "Since, we have multiple features, let's learn bit more notations.\n",
    "- $x_{j}=$ $j^{th}$ feature $\\Rightarrow$ $j=1..4$ \n",
    "- $n=$ number of features  $\\Rightarrow$ $n=4$ \n",
    "- $\\mathbf{x}^{(i)}=$ features of $i^{th}$ training example (aka row vector) $\\Rightarrow$ $\\mathbf{x}^{(2)}=\\begin{bmatrix}1416 & 3 & 2 & 40 \\end{bmatrix}$\n",
    "    - Sometimes, $\\mathbf{x}^{(i)}$ can be expressed as $\\vec{x}^{(i)}$, but the arrow is optional.\n",
    "- $x^{(i)}_{j}=$ value of feature $j$ in $i^{th}$ training example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccaca72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T23:39:26.482195Z",
     "start_time": "2023-03-12T23:39:26.466940Z"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"attachments/multiple_features_3.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "<img src=\"attachments/multiple_features_4.png\" alt=\"Drawing\" style=\"width: 40%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dbb09c",
   "metadata": {},
   "source": [
    "Previously: $f_{w,b}(x) = wx+b$, as there was only **one feature for linear regression**\n",
    "\n",
    "Now: $f_{w,b}(x) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + w_{4}x_{4} + b$, as we have four features.\n",
    "\n",
    "If we generalise this, we will get **multiple linear regression**\n",
    "> $$f_{\\vec{w},b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x}+b = w_{1}x_{1} + w_{2}x_{2} + ... + w_{n}x_{n} + b$$\n",
    "\n",
    "where\n",
    "- parameters of the model are\n",
    "    - $\\vec{w} = \\begin{bmatrix}w_{1} & w_{2} & w_{3} & ... & w_{n} \\end{bmatrix}$ is a vactor\n",
    "    - $b$ is a number\n",
    "- $\\vec{x} = \\begin{bmatrix}x_{1} & x_{2} & x_{3} & ... & x_{n} \\end{bmatrix}$ is a vector\n",
    "- $\\vec{w}\\cdot\\vec{x}$ refers to dot product between vectors $w$ and $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc796639",
   "metadata": {},
   "source": [
    "### Vectorization part 1\n",
    "<div align=\"center\"><img src=\"attachments/vectorization_part1.png\" alt=\"Drawing\" style=\"width: 60%;\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03081efd",
   "metadata": {},
   "source": [
    "We can use vectorization to make our code shorter and more efficient/faster as explained above. For e.g., `np.dot(w,x)` uses parallel hardware instead of sequential calculation.\n",
    "\n",
    "Note on **Indexing**\n",
    "- In linear algebra, indexing starts from 1\n",
    "- In Python, indexing starts from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61021796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:03:00.421849Z",
     "start_time": "2023-03-13T00:03:00.404011Z"
    }
   },
   "source": [
    "### Vectorization part 2\n",
    "\n",
    "<div align=\"center\"><img src=\"attachments/vectorization_part2_1.png\" alt=\"Drawing\" style=\"width: 60%;\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f88893",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:15:03.812242Z",
     "start_time": "2023-03-13T00:15:03.794927Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(1000)\n",
    "w = np.random.rand(1000)\n",
    "n = len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2db2a04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:16:24.907011Z",
     "start_time": "2023-03-13T00:16:24.894692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.45 µs ± 1.64 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "f = np.dot(x,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80421a63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:16:25.392175Z",
     "start_time": "2023-03-13T00:16:25.132473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349 µs ± 44.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "f=0\n",
    "for i in range(n):\n",
    "    f+= w[i]*x[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42fa920",
   "metadata": {},
   "source": [
    "<p>From the above code cells, we can see that the vectorized operation is <u>100x faster</u> than sequential operation.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b7da9",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/vectorization_part2_2.png\" alt=\"Drawing\" style=\"width: 60%;\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6fa24",
   "metadata": {},
   "source": [
    "From above illustration, we can see that with vectorised implementation, the weights can be updated much faster in gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563835f",
   "metadata": {},
   "source": [
    "### Optional lab: Python, Numpy and vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c0a9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T01:42:58.942090Z",
     "start_time": "2023-03-13T01:42:58.934192Z"
    }
   },
   "source": [
    "Refer to [greyhat](greyhatguy007/Optional%20Labs/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb) solutions for now\n",
    "\n",
    "Note on notations:\n",
    "- Vectors are one dimensional arrays. Vectors are denoted with lower case bold letters such as $\\mathbf{x}$.\n",
    "- Matrices, are two dimensional arrays. Matrices are denoted with capitol, bold letter such as $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be4178",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient descent for multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b3fce",
   "metadata": {},
   "source": [
    "Gradient descent for multiple linear regression with and without **vectorization**\n",
    "\n",
    "<div align=\"center\"><img src=\"attachments/gradient_descent_for_multiple_linear_regression_1.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "<img src=\"attachments/gradient_descent_for_multiple_linear_regression_2.png\" alt=\"Drawing\" style=\"width: 45%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb772b41",
   "metadata": {},
   "source": [
    "| Details | Previous notation (without vectorization) | Vector notation |\n",
    "| -: | :- | :- |\n",
    "| Parameters | $w_1, ..., w_n$ <br>$b$| $\\vec{w}=[w_1 \\dotsc w_n]$<br>$b$ |\n",
    "| Model | $f_{\\vec{w},b}(\\vec{x}) = w_{1}x_{1} + \\dotsc + w_{n}x_{n} + b$ | $f_{\\vec{w},b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x}+b$ |\n",
    "| Cost function | $J(w_1, \\dotsc, w_n, b)$ | $J(\\vec{w},b)$ |\n",
    "| Gradient descent | repeat {<br>$\\; w_j = w_j -\\alpha\\frac{\\partial }{\\partial w_{j}}J(w_1, \\dotsc, w_n, b)$ <br> $\\; b = b -\\alpha\\frac{\\partial }{\\partial b}J(w_1, \\dotsc, w_n, b)$ <br>}| repeat {<br>$\\; w_j = w_j -\\alpha\\frac{\\partial }{\\partial w_{j}}J(\\vec{w}, b)$ <br> $\\; b = b -\\alpha\\frac{\\partial }{\\partial b}J(\\vec{w}, b)$ <br>} |\n",
    "| $$$$ | $$$$ | $$$$ |\n",
    "\n",
    "where \n",
    "- $\\vec{w}$ is vector of length $n$, and $n$ represents to number of columns/features\n",
    "- $b$ is still a number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f39307",
   "metadata": {},
   "source": [
    "Now, let's see what this looks like when we implement **gradient descent**. In particular, let's look at the derivative term when we have single feature vs. multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71654f72",
   "metadata": {},
   "source": [
    "| One feature | $n$ features ($n \\ge 2$) |\n",
    "| :--- | :--- |\n",
    "| repeat{ |  repeat{|\n",
    "| $w = w - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w}J(w,b)$ | $w_1 = w_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)})x_{1}^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w_1}J(\\vec{w},b)$ <br> $\\vdots$ <br> $w_n = w_n - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)})x_{n}^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w_n}J(\\vec{w},b)$ |\n",
    "| $b = b - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)}) = b - \\alpha\\frac{\\partial}{\\partial b}J(w,b)$ | $b = b - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)}) = b - \\alpha\\frac{\\partial}{\\partial b}J(\\vec{w},b)$ |\n",
    "| simultaneously update <br> $w,b $ <br>}|  simultaneously update <br> $w_j$ for $(j=1, \\dotsc, n)$ and $b$<br>} |\n",
    "| $$$$ | $$$$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a7f47",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/gradient_descent_for_multiple_linear_regression_3.png\" alt=\"Drawing\" style=\"width: 55%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd895c",
   "metadata": {},
   "source": [
    "\n",
    "### Optional lab: Multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c55efb",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Optional%20Labs/C1_W2_Lab02_Multiple_Variable_Soln.ipynb) solutions and [my practice](greyhatguy007/Optional%20Labs/C1_W2_Lab02_Multiple_Variable_Practice_TP.ipynb) for now\n",
    "\n",
    "Note on notations:\n",
    "- Here is a summary of some of the notation we will encounter, updated for multiple features.  \n",
    "\n",
    "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalar, non bold                                                      ||\n",
    "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
    "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | training example maxtrix                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95beceed",
   "metadata": {},
   "source": [
    "## Practice Quiz: Multiple linear regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7b4591",
   "metadata": {},
   "source": [
    "## Gradient descent in practice\n",
    "### Feature scaling part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07baa7fb",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/feature_scaling_part_1_1.png\" alt=\"Drawing\" style=\"width: 50%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8944b65",
   "metadata": {},
   "source": [
    "- When possible range of **feature values are large**, it's more likely that a good model will learn to choose a relatively **small parameter value**, like 0.1.\n",
    "    - e.g., House size, $x_1=2000$ sq. feet and $w_1 = 0.1$\n",
    "- Likewise, when possible **feature values are small**, the reasonable value for these **parameters will be relatively large**\n",
    "    - e.g., No. of bedrooms, $x_2=5$ and $w_2 = 50$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdef258",
   "metadata": {},
   "source": [
    "**How does this relate to gradient descent?**\n",
    "\n",
    "- Let's look at the scatter plot of the features where size (in sq. feet) is the horiontal axis, and the number of bedroom is on the vertical axis. \n",
    "    - If we plot the training data, we notice that *horizontal axis is on a much large scale/range of values compared to vertical axis*\n",
    "- Let's then look at the contour plot of cost function, where $w_1$ is the horizontal axis, and $w_2$ is the vertical axis. \n",
    "    - Here, *horizontal axis has much narrower range, whereas vertical axis takes on much larger values*. $\\Rightarrow$ Contours form ovals/elipses and they are short on one side and long on other.\n",
    "        - This is because a **small change in $w_1$ can have a large impact on estimated price** as it is multiplied with a very large value $x_1$ $\\Rightarrow$ Very large impact on cost $J$\n",
    "        - In contrast, it takes much larger change in $w_2$ in order to change predictions much.  $\\Rightarrow$  **small changes to $w_2$ don't change the cost function nearly that much**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a98bc",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/feature_scaling_part_1_2.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "    <img src=\"attachments/feature_scaling_part_1_3.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35978be4",
   "metadata": {},
   "source": [
    "The problem with skewed contours is that the gradient descent may end up taking long time before it can find its way to global minima. In this situation, a useful thing to do is to <span style=\"color:green\"><b>scale the features</b></span> by performing some transformation of our training data so that the features range from 0 to 1. If we run gradient descent on a cost function to find on this scaled features, then contours will look more like circles and less skewed $\\Rightarrow$ <span style=\"color:green\"><b>Gradient descent will run faster</b></span> as it can find a much more direct path to global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b1f8e",
   "metadata": {},
   "source": [
    "### Feature scaling part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25afdc88",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/feature_scaling_part_2_1.png\" alt=\"Drawing\" style=\"width: 50%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf4d268",
   "metadata": {},
   "source": [
    "**Divide by maximum**: One way to scale the features is to take the maximum of the feature value and divide all the samples by the maximum as shown above.\n",
    "**More generally**: Rescale each feature by both its minimum and maximum value by\n",
    "\n",
    "$$x_1 = \\frac{x_1 - min(x_1)}{max(x_1)- min(x_1)} \\;\\;\\;\\; x_2 = \\frac{x_2 - min(x_2)}{max(x_2) - min(x_2)}$$\n",
    "\n",
    "Both ways normalizes features to the range of -1 and 1, where the former method works for positive features which is simple and serves well for the lecture's example, and the latter method works for any features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f055f22",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/feature_scaling_part_2_2.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "    <img src=\"attachments/feature_scaling_part_2_3.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d73d53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T00:58:37.315084Z",
     "start_time": "2023-03-14T00:58:37.298439Z"
    }
   },
   "source": [
    "**Mean normalization**: Rescale the features so that they are cenetered around zero. \n",
    "\n",
    "To calculate mean normalization: \n",
    "$$x_1 = \\frac{x_1 - \\mu_{1}}{max(x_1)- min(x_1)} \\;\\;\\;\\; x_2 = \\frac{x_2 - \\mu_{2}}{max(x_2) - min(x_2)}$$\n",
    "\n",
    "where $u_1$ and $u_2$ are mean of $x_1$ and $x_2$ respectively\n",
    "\n",
    "**Z-score normalization**: Uses standard deviation $\\sigma$\n",
    "\n",
    "To calculate z-score normalization,\n",
    "$$x_1 = \\frac{x_1 - \\mu_{1}}{\\sigma_1} \\;\\;\\;\\; x_2 = \\frac{x_2 - \\mu_{2}}{\\sigma_2}$$\n",
    "\n",
    "where $u_1$ and $\\sigma_1$ are mean and standard deviation of $x_1$\n",
    "\n",
    "After z-score normalization, all features will have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351c40d",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><b><u>Rule of thumb for feature scaling</u></b></span>\n",
    "\n",
    "When in doubt, carry out the feature rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22191fc5",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/feature_scaling_part_2_4.png\" alt=\"Drawing\" style=\"width: 50%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af949baf",
   "metadata": {},
   "source": [
    "### Checking gradient descent for convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203693b",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\"><img src=\"attachments/checking_gradient_descent_for_convergence.png\" alt=\"Drawing\" style=\"width: 55%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c744a1",
   "metadata": {},
   "source": [
    "**How to ensure gradient descent is working correctly**\n",
    "\n",
    "- Objective: $\\underset{\\vec{w},b}{min}J(\\vec{w},b)$\n",
    "- Solution: \n",
    "    - We can plot the cost function over the number of iterations of gradient descent. This curve is aka **learning curve**\n",
    "    - <span style=\"color:green\">If gradient descent is working properly, the cost $J$ should decrease every single iteration</span>. <span style=\"color:red\">If the cost is increasing, it implies a bug in code or poor choice of $\\alpha$</span>\n",
    "    \n",
    "Note: The # of iterations needed for gradient descent to converge varies a lot by applications.\n",
    "Then, how do we decide when to stop the iterations? \n",
    "- **Automatic convergence test for gradient descent**: If $J(\\vec{w},b)$ decreases by $\\le \\epsilon$ in one iteration, declare *convergence* and stop the iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0c399",
   "metadata": {},
   "source": [
    "### Choosing the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9a47c",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/choosing_learning_rate_1.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "    <img src=\"attachments/choosing_learning_rate_2.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63df106",
   "metadata": {},
   "source": [
    "In the learning curve (cost vs. numbe of iterations), <span style=\"color:red\">if cost sometimes goes up and sometimes goes down, it implies that either gradient descent is not working properly or learning rate is too large</span>.\n",
    "\n",
    "**How to choose a proper learning rate for gradient descent?**\n",
    "- Ensure, that weights are properly updated ($w_1 \\neq w_1 + \\alpha d_1$, instead $w_1 = w_1 - \\alpha d_1$)\n",
    "- When the cost goes up or keeps wobbling, try smaller learning rate, and go down gradually.\n",
    "    - One way to debug correct implementation of gradient descent is that with small enough $\\alpha$, $J$ should decrease on every single iteration.\n",
    "    - Remember, setting $\\alpha$ to be very small value means gradient descent can take a lot of iterations to converge.\n",
    "    - Perhaps, we can try range of values for learning rate for every iterations\n",
    "        - Try 3x bigger $\\alpha$ for each new variant to find the largest possible learning rate\n",
    "            - e.g., $0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d8338",
   "metadata": {},
   "source": [
    "### Optional lab: Feature scaling and learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12005355",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Optional%20Labs/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb) solutions and [my practice](greyhatguy007/Optional%20Labs/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Practice_TP.ipynb) for now\n",
    "\n",
    "Note on notations:\n",
    "- Here is a summary of some of the notation we will encounter, updated for multiple features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d0c2e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T23:21:19.464010Z",
     "start_time": "2023-03-16T23:21:19.427426Z"
    }
   },
   "source": [
    "|General <br />  Notation  | Description| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalar, non bold                                                      ||\n",
    "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
    "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | training example maxtrix                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at  $\\mathbf{x}^{(i)}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n",
    "|$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$| the gradient or partial derivative of cost with respect to a parameter $w_j$ |`dj_dw[j]`| \n",
    "|$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$| the gradient or partial derivative of cost with respect to a parameter $b$| `dj_db`|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d14b57",
   "metadata": {},
   "source": [
    "Other things to note:\n",
    "- When $w$'s are updated unevenly during training, it implies that the features vary significantly in magnitude making some features update much faster than others. \n",
    "    - **Why are the $w$'s updated unevenly**? \n",
    "        - This is because when the gradients are computed, the common error term $f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{i}$ is multiplied by the features $\\color{Red}{x_j^{(i)}}$ for the $w$'s. \n",
    "        - In the lab housing dataset, feature $w_0$ (size in sq.ft) is 2-3 orders of magnitude larger than other features.\n",
    "        - This makes the gradient bigger. When the weights are updated based on learning rate $\\alpha$ and gradient computed from partial derivative with respect to each feature, $w_0$ will make more rapid progress than the other parameters due to its much larger gradient. \n",
    "$$\\frac{\\partial}{\\partial w_j}J(\\vec{w},b) = \\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{i})\\color{Red}{x_{j}^{(i)}}$$\n",
    "- When the features are normalized, the cost contour is much more symmetric. The result is that updates to parameters during gradient descent can make equal progress for each parameter. $\\Rightarrow$ The scaled features get very accurate results much, much faster!. Also, the gradient of each parameter will be tiny by the end of this fairly short run (iterations). \n",
    "- A learning rate of 0.1 is a good start for regression with normalized features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db650ee3",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5940c",
   "metadata": {},
   "source": [
    "**Feature engineering**: Using *intuition* or *knowledge* to design *new features*, by transforming or combining original features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7de844",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/feature_engineering_1.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74434a97",
   "metadata": {},
   "source": [
    "For e.g., if we have measurements for the dimensions of a swimming pool ($length$, $width$, $height$), a more useful engineered feature would be $length×width×height$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f22e86",
   "metadata": {},
   "source": [
    "### Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524410a6",
   "metadata": {},
   "source": [
    "Polynomial regression\n",
    "- A flavor of feature engineering that allows us to fit not just straight lines, but curves (non-linear functions) to our data.\n",
    "- Combines the ideas of multiple linear regression and feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d551a",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/polynomial_regression_1.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "    <img src=\"attachments/polynomial_regression_2.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250488fa",
   "metadata": {},
   "source": [
    "It doesn't look like a straight line fits this data-set very well. Maybe we want to fit a curve, maybe a **quadratic** function to the data like this which includes a size $x$ and also $x^2$. Maybe that will give us a better fit to the data. But then we may decide that our <span style=\"color:red\">quadratic model doesn't really make sense because a quadratic function eventually comes back down</span>. Well, we wouldn't really expect housing prices to go down when the size increases. Big houses seem like they should usually cost more. Then we may choose a <span style=\"color:green\">**cubic** function where we now have not only $x^2$, but $x^3$</span>. Maybe this model produces this curve here, which is a somewhat better fit to the data because the price does eventually come back up as the size increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cdcbe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T23:58:31.980440Z",
     "start_time": "2023-03-16T23:58:31.971077Z"
    }
   },
   "source": [
    "Instead of $x^2$, we can also consider taking $\\sqrt{x}$.\n",
    "\n",
    "So, how do decide which features to use? Will be addressed in Course 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f0079a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>If we create features like $x^2$, then feature scaling becomes increasingly important.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547bc712",
   "metadata": {},
   "source": [
    "### Optional lab: Feature engineering and Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac928dcb",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Optional%20Labs/C1_W2_Lab04_FeatEng_PolyReg_Soln.ipynb) solutions and [my practice](greyhatguy007/Optional%20Labs/C1_W2_Lab04_FeatEng_PolyReg_Practice_TP.ipynb) for now\n",
    "\n",
    "\n",
    "Note:\n",
    "- Polynomial features were chosen based on how well they matched the target data. Another way to think about this is to note that <span style=\"color:blue\">we are still using linear regression once we have created new (engineered) features. Given that, <b>the best new (engineered) features will be linear relative to the target</b></span>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e2a039",
   "metadata": {},
   "source": [
    "### Optional lab: Linear regression with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812feb8",
   "metadata": {},
   "source": [
    "For now, refer to\n",
    "- [greyhat](greyhatguy007/Optional%20Labs/C1_W2_Lab05_Sklearn_GD_Soln.ipynb) solutions and [my practice](greyhatguy007/Optional%20Labs/C1_W2_Lab05_Sklearn_GD_Practice_TP.ipynb) - For Gradient Descent regression model using  `sklearn.linear_model.SGDRegressor`\n",
    "- [greyhat](greyhatguy007/Optional%20Labs/C1_W2_Lab06_Sklearn_Normal_Soln.ipynb) solutions and [my practice](greyhatguy007/Optional%20Labs/C1_W2_Lab06_Sklearn_Normal_Practice_TP.ipynb) - For  closed-form linear regression using `sklearn.linear_model.LinearRegression`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba15e6e",
   "metadata": {},
   "source": [
    "Note\n",
    "- **Gradient Descent based**\n",
    "    - It is important to apply feature scaling when doing feature engineering with gradient descent based regression\n",
    "- **Closed form (Alternative to gradient descent)** \n",
    "    - The closed-form solution work well and faster on smaller data sets such as these but can be computationally demanding on larger data sets.\n",
    "    - The closed-form solution does not require normalization.\n",
    "    - An example of a closed form solution in linear regression would be the least square equation [Source](https://stats.stackexchange.com/questions/70848/what-does-a-closed-form-solution-mean)\n",
    "\n",
    "$$y = wX$$\n",
    "\n",
    "$$w=(X^{T}X)^{-1}X^{T}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce771a64",
   "metadata": {},
   "source": [
    "## Practice quiz: Gradient descent in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2838edd",
   "metadata": {},
   "source": [
    "## Week 2 practice lab: Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5574884a",
   "metadata": {},
   "source": [
    "For now, refer to [greyhat](greyhatguy007/C1W2A1/C1_W2_Linear_Regression.ipynb) solutions and [my practice](greyhatguy007/C1W2A1/C1_W2_Linear_Regression_Practice_TP.ipynb) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ga]",
   "language": "python",
   "name": "conda-env-ga-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "781px",
    "left": "43px",
    "top": "139px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
