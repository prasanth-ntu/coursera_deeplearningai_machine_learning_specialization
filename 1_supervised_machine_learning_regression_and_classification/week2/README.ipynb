{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e83f04",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Week-2:-Regression-with-multiple-input-variable\" data-toc-modified-id=\"Week-2:-Regression-with-multiple-input-variable-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Week 2: Regression with multiple input variable</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multiple-linear-regression\" data-toc-modified-id=\"Multiple-linear-regression-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Multiple linear regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multiple-features\" data-toc-modified-id=\"Multiple-features-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Multiple features</a></span></li><li><span><a href=\"#Vectorization-part-1\" data-toc-modified-id=\"Vectorization-part-1-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Vectorization part 1</a></span></li><li><span><a href=\"#Vectorization-part-2\" data-toc-modified-id=\"Vectorization-part-2-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Vectorization part 2</a></span></li><li><span><a href=\"#Optional-lab:-Python,-Numpy-and-vectorization\" data-toc-modified-id=\"Optional-lab:-Python,-Numpy-and-vectorization-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Optional lab: Python, Numpy and vectorization</a></span></li><li><span><a href=\"#Gradient-descent-for-multiple-linear-regression\" data-toc-modified-id=\"Gradient-descent-for-multiple-linear-regression-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>Gradient descent for multiple linear regression</a></span></li><li><span><a href=\"#Optional-lab:-Multiple-linear-regression\" data-toc-modified-id=\"Optional-lab:-Multiple-linear-regression-1.1.6\"><span class=\"toc-item-num\">1.1.6&nbsp;&nbsp;</span>Optional lab: Multiple linear regression</a></span></li></ul></li><li><span><a href=\"#Practice-Quiz:-Multiple-linear-regression\" data-toc-modified-id=\"Practice-Quiz:-Multiple-linear-regression-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Practice Quiz: Multiple linear regression</a></span></li><li><span><a href=\"#Gradient-descent-in-practice\" data-toc-modified-id=\"Gradient-descent-in-practice-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Gradient descent in practice</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-scaling-part-1\" data-toc-modified-id=\"Feature-scaling-part-1-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Feature scaling part 1</a></span></li><li><span><a href=\"#Feature-scaling-part-2\" data-toc-modified-id=\"Feature-scaling-part-2-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Feature scaling part 2</a></span></li><li><span><a href=\"#Checking-gradient-descent-for-convergence\" data-toc-modified-id=\"Checking-gradient-descent-for-convergence-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Checking gradient descent for convergence</a></span></li><li><span><a href=\"#Choosing-the-learning-rate\" data-toc-modified-id=\"Choosing-the-learning-rate-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Choosing the learning rate</a></span></li><li><span><a href=\"#Optional-lab:-Feature-scaling-and-learning-rate\" data-toc-modified-id=\"Optional-lab:-Feature-scaling-and-learning-rate-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Optional lab: Feature scaling and learning rate</a></span></li><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-1.3.6\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>Feature engineering</a></span></li><li><span><a href=\"#Polynomial-regression\" data-toc-modified-id=\"Polynomial-regression-1.3.7\"><span class=\"toc-item-num\">1.3.7&nbsp;&nbsp;</span>Polynomial regression</a></span></li><li><span><a href=\"#Optional-lab:-Feature-engineering-and-Polynomial-regression\" data-toc-modified-id=\"Optional-lab:-Feature-engineering-and-Polynomial-regression-1.3.8\"><span class=\"toc-item-num\">1.3.8&nbsp;&nbsp;</span>Optional lab: Feature engineering and Polynomial regression</a></span></li><li><span><a href=\"#Optional-lab:-Linear-regression-with-scikit-learn\" data-toc-modified-id=\"Optional-lab:-Linear-regression-with-scikit-learn-1.3.9\"><span class=\"toc-item-num\">1.3.9&nbsp;&nbsp;</span>Optional lab: Linear regression with scikit-learn</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Gradient-descent-in-practice\" data-toc-modified-id=\"Practice-quiz:-Gradient-descent-in-practice-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Practice quiz: Gradient descent in practice</a></span></li><li><span><a href=\"#Week-2-practice-lab:-Linear-regression\" data-toc-modified-id=\"Week-2-practice-lab:-Linear-regression-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Week 2 practice lab: Linear regression</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b8f15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T22:40:15.173425Z",
     "start_time": "2023-03-12T22:40:15.157345Z"
    }
   },
   "source": [
    "# Week 2: Regression with multiple input variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa521c75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T22:40:48.421724Z",
     "start_time": "2023-03-12T22:40:48.406390Z"
    }
   },
   "source": [
    "## Multiple linear regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169eff0d",
   "metadata": {},
   "source": [
    "### Multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc8046",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/multiple_features_1.png\" alt=\"Drawing\" style=\"width: 40%;\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03d44d",
   "metadata": {},
   "source": [
    "In our original version of our linear regression, we had a single feature $x$, the size of the house, and we are able to predict $y$, the price of the house. So, the model was\n",
    "$$f_{w,b}(x) = wx+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f75e18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T23:20:22.904354Z",
     "start_time": "2023-03-12T23:20:22.895787Z"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"attachments/multiple_features_2.png\" alt=\"Drawing\" style=\"width: 45%;\"/></div>\n",
    "Now, we have more features that gives us lot more info to predict the price of our house."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edebf82f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T23:30:11.771368Z",
     "start_time": "2023-03-12T23:30:11.749780Z"
    }
   },
   "source": [
    "Since, we have multiple features, let's learn bit more notations.\n",
    "- $x_{j}=$ $j^{th}$ feature $\\Rightarrow$ $j=1..4$ \n",
    "- $n=$ number of features  $\\Rightarrow$ $n=4$ \n",
    "- $\\mathbf{x}^{(i)}=$ features of $i^{th}$ training example (aka row vector) $\\Rightarrow$ $\\mathbf{x}^{(2)}=\\begin{bmatrix}1416 & 3 & 2 & 40 \\end{bmatrix}$\n",
    "    - Sometimes, $\\mathbf{x}^{(i)}$ can be expressed as $\\vec{x}^{(i)}$, but the arrow is optional.\n",
    "- $x^{(i)}_{j}=$ value of feature $j$ in $i^{th}$ training example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a896b138",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T23:39:26.482195Z",
     "start_time": "2023-03-12T23:39:26.466940Z"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"attachments/multiple_features_3.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "<img src=\"attachments/multiple_features_4.png\" alt=\"Drawing\" style=\"width: 40%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62ad7ef",
   "metadata": {},
   "source": [
    "Previously: $f_{w,b}(x) = wx+b$, as there was only **one feature for linear regression**\n",
    "\n",
    "Now: $f_{w,b}(x) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + w_{4}x_{4} + b$, as we have four features.\n",
    "\n",
    "If we generalise this, we will get **multiple linear regression**\n",
    "> $$f_{\\vec{w},b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x}+b = w_{1}x_{1} + w_{2}x_{2} + ... + w_{n}x_{n} + b$$\n",
    "\n",
    "where\n",
    "- parameters of the model are\n",
    "    - $\\vec{w} = \\begin{bmatrix}w_{1} & w_{2} & w_{3} & ... & w_{n} \\end{bmatrix}$ is a vactor\n",
    "    - $b$ is a number\n",
    "- $\\vec{x} = \\begin{bmatrix}x_{1} & x_{2} & x_{3} & ... & x_{n} \\end{bmatrix}$ is a vector\n",
    "- $\\vec{w}\\cdot\\vec{x}$ refers to dot product between vectors $w$ and $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe29fab",
   "metadata": {},
   "source": [
    "### Vectorization part 1\n",
    "<div align=\"center\"><img src=\"attachments/vectorization_part1.png\" alt=\"Drawing\" style=\"width: 60%;\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f22d2",
   "metadata": {},
   "source": [
    "We can use vectorization to make our code shorter and more efficient/faster as explained above. For e.g., `np.dot(w,x)` uses parallel hardware instead of sequential calculation.\n",
    "\n",
    "Note on **Indexing**\n",
    "- In linear algebra, indexing starts from 1\n",
    "- In Python, indexing starts from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faafe629",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:03:00.421849Z",
     "start_time": "2023-03-13T00:03:00.404011Z"
    }
   },
   "source": [
    "### Vectorization part 2\n",
    "\n",
    "<div align=\"center\"><img src=\"attachments/vectorization_part2_1.png\" alt=\"Drawing\" style=\"width: 60%;\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbaf79bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:15:03.812242Z",
     "start_time": "2023-03-13T00:15:03.794927Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(1000)\n",
    "w = np.random.rand(1000)\n",
    "n = len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79d03f3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:16:24.907011Z",
     "start_time": "2023-03-13T00:16:24.894692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.45 µs ± 1.64 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "f = np.dot(x,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc94e6d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:16:25.392175Z",
     "start_time": "2023-03-13T00:16:25.132473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349 µs ± 44.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "f=0\n",
    "for i in range(n):\n",
    "    f+= w[i]*x[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa66fc",
   "metadata": {},
   "source": [
    "<p>From the above code cells, we can see that the vectorized operation is <u>100x faster</u> than sequential operation.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be5490c",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/vectorization_part2_2.png\" alt=\"Drawing\" style=\"width: 60%;\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e911b2",
   "metadata": {},
   "source": [
    "From above illustration, we can see that with vectorised implementation, the weights can be updated much faster in gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c6a0a",
   "metadata": {},
   "source": [
    "### Optional lab: Python, Numpy and vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255aa64",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient descent for multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda395b3",
   "metadata": {},
   "source": [
    "\n",
    "### Optional lab: Multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d75426",
   "metadata": {},
   "source": [
    "## Practice Quiz: Multiple linear regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b3a5c",
   "metadata": {},
   "source": [
    "## Gradient descent in practice\n",
    "### Feature scaling part 1\n",
    "### Feature scaling part 2\n",
    "### Checking gradient descent for convergence\n",
    "### Choosing the learning rate\n",
    "### Optional lab: Feature scaling and learning rate\n",
    "### Feature engineering\n",
    "### Polynomial regression\n",
    "### Optional lab: Feature engineering and Polynomial regression\n",
    "### Optional lab: Linear regression with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171dd2f8",
   "metadata": {},
   "source": [
    "## Practice quiz: Gradient descent in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e698e3",
   "metadata": {},
   "source": [
    "## Week 2 practice lab: Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888220f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ga]",
   "language": "python",
   "name": "conda-env-ga-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
