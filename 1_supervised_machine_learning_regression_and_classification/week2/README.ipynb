{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16f858f",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Week-2:-Regression-with-multiple-input-variable\" data-toc-modified-id=\"Week-2:-Regression-with-multiple-input-variable-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Week 2: Regression with multiple input variable</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multiple-linear-regression\" data-toc-modified-id=\"Multiple-linear-regression-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Multiple linear regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multiple-features\" data-toc-modified-id=\"Multiple-features-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Multiple features</a></span></li><li><span><a href=\"#Vectorization-part-1\" data-toc-modified-id=\"Vectorization-part-1-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Vectorization part 1</a></span></li><li><span><a href=\"#Vectorization-part-2\" data-toc-modified-id=\"Vectorization-part-2-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Vectorization part 2</a></span></li><li><span><a href=\"#Optional-lab:-Python,-Numpy-and-vectorization\" data-toc-modified-id=\"Optional-lab:-Python,-Numpy-and-vectorization-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Optional lab: Python, Numpy and vectorization</a></span></li><li><span><a href=\"#Gradient-descent-for-multiple-linear-regression\" data-toc-modified-id=\"Gradient-descent-for-multiple-linear-regression-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>Gradient descent for multiple linear regression</a></span></li><li><span><a href=\"#Optional-lab:-Multiple-linear-regression\" data-toc-modified-id=\"Optional-lab:-Multiple-linear-regression-1.1.6\"><span class=\"toc-item-num\">1.1.6&nbsp;&nbsp;</span>Optional lab: Multiple linear regression</a></span></li></ul></li><li><span><a href=\"#Practice-Quiz:-Multiple-linear-regression\" data-toc-modified-id=\"Practice-Quiz:-Multiple-linear-regression-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Practice Quiz: Multiple linear regression</a></span></li><li><span><a href=\"#Gradient-descent-in-practice\" data-toc-modified-id=\"Gradient-descent-in-practice-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Gradient descent in practice</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-scaling-part-1\" data-toc-modified-id=\"Feature-scaling-part-1-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Feature scaling part 1</a></span></li><li><span><a href=\"#Feature-scaling-part-2\" data-toc-modified-id=\"Feature-scaling-part-2-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Feature scaling part 2</a></span></li><li><span><a href=\"#Checking-gradient-descent-for-convergence\" data-toc-modified-id=\"Checking-gradient-descent-for-convergence-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Checking gradient descent for convergence</a></span></li><li><span><a href=\"#Choosing-the-learning-rate\" data-toc-modified-id=\"Choosing-the-learning-rate-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Choosing the learning rate</a></span></li><li><span><a href=\"#Optional-lab:-Feature-scaling-and-learning-rate\" data-toc-modified-id=\"Optional-lab:-Feature-scaling-and-learning-rate-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Optional lab: Feature scaling and learning rate</a></span></li><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-1.3.6\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>Feature engineering</a></span></li><li><span><a href=\"#Polynomial-regression\" data-toc-modified-id=\"Polynomial-regression-1.3.7\"><span class=\"toc-item-num\">1.3.7&nbsp;&nbsp;</span>Polynomial regression</a></span></li><li><span><a href=\"#Optional-lab:-Feature-engineering-and-Polynomial-regression\" data-toc-modified-id=\"Optional-lab:-Feature-engineering-and-Polynomial-regression-1.3.8\"><span class=\"toc-item-num\">1.3.8&nbsp;&nbsp;</span>Optional lab: Feature engineering and Polynomial regression</a></span></li><li><span><a href=\"#Optional-lab:-Linear-regression-with-scikit-learn\" data-toc-modified-id=\"Optional-lab:-Linear-regression-with-scikit-learn-1.3.9\"><span class=\"toc-item-num\">1.3.9&nbsp;&nbsp;</span>Optional lab: Linear regression with scikit-learn</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Gradient-descent-in-practice\" data-toc-modified-id=\"Practice-quiz:-Gradient-descent-in-practice-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Practice quiz: Gradient descent in practice</a></span></li><li><span><a href=\"#Week-2-practice-lab:-Linear-regression\" data-toc-modified-id=\"Week-2-practice-lab:-Linear-regression-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Week 2 practice lab: Linear regression</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0c86d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T22:40:15.173425Z",
     "start_time": "2023-03-12T22:40:15.157345Z"
    }
   },
   "source": [
    "# Week 2: Regression with multiple input variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99be304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T22:40:48.421724Z",
     "start_time": "2023-03-12T22:40:48.406390Z"
    }
   },
   "source": [
    "## Multiple linear regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37efbf46",
   "metadata": {},
   "source": [
    "### Multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb34bd",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/multiple_features_1.png\" alt=\"Drawing\" style=\"width: 40%;\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec41c9",
   "metadata": {},
   "source": [
    "In our original version of our linear regression, we had a single feature $x$, the size of the house, and we are able to predict $y$, the price of the house. So, the model was\n",
    "$$f_{w,b}(x) = wx+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67c6cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T23:20:22.904354Z",
     "start_time": "2023-03-12T23:20:22.895787Z"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"attachments/multiple_features_2.png\" alt=\"Drawing\" style=\"width: 45%;\"/></div>\n",
    "Now, we have more features that gives us lot more info to predict the price of our house."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d7368",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T23:30:11.771368Z",
     "start_time": "2023-03-12T23:30:11.749780Z"
    }
   },
   "source": [
    "Since, we have multiple features, let's learn bit more notations.\n",
    "- $x_{j}=$ $j^{th}$ feature $\\Rightarrow$ $j=1..4$ \n",
    "- $n=$ number of features  $\\Rightarrow$ $n=4$ \n",
    "- $\\mathbf{x}^{(i)}=$ features of $i^{th}$ training example (aka row vector) $\\Rightarrow$ $\\mathbf{x}^{(2)}=\\begin{bmatrix}1416 & 3 & 2 & 40 \\end{bmatrix}$\n",
    "    - Sometimes, $\\mathbf{x}^{(i)}$ can be expressed as $\\vec{x}^{(i)}$, but the arrow is optional.\n",
    "- $x^{(i)}_{j}=$ value of feature $j$ in $i^{th}$ training example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd9786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T23:39:26.482195Z",
     "start_time": "2023-03-12T23:39:26.466940Z"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"attachments/multiple_features_3.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "<img src=\"attachments/multiple_features_4.png\" alt=\"Drawing\" style=\"width: 40%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97b8bfe",
   "metadata": {},
   "source": [
    "Previously: $f_{w,b}(x) = wx+b$, as there was only **one feature for linear regression**\n",
    "\n",
    "Now: $f_{w,b}(x) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + w_{4}x_{4} + b$, as we have four features.\n",
    "\n",
    "If we generalise this, we will get **multiple linear regression**\n",
    "> $$f_{\\vec{w},b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x}+b = w_{1}x_{1} + w_{2}x_{2} + ... + w_{n}x_{n} + b$$\n",
    "\n",
    "where\n",
    "- parameters of the model are\n",
    "    - $\\vec{w} = \\begin{bmatrix}w_{1} & w_{2} & w_{3} & ... & w_{n} \\end{bmatrix}$ is a vactor\n",
    "    - $b$ is a number\n",
    "- $\\vec{x} = \\begin{bmatrix}x_{1} & x_{2} & x_{3} & ... & x_{n} \\end{bmatrix}$ is a vector\n",
    "- $\\vec{w}\\cdot\\vec{x}$ refers to dot product between vectors $w$ and $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10aca90",
   "metadata": {},
   "source": [
    "### Vectorization part 1\n",
    "<div align=\"center\"><img src=\"attachments/vectorization_part1.png\" alt=\"Drawing\" style=\"width: 60%;\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd10c63",
   "metadata": {},
   "source": [
    "We can use vectorization to make our code shorter and more efficient/faster as explained above. For e.g., `np.dot(w,x)` uses parallel hardware instead of sequential calculation.\n",
    "\n",
    "Note on **Indexing**\n",
    "- In linear algebra, indexing starts from 1\n",
    "- In Python, indexing starts from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e184f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:03:00.421849Z",
     "start_time": "2023-03-13T00:03:00.404011Z"
    }
   },
   "source": [
    "### Vectorization part 2\n",
    "\n",
    "<div align=\"center\"><img src=\"attachments/vectorization_part2_1.png\" alt=\"Drawing\" style=\"width: 60%;\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4684b3b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:15:03.812242Z",
     "start_time": "2023-03-13T00:15:03.794927Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(1000)\n",
    "w = np.random.rand(1000)\n",
    "n = len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa690b1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:16:24.907011Z",
     "start_time": "2023-03-13T00:16:24.894692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.45 µs ± 1.64 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "f = np.dot(x,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f40b91fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:16:25.392175Z",
     "start_time": "2023-03-13T00:16:25.132473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349 µs ± 44.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "f=0\n",
    "for i in range(n):\n",
    "    f+= w[i]*x[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d74e28",
   "metadata": {},
   "source": [
    "<p>From the above code cells, we can see that the vectorized operation is <u>100x faster</u> than sequential operation.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee611a6",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/vectorization_part2_2.png\" alt=\"Drawing\" style=\"width: 60%;\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc36226",
   "metadata": {},
   "source": [
    "From above illustration, we can see that with vectorised implementation, the weights can be updated much faster in gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca820d2b",
   "metadata": {},
   "source": [
    "### Optional lab: Python, Numpy and vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02505b76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T01:42:58.942090Z",
     "start_time": "2023-03-13T01:42:58.934192Z"
    }
   },
   "source": [
    "Refer to [greyhat](greyhatguy007/Optional%20Labs/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb) solutions for now\n",
    "\n",
    "Note on notations:\n",
    "- Vectors are one dimensional arrays. Vectors are denoted with lower case bold letters such as $\\mathbf{x}$.\n",
    "- Matrices, are two dimensional arrays. Matrices are denoted with capitol, bold letter such as $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baef2b81",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient descent for multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de46d87",
   "metadata": {},
   "source": [
    "Gradient descent for multiple linear regression with and without **vectorization**\n",
    "\n",
    "<div align=\"center\"><img src=\"attachments/gradient_descent_for_multiple_linear_regression_1.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "<img src=\"attachments/gradient_descent_for_multiple_linear_regression_2.png\" alt=\"Drawing\" style=\"width: 45%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e4e041",
   "metadata": {},
   "source": [
    "| Details | Previous notation (without vectorization) | Vector notation |\n",
    "| -: | :- | :- |\n",
    "| Parameters | $w_1, ..., w_n$ <br>$b$| $\\vec{w}=[w_1 \\dotsc w_n]$<br>$b$ |\n",
    "| Model | $f_{\\vec{w},b}(\\vec{x}) = w_{1}x_{1} + \\dotsc + w_{n}x_{n} + b$ | $f_{\\vec{w},b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x}+b$ |\n",
    "| Cost function | $J(w_1, \\dotsc, w_n, b)$ | $J(\\vec{w},b)$ |\n",
    "| Gradient descent | repeat {<br>$\\; w_j = w_j -\\alpha\\frac{\\partial }{\\partial w_{j}}J(w_1, \\dotsc, w_n, b)$ <br> $\\; b = b -\\alpha\\frac{\\partial }{\\partial b}J(w_1, \\dotsc, w_n, b)$ <br>}| repeat {<br>$\\; w_j = w_j -\\alpha\\frac{\\partial }{\\partial w_{j}}J(\\vec{w}, b)$ <br> $\\; b = b -\\alpha\\frac{\\partial }{\\partial b}J(\\vec{w}, b)$ <br>} |\n",
    "| $$$$ | $$$$ | $$$$ |\n",
    "\n",
    "where \n",
    "- $\\vec{w}$ is vector of length $n$, and $n$ represents to number of columns/features\n",
    "- $b$ is still a number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bc367b",
   "metadata": {},
   "source": [
    "Now, let's see what this looks like when we implement **gradient descent**. In particular, let's look at the derivative term when we have single feature vs. multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed17782",
   "metadata": {},
   "source": [
    "| One feature | $n$ features ($n \\ge 2$) |\n",
    "| :--- | :--- |\n",
    "| repeat{ |  repeat{|\n",
    "| $w = w - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{w,b})(x^{(i)}-y^{(i)})x^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w}J(w,b)$ | $w_1 = w_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b})(\\vec{x}^{(i)}-y^{(i)})x_{1}^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w_1}J(\\vec{w},b)$ <br> $\\vdots$ <br> $w_n = w_n - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b})(\\vec{x}^{(i)}-y^{(i)})x_{n}^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w_n}J(\\vec{w},b)$ |\n",
    "| $b = b - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{w,b})(x^{(i)}-y^{(i)}) = b - \\alpha\\frac{\\partial}{\\partial b}J(w,b)$ | $b = b - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b})(\\vec{x}^{(i)}-y^{(i)}) = b - \\alpha\\frac{\\partial}{\\partial b}J(\\vec{w},b)$ |\n",
    "| simultaneously update <br> $w,b $ <br>}|  simultaneously update <br> $w_j$ for $(j=1, \\dotsc, n)$ and $b$<br>} |\n",
    "| $$$$ | $$$$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4a1ceb",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/gradient_descent_for_multiple_linear_regression_3.png\" alt=\"Drawing\" style=\"width: 55%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ba8f6",
   "metadata": {},
   "source": [
    "\n",
    "### Optional lab: Multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913ee096",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Optional%20Labs/C1_W2_Lab02_Multiple_Variable_Soln.ipynb) solutions and [my practice](greyhatguy007/Optional%20Labs/C1_W2_Lab02_Multiple_Variable_Practice_TP.ipynb) for now\n",
    "\n",
    "Note on notations:\n",
    "- Here is a summary of some of the notation we will encounter, updated for multiple features.  \n",
    "\n",
    "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalar, non bold                                                      ||\n",
    "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
    "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | training example maxtrix                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c647cb0b",
   "metadata": {},
   "source": [
    "## Practice Quiz: Multiple linear regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5438df",
   "metadata": {},
   "source": [
    "## Gradient descent in practice\n",
    "### Feature scaling part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d3e31",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/feature_scaling_part_1_1.png\" alt=\"Drawing\" style=\"width: 50%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ea091",
   "metadata": {},
   "source": [
    "- When possible range of **feature values are large**, it's more likely that a good model will learn to choose a relatively **small parameter value**, like 0.1.\n",
    "    - e.g., House size, $x_1=2000$ sq. feet and $w_1 = 0.1$\n",
    "- Likewise, when possible **feature values are small**, the reasonable value for these **parameters will be relatively large**\n",
    "    - e.g., No. of bedrooms, $x_2=5$ and $w_2 = 50$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eeab12",
   "metadata": {},
   "source": [
    "**How does this relate to gradient descent?**\n",
    "\n",
    "- Let's look at the scatter plot of the features where size (in sq. feet) is the horiontal axis, and the number of bedroom is on the vertical axis. \n",
    "    - If we plot the training data, we notice that *horizontal axis is on a much large scale/range of values compared to vertical axis*\n",
    "- Let's then look at the contour plot of cost function, where $w_1$ is the horizontal axis, and $w_2$ is the vertical axis. \n",
    "    - Here, *horizontal axis has much narrower range, whereas vertical axis takes on much larger values*. $\\Rightarrow$ Contours form ovals/applises and they are short on one side and long on other.\n",
    "        - This is because a **small change in $w_1$ can have a large impact on estimated price** as it is multiplied with a very large value $x_1$ $\\Rightarrow$ Very large impact on cost $J$\n",
    "        - In contrast, it takes much larger change in $w_2$ in order to change predictions much.  $\\Rightarrow$  **small changes to $w_2$ don't change the cost function nearly that much**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3297950f",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/feature_scaling_part_1_2.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "    <img src=\"attachments/feature_scaling_part_1_3.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caffefdc",
   "metadata": {},
   "source": [
    "The problam with skewed contours is that the gradient descent may end up taking long time before it can find its way to global minima. In this situation, a useful thing to do is to <span style=\"color:green\"><b>scale the features</b></span> by performing some transformation of our training data so that the features range from 0 to 1. If we run gradient descent on a cost function to find on this scaled features, then contours will look more like circles and less skewed $\\Rightarrow$ <span style=\"color:green\"><b>Gradient descent will run faster</b></span> as it can find a much more direct path to global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc1497",
   "metadata": {},
   "source": [
    "### Feature scaling part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c632c4",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/feature_scaling_part_2_1.png\" alt=\"Drawing\" style=\"width: 50%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce4d9a",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/feature_scaling_part_2_2.png\" alt=\"Drawing\" style=\"width: 50%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ca374",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/feature_scaling_part_2_3.png\" alt=\"Drawing\" style=\"width: 50%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac670f1",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/feature_scaling_part_2_4.png\" alt=\"Drawing\" style=\"width: 50%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9367f4a6",
   "metadata": {},
   "source": [
    "### Checking gradient descent for convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c48ca3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b0b2f3",
   "metadata": {},
   "source": [
    "### Choosing the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b8a88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5082026b",
   "metadata": {},
   "source": [
    "### Optional lab: Feature scaling and learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8de5c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c35fba6",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b0a05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ee9eff1",
   "metadata": {},
   "source": [
    "### Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb94fa88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "649da48c",
   "metadata": {},
   "source": [
    "### Optional lab: Feature engineering and Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb1551",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65f6e370",
   "metadata": {},
   "source": [
    "### Optional lab: Linear regression with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e818d29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c0370ea",
   "metadata": {},
   "source": [
    "## Practice quiz: Gradient descent in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2d5ed",
   "metadata": {},
   "source": [
    "## Week 2 practice lab: Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7209ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ga]",
   "language": "python",
   "name": "conda-env-ga-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
