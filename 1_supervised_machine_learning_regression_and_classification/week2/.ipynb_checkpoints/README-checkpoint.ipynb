{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ed828b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Week-2:-Regression-with-multiple-input-variable\" data-toc-modified-id=\"Week-2:-Regression-with-multiple-input-variable-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Week 2: Regression with multiple input variable</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multiple-linear-regression\" data-toc-modified-id=\"Multiple-linear-regression-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Multiple linear regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multiple-features\" data-toc-modified-id=\"Multiple-features-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Multiple features</a></span></li><li><span><a href=\"#Vectorization-part-1\" data-toc-modified-id=\"Vectorization-part-1-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Vectorization part 1</a></span></li><li><span><a href=\"#Vectorization-part-2\" data-toc-modified-id=\"Vectorization-part-2-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Vectorization part 2</a></span></li><li><span><a href=\"#Optional-lab:-Python,-Numpy-and-vectorization\" data-toc-modified-id=\"Optional-lab:-Python,-Numpy-and-vectorization-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Optional lab: Python, Numpy and vectorization</a></span></li><li><span><a href=\"#Gradient-descent-for-multiple-linear-regression\" data-toc-modified-id=\"Gradient-descent-for-multiple-linear-regression-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>Gradient descent for multiple linear regression</a></span></li><li><span><a href=\"#Optional-lab:-Multiple-linear-regression\" data-toc-modified-id=\"Optional-lab:-Multiple-linear-regression-1.1.6\"><span class=\"toc-item-num\">1.1.6&nbsp;&nbsp;</span>Optional lab: Multiple linear regression</a></span></li></ul></li><li><span><a href=\"#Practice-Quiz:-Multiple-linear-regression\" data-toc-modified-id=\"Practice-Quiz:-Multiple-linear-regression-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Practice Quiz: Multiple linear regression</a></span></li><li><span><a href=\"#Gradient-descent-in-practice\" data-toc-modified-id=\"Gradient-descent-in-practice-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Gradient descent in practice</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-scaling-part-1\" data-toc-modified-id=\"Feature-scaling-part-1-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Feature scaling part 1</a></span></li><li><span><a href=\"#Feature-scaling-part-2\" data-toc-modified-id=\"Feature-scaling-part-2-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Feature scaling part 2</a></span></li><li><span><a href=\"#Checking-gradient-descent-for-convergence\" data-toc-modified-id=\"Checking-gradient-descent-for-convergence-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Checking gradient descent for convergence</a></span></li><li><span><a href=\"#Choosing-the-learning-rate\" data-toc-modified-id=\"Choosing-the-learning-rate-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Choosing the learning rate</a></span></li><li><span><a href=\"#Optional-lab:-Feature-scaling-and-learning-rate\" data-toc-modified-id=\"Optional-lab:-Feature-scaling-and-learning-rate-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Optional lab: Feature scaling and learning rate</a></span></li><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-1.3.6\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>Feature engineering</a></span></li><li><span><a href=\"#Polynomial-regression\" data-toc-modified-id=\"Polynomial-regression-1.3.7\"><span class=\"toc-item-num\">1.3.7&nbsp;&nbsp;</span>Polynomial regression</a></span></li><li><span><a href=\"#Optional-lab:-Feature-engineering-and-Polynomial-regression\" data-toc-modified-id=\"Optional-lab:-Feature-engineering-and-Polynomial-regression-1.3.8\"><span class=\"toc-item-num\">1.3.8&nbsp;&nbsp;</span>Optional lab: Feature engineering and Polynomial regression</a></span></li><li><span><a href=\"#Optional-lab:-Linear-regression-with-scikit-learn\" data-toc-modified-id=\"Optional-lab:-Linear-regression-with-scikit-learn-1.3.9\"><span class=\"toc-item-num\">1.3.9&nbsp;&nbsp;</span>Optional lab: Linear regression with scikit-learn</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Gradient-descent-in-practice\" data-toc-modified-id=\"Practice-quiz:-Gradient-descent-in-practice-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Practice quiz: Gradient descent in practice</a></span></li><li><span><a href=\"#Week-2-practice-lab:-Linear-regression\" data-toc-modified-id=\"Week-2-practice-lab:-Linear-regression-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Week 2 practice lab: Linear regression</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e533bbe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T22:40:15.173425Z",
     "start_time": "2023-03-12T22:40:15.157345Z"
    }
   },
   "source": [
    "# Week 2: Regression with multiple input variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851d017",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T22:40:48.421724Z",
     "start_time": "2023-03-12T22:40:48.406390Z"
    }
   },
   "source": [
    "## Multiple linear regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22646f0b",
   "metadata": {},
   "source": [
    "### Multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2398df",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/multiple_features_1.png\" alt=\"Drawing\" style=\"width: 40%;\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef5ce01",
   "metadata": {},
   "source": [
    "In our original version of our linear regression, we had a single feature $x$, the size of the house, and we are able to predict $y$, the price of the house. So, the model was\n",
    "$$f_{w,b}(x) = wx+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d5e8f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T23:20:22.904354Z",
     "start_time": "2023-03-12T23:20:22.895787Z"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"attachments/multiple_features_2.png\" alt=\"Drawing\" style=\"width: 45%;\"/></div>\n",
    "Now, we have more features that gives us lot more info to predict the price of our house."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a95d014",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T23:30:11.771368Z",
     "start_time": "2023-03-12T23:30:11.749780Z"
    }
   },
   "source": [
    "Since, we have multiple features, let's learn bit more notations.\n",
    "- $x_{j}=$ $j^{th}$ feature $\\Rightarrow$ $j=1..4$ \n",
    "- $n=$ number of features  $\\Rightarrow$ $n=4$ \n",
    "- $\\mathbf{x}^{(i)}=$ features of $i^{th}$ training example (aka row vector) $\\Rightarrow$ $\\mathbf{x}^{(2)}=\\begin{bmatrix}1416 & 3 & 2 & 40 \\end{bmatrix}$\n",
    "    - Sometimes, $\\mathbf{x}^{(i)}$ can be expressed as $\\vec{x}^{(i)}$, but the arrow is optional.\n",
    "- $x^{(i)}_{j}=$ value of feature $j$ in $i^{th}$ training example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccaca72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-12T23:39:26.482195Z",
     "start_time": "2023-03-12T23:39:26.466940Z"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"attachments/multiple_features_3.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "<img src=\"attachments/multiple_features_4.png\" alt=\"Drawing\" style=\"width: 40%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dbb09c",
   "metadata": {},
   "source": [
    "Previously: $f_{w,b}(x) = wx+b$, as there was only **one feature for linear regression**\n",
    "\n",
    "Now: $f_{w,b}(x) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + w_{4}x_{4} + b$, as we have four features.\n",
    "\n",
    "If we generalise this, we will get **multiple linear regression**\n",
    "> $$f_{\\vec{w},b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x}+b = w_{1}x_{1} + w_{2}x_{2} + ... + w_{n}x_{n} + b$$\n",
    "\n",
    "where\n",
    "- parameters of the model are\n",
    "    - $\\vec{w} = \\begin{bmatrix}w_{1} & w_{2} & w_{3} & ... & w_{n} \\end{bmatrix}$ is a vactor\n",
    "    - $b$ is a number\n",
    "- $\\vec{x} = \\begin{bmatrix}x_{1} & x_{2} & x_{3} & ... & x_{n} \\end{bmatrix}$ is a vector\n",
    "- $\\vec{w}\\cdot\\vec{x}$ refers to dot product between vectors $w$ and $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc796639",
   "metadata": {},
   "source": [
    "### Vectorization part 1\n",
    "<div align=\"center\"><img src=\"attachments/vectorization_part1.png\" alt=\"Drawing\" style=\"width: 60%;\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03081efd",
   "metadata": {},
   "source": [
    "We can use vectorization to make our code shorter and more efficient/faster as explained above. For e.g., `np.dot(w,x)` uses parallel hardware instead of sequential calculation.\n",
    "\n",
    "Note on **Indexing**\n",
    "- In linear algebra, indexing starts from 1\n",
    "- In Python, indexing starts from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61021796",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:03:00.421849Z",
     "start_time": "2023-03-13T00:03:00.404011Z"
    }
   },
   "source": [
    "### Vectorization part 2\n",
    "\n",
    "<div align=\"center\"><img src=\"attachments/vectorization_part2_1.png\" alt=\"Drawing\" style=\"width: 60%;\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f88893",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:15:03.812242Z",
     "start_time": "2023-03-13T00:15:03.794927Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(1000)\n",
    "w = np.random.rand(1000)\n",
    "n = len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2db2a04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:16:24.907011Z",
     "start_time": "2023-03-13T00:16:24.894692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.45 µs ± 1.64 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "f = np.dot(x,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80421a63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T00:16:25.392175Z",
     "start_time": "2023-03-13T00:16:25.132473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349 µs ± 44.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100\n",
    "f=0\n",
    "for i in range(n):\n",
    "    f+= w[i]*x[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42fa920",
   "metadata": {},
   "source": [
    "<p>From the above code cells, we can see that the vectorized operation is <u>100x faster</u> than sequential operation.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b7da9",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/vectorization_part2_2.png\" alt=\"Drawing\" style=\"width: 60%;\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6fa24",
   "metadata": {},
   "source": [
    "From above illustration, we can see that with vectorised implementation, the weights can be updated much faster in gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563835f",
   "metadata": {},
   "source": [
    "### Optional lab: Python, Numpy and vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c0a9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T01:42:58.942090Z",
     "start_time": "2023-03-13T01:42:58.934192Z"
    }
   },
   "source": [
    "Refer to [greyhat](greyhatguy007/Optional%20Labs/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb) solutions for now\n",
    "\n",
    "Note on notations:\n",
    "- Vectors are one dimensional arrays. Vectors are denoted with lower case bold letters such as $\\mathbf{x}$.\n",
    "- Matrices, are two dimensional arrays. Matrices are denoted with capitol, bold letter such as $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be4178",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient descent for multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b3fce",
   "metadata": {},
   "source": [
    "Gradient descent for multiple linear regression with and without **vectorization**\n",
    "\n",
    "<div align=\"center\"><img src=\"attachments/gradient_descent_for_multiple_linear_regression_1.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "<img src=\"attachments/gradient_descent_for_multiple_linear_regression_2.png\" alt=\"Drawing\" style=\"width: 45%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb772b41",
   "metadata": {},
   "source": [
    "| Details | Previous notation (without vectorization) | Vector notation |\n",
    "| -: | :- | :- |\n",
    "| Parameters | $w_1, ..., w_n$ <br>$b$| $\\vec{w}=[w_1 \\dotsc w_n]$<br>$b$ |\n",
    "| Model | $f_{\\vec{w},b}(\\vec{x}) = w_{1}x_{1} + \\dotsc + w_{n}x_{n} + b$ | $f_{\\vec{w},b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x}+b$ |\n",
    "| Cost function | $J(w_1, \\dotsc, w_n, b)$ | $J(\\vec{w},b)$ |\n",
    "| Gradient descent | repeat {<br>$\\; w_j = w_j -\\alpha\\frac{\\partial }{\\partial w_{j}}J(w_1, \\dotsc, w_n, b)$ <br> $\\; b = b -\\alpha\\frac{\\partial }{\\partial b}J(w_1, \\dotsc, w_n, b)$ <br>}| repeat {<br>$\\; w_j = w_j -\\alpha\\frac{\\partial }{\\partial w_{j}}J(\\vec{w}, b)$ <br> $\\; b = b -\\alpha\\frac{\\partial }{\\partial b}J(\\vec{w}, b)$ <br>} |\n",
    "| $$$$ | $$$$ | $$$$ |\n",
    "\n",
    "where \n",
    "- $\\vec{w}$ is vector of length $n$, and $n$ represents to number of columns/features\n",
    "- $b$ is still a number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f39307",
   "metadata": {},
   "source": [
    "Now, let's see what this looks like when we implement **gradient descent**. In particular, let's look at the derivative term when we have single feature vs. multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71654f72",
   "metadata": {},
   "source": [
    "| One feature | $n$ features ($n \\ge 2$) |\n",
    "| :--- | :--- |\n",
    "| repeat{ |  repeat{|\n",
    "| $w = w - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{w,b})(x^{(i)}-y^{(i)})x^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w}J(w,b)$ | $w_1 = w_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b})(\\vec{x}^{(i)}-y^{(i)})x_{1}^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w_1}J(\\vec{w},b)$ <br> $\\vdots$ <br> $w_n = w_n - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b})(\\vec{x}^{(i)}-y^{(i)})x_{n}^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w_n}J(\\vec{w},b)$ |\n",
    "| $b = b - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{w,b})(x^{(i)}-y^{(i)}) = b - \\alpha\\frac{\\partial}{\\partial b}J(w,b)$ | $b = b - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b})(\\vec{x}^{(i)}-y^{(i)}) = b - \\alpha\\frac{\\partial}{\\partial b}J(\\vec{w},b)$ |\n",
    "| simultaneously update <br> $w,b $ <br>}|  simultaneously update <br> $w_j$ for $(j=1, \\dotsc, n)$ and $b$<br>} |\n",
    "| $$$$ | $$$$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a7f47",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/gradient_descent_for_multiple_linear_regression_3.png\" alt=\"Drawing\" style=\"width: 55%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cd895c",
   "metadata": {},
   "source": [
    "\n",
    "### Optional lab: Multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c55efb",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Optional%20Labs/C1_W2_Lab02_Multiple_Variable_Soln.ipynb) solutions and [my practice](greyhatguy007/Optional%20Labs/C1_W2_Lab02_Multiple_Variable_Practice_TP.ipynb) for now\n",
    "\n",
    "Note on notations:\n",
    "- Here is a summary of some of the notation we will encounter, updated for multiple features.  \n",
    "\n",
    "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalar, non bold                                                      ||\n",
    "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
    "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | training example maxtrix                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95beceed",
   "metadata": {},
   "source": [
    "## Practice Quiz: Multiple linear regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7b4591",
   "metadata": {},
   "source": [
    "## Gradient descent in practice\n",
    "### Feature scaling part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07baa7fb",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/feature_scaling_part_1_1.png\" alt=\"Drawing\" style=\"width: 50%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8944b65",
   "metadata": {},
   "source": [
    "- When possible range of **feature values are large**, it's more likely that a good model will learn to choose a relatively **small parameter value**, like 0.1.\n",
    "    - e.g., House size, $x_1=2000$ sq. feet and $w_1 = 0.1$\n",
    "- Likewise, when possible **feature values are small**, the reasonable value for these **parameters will be relatively large**\n",
    "    - e.g., No. of bedrooms, $x_2=5$ and $w_2 = 50$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdef258",
   "metadata": {},
   "source": [
    "**How does this relate to gradient descent?**\n",
    "\n",
    "- Let's look at the scatter plot of the features where size (in sq. feet) is the horiontal axis, and the number of bedroom is on the vertical axis. \n",
    "    - If we plot the training data, we notice that *horizontal axis is on a much large scale/range of values compared to vertical axis*\n",
    "- Let's then look at the contour plot of cost function, where $w_1$ is the horizontal axis, and $w_2$ is the vertical axis. \n",
    "    - Here, *horizontal axis has much narrower range, whereas vertical axis takes on much larger values*. $\\Rightarrow$ Contours form ovals/applises and they are short on one side and long on other.\n",
    "        - This is because a **small change in $w_1$ can have a large impact on estimated price** as it is multiplied with a very large value $x_1$ $\\Rightarrow$ Very large impact on cost $J$\n",
    "        - In contrast, it takes much larger change in $w_2$ in order to change predictions much.  $\\Rightarrow$  **small changes to $w_2$ don't change the cost function nearly that much**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a98bc",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/feature_scaling_part_1_2.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "    <img src=\"attachments/feature_scaling_part_1_3.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35978be4",
   "metadata": {},
   "source": [
    "The problam with skewed contours is that the gradient descent may end up taking long time before it can find its way to global minima. In this situation, a useful thing to do is to <span style=\"color:green\"><b>scale the features</b></span> by performing some transformation of our training data so that the features range from 0 to 1. If we run gradient descent on a cost function to find on this scaled features, then contours will look more like circles and less skewed $\\Rightarrow$ <span style=\"color:green\"><b>Gradient descent will run faster</b></span> as it can find a much more direct path to global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b1f8e",
   "metadata": {},
   "source": [
    "### Feature scaling part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25afdc88",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/feature_scaling_part_2_1.png\" alt=\"Drawing\" style=\"width: 50%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf4d268",
   "metadata": {},
   "source": [
    "**Divide by maximum**: One way to scale the features is to take the maximum of the feature value and divide all the samples by the maximum as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f055f22",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/feature_scaling_part_2_2.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "    <img src=\"attachments/feature_scaling_part_2_3.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d73d53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T00:58:37.315084Z",
     "start_time": "2023-03-14T00:58:37.298439Z"
    }
   },
   "source": [
    "**Mean normalization**: Rescale the features so that they are cenetered around zero. \n",
    "\n",
    "To calculate mean normalization: \n",
    "$$x_1 = \\frac{x_1 - \\mu_{1}}{max(x_1)- min(x_1)} \\;\\;\\;\\; x_2 = \\frac{x_2 - \\mu_{2}}{max(x_2) - min(x_2)}$$\n",
    "\n",
    "where $u_1$ and $u_2$ are mean of $x_1$ and $x_2$ respectively\n",
    "\n",
    "**Z-score normalization**: Uses standard deviation $\\sigma$\n",
    "\n",
    "To calculate z-score normalization,\n",
    "$$x_1 = \\frac{x_1 - \\mu_{1}}{\\sigma_1} \\;\\;\\;\\; x_2 = \\frac{x_2 - \\mu_{2}}{\\sigma_2}$$\n",
    "\n",
    "\n",
    "where $u_1$ and $\\sigma_1$ are mean and standard deviation of $x_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351c40d",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><b><u>Rule of thumb for feature scaling</u></b></span>\n",
    "\n",
    "When in doubt, carry out the feature rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22191fc5",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"attachments/feature_scaling_part_2_4.png\" alt=\"Drawing\" style=\"width: 50%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af949baf",
   "metadata": {},
   "source": [
    "### Checking gradient descent for convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203693b",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\"><img src=\"attachments/checking_gradient_descent_for_convergence.png\" alt=\"Drawing\" style=\"width: 55%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c744a1",
   "metadata": {},
   "source": [
    "**How to ensure gradient descent is working correctly**\n",
    "\n",
    "- Objective: $\\underset{\\vec{w},b}{min}J(\\vec{w},b)$\n",
    "- Solution: \n",
    "    - We can plot the cost function over the number of iterations of gradient descent. This curve is aka **learning curve**\n",
    "    - <span style=\"color:green\">If gradient descent is working properly, the cost $J$ should decrease every single iteration</span>. <span style=\"color:red\">If the cost is increasing, it implies a bug in code or poor choice of $\\alpha$</span>\n",
    "    \n",
    "Note: The # of iterations needed for gradient descent to converge varies a lot by applications.\n",
    "Then, how do we decide when to stop the iterations? \n",
    "- **Automatic convergence test for gradient descent**: If $J(\\vec{w},b)$ decreases by $\\le \\epsilon$ in one iteration, declare *convergence* and stop the iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0c399",
   "metadata": {},
   "source": [
    "### Choosing the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9a47c",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"attachments/choosing_learning_rate_1.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "    <img src=\"attachments/choosing_learning_rate_2.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63df106",
   "metadata": {},
   "source": [
    "In the learning curve (cost vs. numbe of iterations), <span style=\"color:red\">if cost sometimes goes up and sometimes goes down, it implies that either gradient descent is not working properly or learning rate is too large</span>.\n",
    "\n",
    "**How to choose a proper learning rate for gradient descent?**\n",
    "- Ensure, that weights are properly updated ($w_1 \\neq w_1 + \\alpha d_1$, instead $w_1 = w_1 + \\alpha d_1$)\n",
    "- When the cost goes up or keeps wobbling, try smaller learning rate, and go down gradually.\n",
    "    - One way to debug correct implementation of gradient descent is that with small enough $\\alpha$, $J$ should decreas on every single iteration.\n",
    "    - Remember, setting $\\alpha$ to be very small value means gradient descent can take a lot of iterations to converge.\n",
    "    - Perhaps, we can try range of values for learning rate for every iterations\n",
    "        - Try 3x bigger $\\alpha$ for each new variant to find the largest possible learning rate\n",
    "            - e.g., $0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d8338",
   "metadata": {},
   "source": [
    "### Optional lab: Feature scaling and learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d14b57",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db650ee3",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7de844",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23f22e86",
   "metadata": {},
   "source": [
    "### Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d551a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "547bc712",
   "metadata": {},
   "source": [
    "### Optional lab: Feature engineering and Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac928dcb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81e2a039",
   "metadata": {},
   "source": [
    "### Optional lab: Linear regression with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b579bf5f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce771a64",
   "metadata": {},
   "source": [
    "## Practice quiz: Gradient descent in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2838edd",
   "metadata": {},
   "source": [
    "## Week 2 practice lab: Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd438f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ga]",
   "language": "python",
   "name": "conda-env-ga-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
