{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184bf8a3",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Mind-map\" data-toc-modified-id=\"Mind-map-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Mind map</a></span></li><li><span><a href=\"#C1:-Supervised-Machine-Learning:-Regression-and-Classification\" data-toc-modified-id=\"C1:-Supervised-Machine-Learning:-Regression-and-Classification-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>C1: Supervised Machine Learning: Regression and Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#C1-W1:-Introduction-to-ML\" data-toc-modified-id=\"C1-W1:-Introduction-to-ML-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>C1 W1: Introduction to ML</a></span><ul class=\"toc-item\"><li><span><a href=\"#Regression-Model\" data-toc-modified-id=\"Regression-Model-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span><strong>Regression Model</strong></a></span></li></ul></li></ul></li><li><span><a href=\"#C1-W2:-Regression-with-multiple-input-variable\" data-toc-modified-id=\"C1-W2:-Regression-with-multiple-input-variable-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>C1 W2: Regression with multiple input variable</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51fe8d7",
   "metadata": {},
   "source": [
    "# Mind map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9277d1a",
   "metadata": {},
   "source": [
    "- Viz: https://dreampuf.github.io/GraphvizOnline/# | https://graphs.grevian.org/graph/6002810497794048\n",
    "- Source: https://renenyffenegger.ch/notes/tools/Graphviz/examples/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a178b240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T03:10:01.408035Z",
     "start_time": "2023-03-21T03:10:01.401284Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pydot\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def view_pydot(pdot):\n",
    "    plt = Image(pdot.create_png())\n",
    "    display(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f3948f0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T04:23:55.732550Z",
     "start_time": "2023-03-21T04:23:55.716984Z"
    },
    "code_folding": [
     0,
     2,
     5
    ]
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Not used\n",
    "#Source: https://renenyffenegger.ch/notes/tools/Graphviz/examples/index\n",
    "        \n",
    "# dot_string = \"\"\"\n",
    "# digraph my_graph {\n",
    "\n",
    "#     node [shape=plaintext]\n",
    "    \n",
    "#     {rank=same; C1W1; C1W1_1; C1W1_2 ;C1W1_2; C1W1_3; C1W1_4}\n",
    "#         {rank=same; C1W1_2a; C1W1_3a; C1W1_4a}\n",
    "#     {rank=same; C1W2; C1W2_1; C1W2_2 ;C1W2_2; C1W2_3; C1W2_4}\n",
    "#         {rank=same; C1W2_2a; C1W2_3a; C1W2_4a}\n",
    "#     {rank=same; C1W3; C1W3_1; C1W3_2 ;C1W3_2; C1W3_3; C1W3_4}\n",
    "\n",
    "#     C1 [shape=box, label=\"Supervised learning:\\n Regression and Classification\"]\n",
    "#     C1W1 [shape=box label=\"Linear regression \\nwith single feature\\n(a.k.a) Univariate Linear Regression\"];\n",
    "#         C1W1_1 [label=\"Single variable\"];\n",
    "#         C1W1_2 [label=\"Model, f(x)\"];\n",
    "#             C1W1_2a [label=\"Straight line\"];\n",
    "#         C1W1_3 [label=\"Cost function, J\"];\n",
    "#         C1W1_4 [label=\"Gradient Descent, dJ/dw\"];\n",
    "#     C1W2 [shape=box label=\"Linear regression\\nwith multiple features\"];\n",
    "#         C1W2_1 [label=\"Single variable\"];\n",
    "#         C1W2_2 [label=\"Model, f(x)\"];\n",
    "#         C1W2_3 [label=\"Cost function, J\"];\n",
    "#         C1W2_4 [label=\"Gradient Descent, dJ/dw\"];\n",
    "#     C1W3 [shape=box label=\"Logistic regression\"];\n",
    "#         C1W3_1 [label=\"<b>Single variable</b>\"];\n",
    "#         C1W3_2 [label=\"Model, f(x)\"];\n",
    "#         C1W3_3 [label=\"Cost function, J\"];\n",
    "#         C1W3_4 [label=\"Gradient Descent, dJ/dw\"];\n",
    "    \n",
    "#     C1 -> C1W1 [penwidth=2];\n",
    "#     C1W1 -> C1W2;\n",
    "#     C1W2 -> C1W3;\n",
    "\n",
    "#     C1W1 -> C1W1_1 -> C1W1_2 -> C1W1_3 -> C1W1_4;\n",
    "    \n",
    "    \n",
    "#     subgraph cluster_C1W1 {\n",
    "#         C1W1;\n",
    "#         C1W1_1;\n",
    "#         C1W1_2;\n",
    "#         C1W1_3;\n",
    "#         C1W1_4;     \n",
    "        \n",
    "#         C1W1_2 -> C1W1_2a;\n",
    "#         C1W1_3 -> C1W1_3a;\n",
    "#         subgraph cluster_C1_{\n",
    "#             C1W1_2a;\n",
    "#             C1W1_3a;\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "    \n",
    "#     C1W2 -> C1W2_1 -> C1W2_2 -> C1W2_3 -> C1W2_4;\n",
    "#     C1W2_2 -> C1W2_2a;\n",
    "#     subgraph cluster_C1W2 {\n",
    "#         C1W2;\n",
    "#         C1W2_1;\n",
    "#         C1W2_2;\n",
    "#         C1W2_3;\n",
    "#         C1W2_4;\n",
    "#     }\n",
    "    \n",
    "#     C1W3 -> C1W3_1 -> C1W3_2 -> C1W3_3 -> C1W3_4;\n",
    "#     subgraph cluster_C1W3 {\n",
    "#         C1W3;\n",
    "#         C1W3_1;\n",
    "#         C1W3_2;\n",
    "#         C1W3_3;\n",
    "#         C1W3_4;\n",
    "#     }\n",
    "    \n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# graphs = pydot.graph_from_dot_data(dot_string)\n",
    "# graph = graphs[0]\n",
    "\n",
    "# graph.write_png(\"map.png\")\n",
    "# view_pydot(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "08c697a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T05:05:39.337494Z",
     "start_time": "2023-03-21T05:05:39.007220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dot_string = \"\"\"\n",
    "digraph D {\n",
    "\n",
    "  node [shape=plaintext fontsize=\"12\"];\n",
    "\n",
    "    {rank=same; C1W1; C1W1_1; C1W1_2; C1W1_3}\n",
    "    {rank=same; C1W2; C1W2_1; C1W2_2; C1W2_3; C1W2_4}\n",
    "    {rank=same; C1W3; C1W3_1; C1W3_2; C1W3_3}\n",
    "\n",
    "  C1W1 [ label=<\n",
    "   <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "     <tr><td align=\"left\"><b>C1 W1: Linear regression with single feature</b></td></tr>\n",
    "   </table>>];\n",
    "\n",
    "  C1W2 [ label=<\n",
    "   <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "     <tr><td align=\"left\"><b>C1 W2: Linear regression with multiple features</b></td></tr>\n",
    "   </table>>];\n",
    "\n",
    "  C1W3 [ label=<\n",
    "   <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "     <tr><td align=\"left\"><b>C1 W3: Logistic regression</b></td></tr>\n",
    "   </table>>];\n",
    "\n",
    "      C1W1_1 [ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b>Model, f(x)</b></td></tr>\n",
    "         <tr><td align=\"left\">- Straight line</td></tr>\n",
    "       </table>>];\n",
    "   \n",
    "      C1W1_2 [ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b>Cost function, J(w,b)</b></td></tr>\n",
    "         <tr><td align=\"left\">- Squared error</td></tr>\n",
    "       </table>>];\n",
    "   \n",
    "      C1W1_3 [ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b>Gradient Descent </b></td></tr>\n",
    "         <tr><td align=\"left\">- Learning rate</td></tr>\n",
    "          <tr><td align=\"left\">- Partial derivative</td></tr>\n",
    "          <tr><td align=\"left\">- Types: Batch, Stochastic, Mini-batch </td></tr>\n",
    "       </table>>];\n",
    "   \n",
    "      C1W2_1 [ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b> Feature scaling </b></td></tr>\n",
    "         <tr><td align=\"left\">  - Min-max norm</td></tr>\n",
    "         <tr><td align=\"left\">  - Mean norm</td></tr>\n",
    "         <tr><td align=\"left\">  - Z-norm</td></tr>\n",
    "\n",
    "       </table>>];\n",
    "   \n",
    "      C1W2_2 [ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b>Check gradient descent</b></td></tr>\n",
    "          <tr><td align=\"left\"><b>for convergence</b></td></tr>\n",
    "          <tr><td align=\"left\">- Learning curve</td></tr>\n",
    "          <tr><td align=\"left\">  - Set proper learning rate</td></tr>\n",
    "          <tr><td align=\"left\">  - Scale feature properly</td></tr>\n",
    "          <tr><td align=\"left\">- Auto-Stopping criterian</td></tr>\n",
    "       </table>>];\n",
    "   \n",
    "      C1W2_3 [ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b>Feature engineering</b></td></tr>\n",
    "         <tr><td align=\"left\">- Polynomial regression</td></tr>\n",
    "         <tr><td align=\"left\">- Feature scaling important</td></tr>\n",
    "       </table>>];\n",
    "\n",
    "      C1W2_4 [ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b>Closed form solution</b></td></tr>\n",
    "         <tr><td align=\"left\">- Alternative to gradient descent</td></tr>\n",
    "         <tr><td align=\"left\">- Faster or smaller data</td></tr>\n",
    "         <tr><td align=\"left\">- No need feature normalization</td></tr>\n",
    "\n",
    "       </table>>]\n",
    "       \n",
    "  C1W3_1[ label=<\n",
    "   <table border=\"\" cellborder=\"0\" cellspacing=\"1\">\n",
    "     <tr><td align=\"left\"><b>...</b></td></tr>\n",
    "     <tr><td align=\"left\">...</td></tr>\n",
    "     <tr><td align=\"left\"><font color=\"red\">...</font></td></tr>\n",
    "   </table>>];\n",
    "\n",
    "\n",
    "  C1W1 -> C1W2;\n",
    "  C1W2 -> C1W3;\n",
    "  \n",
    "  C1W1 -> C1W1_1 -> C1W1_2 -> C1W1_3;\n",
    "  C1W2 -> C1W2_1 -> C1W2_2 -> C1W2_3 -> C1W2_4;\n",
    "  C1W3 -> C1W3_1 -> C1W3_2 -> C1W3_3;\n",
    "\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "graphs = pydot.graph_from_dot_data(dot_string)\n",
    "graph = graphs[0]\n",
    "\n",
    "graph.write_png(\"mindmap/C1W1_mindmap.png\")\n",
    "view_pydot(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b934397",
   "metadata": {},
   "source": [
    "# C1: Supervised Machine Learning: Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c1525c",
   "metadata": {},
   "source": [
    "## C1 W1: Introduction to ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d8ad1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T01:02:07.261717Z",
     "start_time": "2023-03-21T01:02:07.249804Z"
    }
   },
   "source": [
    "**Machine Learning**\n",
    "- Study that gives computers ability to learn withotu being explicitly programmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c1592b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T01:02:33.527194Z",
     "start_time": "2023-03-21T01:02:33.516406Z"
    }
   },
   "source": [
    "**Machine learning types**\n",
    "- Supervised learning\n",
    "- Unsupervised learning\n",
    "- Recommender systems\n",
    "- Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f818357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T01:03:30.226691Z",
     "start_time": "2023-03-21T01:03:30.211077Z"
    }
   },
   "source": [
    "**Supervised learning**\n",
    "- Used in most real world applications. 99% of economic value created by ML today is through supervised learning\n",
    "- Algo that learn x (input) to y (output) mappings\n",
    "- Learns from data *labelled* with the \"right answers\"\n",
    "- Types\n",
    "    - Classification: Predicts categories \n",
    "    - Regression: Predicts a continuous number\n",
    "- Example\n",
    "    - Classification\n",
    "        - Breast cancer detection\n",
    "    - Regression\n",
    "        - Housing price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b22df",
   "metadata": {},
   "source": [
    "**Unspervised learning**\n",
    "- Find something interesting or structure in *unlabelled* data\n",
    "- Types\n",
    "    - Clustering\n",
    "    - Dimensionality Reduction\n",
    "    - Anomaly detection\n",
    "\n",
    "- Example\n",
    "    - Clustering\n",
    "        - Google news\n",
    "        - Grouping customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5eb7cb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd1c6d",
   "metadata": {},
   "source": [
    "### **Regression Model**\n",
    "\n",
    "**Linear regression model**\n",
    "\n",
    "Model that fits a straight line to our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad027a2",
   "metadata": {},
   "source": [
    "> Linear regression with one input variable is a.k.a. **Univariate linear regression**\n",
    "\n",
    "For this section, **we will assume that the linear regression has only one input feature (for simplicity)**. In the next section C1 W2, we will explore regression with multiple input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ef35f",
   "metadata": {},
   "source": [
    "**Function, $f$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e16ca",
   "metadata": {},
   "source": [
    "When we train the model with training data, it will provide a function $f$, sometimes called as *hypothesis*. The job of the function is to take, input $x$ and estimate or predict the output $y$. This predicted output is $\\hat{y}$.\n",
    "\n",
    "- $f$ - Function\n",
    "- $x$ - model input/features\n",
    "- $y$ - actual output/target\n",
    "- $\\hat{y}$ - predicted/ estimated output \n",
    "  - $\\hat{y} = f(x)$\n",
    "\n",
    "How to represent, $f$?\n",
    "- $f_{w, b}{(x)} = wx + b$, assuming the function is a straight line\n",
    "  - $f$ is a function that takes $x$ as input, and depending upon the values of $w$ and $b$, $f$  will output some values of the prediction, $\\hat{y}$.\n",
    "  - For simplicity, $f_{w, b}{(x)}$ will be represented as $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d9a387",
   "metadata": {},
   "source": [
    "**Cost function, $J$** - **Squared error** function for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d936d6",
   "metadata": {},
   "source": [
    "With linear regression, we want to choose value for parameters $w$ and $b$ such that the straight line we get from the function $f$ somehow fits the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0b4dd",
   "metadata": {},
   "source": [
    "How to find $w$ and $b$, such that predicted value $\\hat{y} _{i}$ is close to actual value $y _{i}$ for all $(x^{(i)}, y^{(i)})$ ?\n",
    "- To answer this question, let's first measure how well the line fits the training data using the cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0f4b6",
   "metadata": {},
   "source": [
    "$$ J(w,b) = \\frac{1}{2m}\\sum_{i=1}^{m}{(\\hat{y}_{i} - y_{i})}^{2}$$  \n",
    "$$ J(w,b) = \\frac{1}{2m}\\sum_{i=1}^{m}{(f_{w, b}{(x^{i})}  - y_{i})}^{2}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d0668a",
   "metadata": {},
   "source": [
    "Since $J$ is the cost function that measures how big the squared errors are, so choosing $w$ that minimizes these squared errors, makes them errors as small as possible, gives us a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e7e1dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T01:20:20.964026Z",
     "start_time": "2023-03-21T01:20:20.944222Z"
    }
   },
   "source": [
    "Note: \n",
    "- *For linear regression, the cost function is always squared error cost function.*\n",
    "- *However, for **not** linear regressions, the cost function is not squared error cost function.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3073fc98",
   "metadata": {},
   "source": [
    "> **Goal of linear regression**<br>\n",
    "> - Simplliefed case: $\\underset{w}{\\text{minimize}}\\phantom{1}J(w)$ <br>\n",
    "> - General case: $\\underset{w,b}{\\text{minimize}}\\phantom{1}J(w,b)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80dddbe",
   "metadata": {},
   "source": [
    "| Term | Org. equation | Simplified | |\n",
    "| :---- | :-------------: | :----------: | |\n",
    "| Model | $$f_{w, b}{(x^{(i)})} = wx^{(i)} + b$$ |  $$f_{w}{(x^{(i)})} = wx^{(i)}$$ by setting $b=0$ | |\n",
    "| Parameters | $w,b$ | $w$ | |\n",
    "| Cost function | $$J(w,b) = \\frac{1}{2m}\\sum_{i=1}^{m}{(f_{w, b}{x^{(i)}}-y^{(i)})}^{2}$$ |  $$J(w) = \\frac{1}{2m}\\sum_{i=1}^{m}{(f_{w}{x^{(i)}}-y^{(i)})}^{2}$$ ||\n",
    "| Goal | $\\underset{w,b}{\\text{minimize}}\\phantom{1}J(w,b)$ | $\\underset{w}{\\text{minimize}}\\phantom{1}J(w)$ ||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0f0dc",
   "metadata": {},
   "source": [
    "**Gradient Descent - Intuition and Algo**\n",
    "- Algo that can be used to find the optimal parameters $w$ and $b$ that minimises the cost function, $J(w,b)$.\n",
    "- Is used not only in linear regression, but also for training some of the most advanced neural network models, including deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc52190",
   "metadata": {},
   "source": [
    "In layman words, our goal is to start somewhere randomly in this cost function and get to the bottom of one of these valleys efficiently as possible.\n",
    "- What the gradient descent algorithm does is we are going to spin around 360 degrees and ask ourself, if I were to take a tiny little baby step in one direction, and I want to go downhill as quickly as possible to one of these valleys. $\\Rightarrow$ Mathematically, this is the **direction of the steepest descent**. It means that when we take a tiny baby little step, this takes us downhill faster than a tiny little baby step we could have taken in any other direction. Eventually, we might end up in the local minima after series of this baby steps. In this local minima, the cost will be small as possible => We would have found $w$ and $b$, such that predicted value $\\hat{y} _{i}$ is close to actual value $y _{i}$ for all $(x^{(i)}, y^{(i)})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df9ff9b",
   "metadata": {},
   "source": [
    "**Assuming there's only one input feature $x$,** \n",
    "\n",
    "- Linear regression model\n",
    "  $$f _{w,b}(x^{(i)})=wx^{(i)}+b \\tag{1}$$\n",
    "\n",
    "- Cost function for linear regression\n",
    "   $$J(w,b) = \\frac{1}{2m}\\sum _{i=1}^{m}(f _{w,b}(x^{(i)})-y^{(i)})^2 \\tag{2}$$\n",
    "\n",
    "- Gradient descent \n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace \n",
    "\\end{align*}$$\n",
    "    \n",
    "$\\phantom{.....}\\text{where, parameters }w, b \\text{ are updated simultaneously.}$  \n",
    "    \n",
    "- Gradient \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Here *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd6da09",
   "metadata": {},
   "source": [
    "where \n",
    "- $\\alpha$ is **learning rate** between $0$ and $1$, that controls how big of a step we take downhill $\\Rightarrow$ Huge $\\alpha$ corresponds to very aggressive gradient descent procedure as will take huge steps downhill.\n",
    "- $\\frac{\\partial}{\\partial w}J(w,b)$ is the **partial derivative** term of cost function $J$ with respect to parameter $w$. This derivative is utilized by the gradient descent to update the parameter . In layman terms, it tell us the direction (of the steepest descent) in which we want to take baby steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aac459",
   "metadata": {},
   "source": [
    "**Learning rate**\n",
    "- If $\\alpha$ is too small, gradient descent will work, but may be slow as it will be taking tiny tiny steps every steps  before it reaches the minima\n",
    "- If $\\alpha$ is too large, though the derivative might be a small value that points in the right direction, as the learning rate is multiplied with derivative, it will end up being a very large value and might overshoot. As we will be taking huge steps, we may never reach minimum. $\\Rightarrow$ Fail to converge, and may even diverge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4295f3ce",
   "metadata": {},
   "source": [
    "**Characteristics of Gradient Descent**\n",
    "- In general, the cost $J$ starts large and rapidly declines. The partial derivatives, $\\frac{\\partial}{\\partial{w}}J(w,b)$, and  $\\frac{\\partial}{\\partial{b}}J(w,b)$ also get smaller, rapidly at first and then more slowly. \n",
    "- As we get closer to the **local minimum** (reach the bottom of the bowl in layman terms), the gradient descent starts making **smaller steps**, because as we approach local minimum, the **derivative automatically gets smaller**. $\\Rightarrow$  **Update steps also gets smaller**.\n",
    "    - <span style=\"color:red\">For e.g., say $w$ and $b$ are bouncing back and forth between positive and negative with the absolute value increasing with each iteration. Further, for each iteration $\\frac{\\partial J(w,b)}{\\partial w}$ changes sign and cost is increasing rather than decreasing. Then. this is a clear sign that the *learning rate is too large* and the solution is diverging. </span>\n",
    "\n",
    "\n",
    "**Types of Gradient Descent** \n",
    "- **Batch gradient descent**: **\"Batch\"** implies that for each step of gradient descent uses all the training examples, instead of subset of training data. The name \"batch\" might be not that intuitive though.\n",
    "- There are other versions of gradient descent that do not look at the entire training set, but instead smaller subsets of training data for each update step. However, for linear regression, we will be using linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4bee7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5046e6",
   "metadata": {},
   "source": [
    "# C1 W2: Regression with multiple input variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533a1d0",
   "metadata": {},
   "source": [
    "Notations for multiple linear regression\n",
    "\n",
    "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalar, non bold                                                      ||\n",
    "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
    "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | training example maxtrix                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` |\n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
    "|  $\\mathbf{x}_{j}$ | $j_{th}$ feature | `X[:,j]`|\n",
    "|  $x_{j}^{(i)}$ | $j_{th}$ feature in $i_{th}$ training example| `X[i,j]`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0b1b9",
   "metadata": {},
   "source": [
    "**Gradient descent for multiple linear regression** with and without vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b08578",
   "metadata": {},
   "source": [
    "| Details | without vectorization | Vector notation |\n",
    "| -: | :- | :- |\n",
    "| Parameters | $w_1, ..., w_n$ <br>$b$| $\\vec{w}=[w_1 \\dotsc w_n]$<br>$b$ |\n",
    "| Model | $f_{\\vec{w},b}(\\vec{x}) = w_{1}x_{1} + \\dotsc + w_{n}x_{n} + b$ | $f_{\\vec{w},b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x}+b$ |\n",
    "| Cost function | $J(w_1, \\dotsc, w_n, b)$ | $J(\\vec{w},b)$ |\n",
    "| Gradient descent | repeat {<br>$\\; w_j = w_j -\\alpha\\frac{\\partial }{\\partial w_{j}}J(w_1, \\dotsc, w_n, b)$ <br> $\\; b = b -\\alpha\\frac{\\partial }{\\partial b}J(w_1, \\dotsc, w_n, b)$ <br>}| repeat {<br>$\\; w_j = w_j -\\alpha\\frac{\\partial }{\\partial w_{j}}J(\\vec{w}, b)$ <br> $\\; b = b -\\alpha\\frac{\\partial }{\\partial b}J(\\vec{w}, b)$ <br>} |\n",
    "| $$$$ | $$$$ | $$$$ |\n",
    "\n",
    "where \n",
    "- $\\vec{w}$ is vector of length $n$, and $n$ represents to number of columns/features\n",
    "- $b$ is still a number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2181f4",
   "metadata": {},
   "source": [
    "**Gradient descent and its derivative term when we have single feature vs. multiple features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1027f05e",
   "metadata": {},
   "source": [
    "| One feature | $n$ features ($n \\ge 2$) |\n",
    "| :--- | :--- |\n",
    "| repeat{ |  repeat{|\n",
    "| $w = w - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w}J(w,b)$ | $w_1 = w_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)})x_{1}^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w_1}J(\\vec{w},b)$ <br> $\\vdots$ <br> $w_n = w_n - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)})x_{n}^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w_n}J(\\vec{w},b)$ |\n",
    "| $b = b - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)}) = b - \\alpha\\frac{\\partial}{\\partial b}J(w,b)$ | $b = b - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)}) = b - \\alpha\\frac{\\partial}{\\partial b}J(\\vec{w},b)$ |\n",
    "| simultaneously update <br> $w,b $ <br>}|  simultaneously update <br> $w_j$ for $(j=1, \\dotsc, n)$ and $b$<br>} |\n",
    "| $$$$ | $$$$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf1d921",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca9821d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T02:18:19.396134Z",
     "start_time": "2023-03-21T02:18:19.387449Z"
    }
   },
   "source": [
    "**Feature Scaling** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8944b65",
   "metadata": {},
   "source": [
    "- When possible range of **feature values are large**, it's more likely that a good model will learn to choose a relatively **small parameter value**, like 0.1.\n",
    "    - e.g., House size, $x_1=2000$ sq. feet and $w_1 = 0.1$\n",
    "- Likewise, when possible **feature values are small**, the reasonable value for these **parameters will be relatively large**\n",
    "    - e.g., No. of bedrooms, $x_2=5$ and $w_2 = 50$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdef258",
   "metadata": {},
   "source": [
    "**Intution: How does feature size relate to or affect gradient descent?**\n",
    "\n",
    "- Let's look at the scatter plot of the features where size (in sq. feet) is the horiontal axis, and the number of bedroom is on the vertical axis. \n",
    "    - If we plot the training data, we notice that *horizontal axis is on a much large scale/range of values compared to vertical axis*\n",
    "- Let's then look at the contour plot of cost function, where $w_1$ is the horizontal axis, and $w_2$ is the vertical axis. \n",
    "    - Here, *horizontal axis has much narrower range, whereas vertical axis takes on much larger values*. $\\Rightarrow$ Contours form ovals/elipses and they are short on one side and long on other.\n",
    "        - This is because a **small change in $w_1$ can have a large impact on estimated price** as it is multiplied with a very large value $x_1$ $\\Rightarrow$ Very large impact on cost $J$\n",
    "        - In contrast, it takes much larger change in $w_2$ in order to change predictions much.  $\\Rightarrow$  **small changes to $w_2$ don't change the cost function nearly that much**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a98bc",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"week2/attachments/feature_scaling_part_1_2.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "    <img src=\"week2/attachments/feature_scaling_part_1_3.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35978be4",
   "metadata": {},
   "source": [
    "The problem with skewed contours is that the gradient descent may end up taking long time before it can find its way to global minima. In this situation, a useful thing to do is to <span style=\"color:green\"><b>scale the features</b></span> by performing some transformation of our training data so that the features range from 0 to 1. If we run gradient descent on a cost function to find on this scaled features, then contours will look more like circles and less skewed $\\Rightarrow$ <span style=\"color:green\"><b>Gradient descent will run faster</b></span> as it can find a much more direct path to global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca4b53",
   "metadata": {},
   "source": [
    "Other things to note:\n",
    "- When $w$'s are updated unevenly during training, it implies that the features vary significantly in magnitude making some features update much faster than others. \n",
    "    - **Why are the $w$'s updated unevenly**? \n",
    "        - This is because when the gradients are computed, the common error term $f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{i}$ is multiplied by the features $\\color{Red}{x_j^{(i)}}$ for the $w$'s. \n",
    "        - In the lab housing dataset, feature $w_0$ (size in sq.ft) is 2-3 orders of magnitude larger than other features.\n",
    "        - This makes the gradient bigger. When the weights are updated based on learning rate $\\alpha$ and gradient computed from partial derivative with respect to each feature, $w_0$ will make more rapid progress than the other parameters due to its much larger gradient. \n",
    "$$\\frac{\\partial}{\\partial w_j}J(\\vec{w},b) = \\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{i})\\color{Red}{x_{j}^{(i)}}$$\n",
    "- When the features are normalized, the cost contour is much more symmetric. The result is that updates to parameters during gradient descent can make equal progress for each parameter.<span style=\"color:green\"> $\\Rightarrow$ The scaled features get very accurate results much, much faster!. Also, the gradient of each parameter will be tiny by the end of this fairly short run (iterations)</span>. \n",
    "- A learning rate of 0.1 is a good start for regression with normalized features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356656f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T04:33:36.623558Z",
     "start_time": "2023-03-21T04:33:36.612787Z"
    }
   },
   "source": [
    "**Different feature scaling techniques**\n",
    "1. **Divide by maximum** (or) **Min-max normalization**: \n",
    "    - One way to scale the features is to take the maximum of the feature value and divide all the samples by the maximum as shown above.\n",
    "    - **More generally**: Rescale each feature by both its minimum and maximum value as show below\n",
    "$$x_1 = \\frac{x_1 - min(x_1)}{max(x_1)- min(x_1)} \\;\\;\\;\\; x_2 = \\frac{x_2 - min(x_2)}{max(x_2) - min(x_2)}$$\n",
    "    \n",
    "    Both ways normalizes features to the range of -1 and 1, where the former method works for positive features which is simple and serves well for the lecture's example, and the latter method works for any features.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d73d53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T00:58:37.315084Z",
     "start_time": "2023-03-14T00:58:37.298439Z"
    }
   },
   "source": [
    "2. **Mean normalization**: \n",
    "    - Rescale the features so that they are cenetered around zero. \n",
    "    - To calculate mean normalization: \n",
    "$$x_1 = \\frac{x_1 - \\mu_{1}}{max(x_1)- min(x_1)} \\;\\;\\;\\; x_2 = \\frac{x_2 - \\mu_{2}}{max(x_2) - min(x_2)}$$\n",
    "\n",
    "    where $u_1$ and $u_2$ are mean of $x_1$ and $x_2$ respectively\n",
    "\n",
    "3. **Z-score normalization**: \n",
    "    - Uses standard deviation $\\sigma$\n",
    "    - To calculate z-score normalization,\n",
    "$$x_1 = \\frac{x_1 - \\mu_{1}}{\\sigma_1} \\;\\;\\;\\; x_2 = \\frac{x_2 - \\mu_{2}}{\\sigma_2}$$\n",
    "\n",
    "    where $u_1$ and $\\sigma_1$ are mean and standard deviation of $x_1$\n",
    "\n",
    "    After z-score normalization, all features will have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351c40d",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><b><u>Rule of thumb for feature scaling</u></b></span>\n",
    "\n",
    "When in doubt, carry out the feature rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22191fc5",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"week2/attachments/feature_scaling_part_2_4.png\" alt=\"Drawing\" style=\"width: 50%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c744a1",
   "metadata": {},
   "source": [
    "**How to ensure gradient descent is working correctly**\n",
    "\n",
    "- Objective: $\\underset{\\vec{w},b}{min}J(\\vec{w},b)$\n",
    "- Solution: \n",
    "    - We can plot the cost function over the number of iterations of gradient descent. This curve is aka **learning curve**\n",
    "    - <span style=\"color:green\">If gradient descent is working properly, the cost $J$ should decrease every single iteration</span>. <span style=\"color:red\">If the cost is increasing, it implies a bug in code or poor choice of $\\alpha$</span>\n",
    "    \n",
    "Note: The # of iterations needed for gradient descent to converge varies a lot by applications.\n",
    "Then, how do we decide when to stop the iterations? \n",
    "- **Automatic convergence test for gradient descent aka Stopping Criterian**: If $J(\\vec{w},b)$ decreases by $\\le \\epsilon$ in one iteration, declare *convergence* and stop the iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63df106",
   "metadata": {},
   "source": [
    "**How to choose a proper learning rate for gradient descent?**\n",
    "\n",
    "In the learning curve (cost vs. numbe of iterations), <span style=\"color:red\">if cost sometimes goes up and sometimes goes down, it implies that either gradient descent is not working properly or learning rate is too large</span>.\n",
    "\n",
    "- Ensure, that weights are properly updated ($w_1 \\neq w_1 + \\alpha d_1$, instead $w_1 = w_1 - \\alpha d_1$)\n",
    "    - When $w$'s are updated unevenly during training, it may imply that the features vary significantly in magnitude making some features update much faster than others. So, scale of features might be an issue\n",
    "- When the cost goes up or keeps wobbling, try smaller learning rate, and go down gradually.\n",
    "    - One way to debug correct implementation of gradient descent is that with small enough $\\alpha$, $J$ should decrease on every single iteration.\n",
    "    - Remember, setting $\\alpha$ to be very small value means gradient descent can take a lot of iterations to converge.\n",
    "    - Perhaps, we can try range of values for learning rate for every iterations\n",
    "        - Try 3x bigger $\\alpha$ for each new variant to find the largest possible learning rate\n",
    "            - e.g., $0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ddbc0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5940c",
   "metadata": {},
   "source": [
    "**Feature engineering**: Using *intuition* or *knowledge* to design *new features*, by transforming or combining original features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524410a6",
   "metadata": {},
   "source": [
    "Polynomial regression\n",
    "- A flavor of feature engineering that allows us to fit not just straight lines, but curves (non-linear functions) to our data.\n",
    "- Combines the ideas of multiple linear regression and feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cdcbe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T23:58:31.980440Z",
     "start_time": "2023-03-16T23:58:31.971077Z"
    }
   },
   "source": [
    "So, how do decide which features to use? Will be addressed in Course 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac928dcb",
   "metadata": {},
   "source": [
    "Note:\n",
    "- Polynomial features will be chosen based on how well they matched the target data. Another way to think about this is to note that <span style=\"color:blue\">we are still using linear regression once we have created new (engineered) features. Given that, <b>the best new (engineered) features will be linear relative to the target</b></span>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba15e6e",
   "metadata": {},
   "source": [
    "Note\n",
    "- **Gradient Descent based**\n",
    "    - It is important to apply feature scaling when doing feature engineering with gradient descent based regression\n",
    "- **Closed form (Alternative to gradient descent)** \n",
    "    - The closed-form solution work well and faster on smaller data sets such as these but can be computationally demanding on larger data sets.\n",
    "    - The closed-form solution does not require normalization.\n",
    "    - An example of a closed form solution in linear regression would be the least square equation [Source](https://stats.stackexchange.com/questions/70848/what-does-a-closed-form-solution-mean)\n",
    "\n",
    "$$y = wX$$\n",
    "\n",
    "$$w=(X^{T}X)^{-1}X^{T}y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f9e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ga]",
   "language": "python",
   "name": "conda-env-ga-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
