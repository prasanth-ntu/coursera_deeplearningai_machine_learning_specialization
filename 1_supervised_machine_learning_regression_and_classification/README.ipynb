{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929c9915",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Mind-map\" data-toc-modified-id=\"Mind-map-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Mind map</a></span></li><li><span><a href=\"#C1:-Supervised-Machine-Learning:-Regression-and-Classification\" data-toc-modified-id=\"C1:-Supervised-Machine-Learning:-Regression-and-Classification-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>C1: Supervised Machine Learning: Regression and Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#C1-W1:-Introduction-to-ML\" data-toc-modified-id=\"C1-W1:-Introduction-to-ML-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>C1 W1: Introduction to ML</a></span><ul class=\"toc-item\"><li><span><a href=\"#Regression-Model\" data-toc-modified-id=\"Regression-Model-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Regression Model</a></span></li><li><span><a href=\"#Cost-function\" data-toc-modified-id=\"Cost-function-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Cost function</a></span></li><li><span><a href=\"#Gradient-Descent\" data-toc-modified-id=\"Gradient-Descent-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Gradient Descent</a></span></li></ul></li><li><span><a href=\"#C1-W2:-Regression-with-multiple-input-variable\" data-toc-modified-id=\"C1-W2:-Regression-with-multiple-input-variable-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>C1 W2: Regression with multiple input variable</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Scaling\" data-toc-modified-id=\"Feature-Scaling-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Feature Scaling</a></span></li><li><span><a href=\"#Check-gradient-descent-for-convergence\" data-toc-modified-id=\"Check-gradient-descent-for-convergence-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Check gradient descent for convergence</a></span></li><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Feature engineering</a></span></li><li><span><a href=\"#Closed-form-solution\" data-toc-modified-id=\"Closed-form-solution-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Closed form solution</a></span></li></ul></li><li><span><a href=\"#C1-W3:-Classification\" data-toc-modified-id=\"C1-W3:-Classification-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>C1 W3: Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#Cost-function\" data-toc-modified-id=\"Cost-function-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Cost function</a></span></li><li><span><a href=\"#Gradient-descent-for-logistic-regression\" data-toc-modified-id=\"Gradient-descent-for-logistic-regression-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Gradient descent for logistic regression</a></span></li><li><span><a href=\"#Overfitting-Problem\" data-toc-modified-id=\"Overfitting-Problem-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>Overfitting Problem</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2966622",
   "metadata": {},
   "source": [
    "# Mind map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42e029",
   "metadata": {},
   "source": [
    "- Viz: https://dreampuf.github.io/GraphvizOnline/# | https://graphs.grevian.org/graph/6002810497794048\n",
    "- Source: https://renenyffenegger.ch/notes/tools/Graphviz/examples/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f10e33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:51:12.503697Z",
     "start_time": "2023-04-02T22:51:12.436127Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pydot\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def view_pydot(pdot):\n",
    "    plt = Image(pdot.create_png())\n",
    "    display(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337111cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-02T22:51:12.854990Z",
     "start_time": "2023-04-02T22:51:12.840590Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# \n",
    "# Not used\n",
    "#Source: https://renenyffenegger.ch/notes/tools/Graphviz/examples/index\n",
    "        \n",
    "# dot_string = \"\"\"\n",
    "# digraph my_graph {\n",
    "\n",
    "#     node [shape=plaintext]\n",
    "    \n",
    "#     {rank=same; C1W1; C1W1_1; C1W1_2 ;C1W1_2; C1W1_3; C1W1_4}\n",
    "#         {rank=same; C1W1_2a; C1W1_3a; C1W1_4a}\n",
    "#     {rank=same; C1W2; C1W2_1; C1W2_2 ;C1W2_2; C1W2_3; C1W2_4}\n",
    "#         {rank=same; C1W2_2a; C1W2_3a; C1W2_4a}\n",
    "#     {rank=same; C1W3; C1W3_1; C1W3_2 ;C1W3_2; C1W3_3; C1W3_4}\n",
    "\n",
    "#     C1 [shape=box, label=\"Supervised learning:\\n Regression and Classification\"]\n",
    "#     C1W1 [shape=box label=\"Linear regression \\nwith single feature\\n(a.k.a) Univariate Linear Regression\"];\n",
    "#         C1W1_1 [label=\"Single variable\"];\n",
    "#         C1W1_2 [label=\"Model, f(x)\"];\n",
    "#             C1W1_2a [label=\"Straight line\"];\n",
    "#         C1W1_3 [label=\"Cost function, J\"];\n",
    "#         C1W1_4 [label=\"Gradient Descent, dJ/dw\"];\n",
    "#     C1W2 [shape=box label=\"Linear regression\\nwith multiple features\"];\n",
    "#         C1W2_1 [label=\"Single variable\"];\n",
    "#         C1W2_2 [label=\"Model, f(x)\"];\n",
    "#         C1W2_3 [label=\"Cost function, J\"];\n",
    "#         C1W2_4 [label=\"Gradient Descent, dJ/dw\"];\n",
    "#     C1W3 [shape=box label=\"Logistic regression\"];\n",
    "#         C1W3_1 [label=\"<b>Single variable</b>\"];\n",
    "#         C1W3_2 [label=\"Model, f(x)\"];\n",
    "#         C1W3_3 [label=\"Cost function, J\"];\n",
    "#         C1W3_4 [label=\"Gradient Descent, dJ/dw\"];\n",
    "    \n",
    "#     C1 -> C1W1 [penwidth=2];\n",
    "#     C1W1 -> C1W2;\n",
    "#     C1W2 -> C1W3;\n",
    "\n",
    "#     C1W1 -> C1W1_1 -> C1W1_2 -> C1W1_3 -> C1W1_4;\n",
    "    \n",
    "    \n",
    "#     subgraph cluster_C1W1 {\n",
    "#         C1W1;\n",
    "#         C1W1_1;\n",
    "#         C1W1_2;\n",
    "#         C1W1_3;\n",
    "#         C1W1_4;     \n",
    "        \n",
    "#         C1W1_2 -> C1W1_2a;\n",
    "#         C1W1_3 -> C1W1_3a;\n",
    "#         subgraph cluster_C1_{\n",
    "#             C1W1_2a;\n",
    "#             C1W1_3a;\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "    \n",
    "#     C1W2 -> C1W2_1 -> C1W2_2 -> C1W2_3 -> C1W2_4;\n",
    "#     C1W2_2 -> C1W2_2a;\n",
    "#     subgraph cluster_C1W2 {\n",
    "#         C1W2;\n",
    "#         C1W2_1;\n",
    "#         C1W2_2;\n",
    "#         C1W2_3;\n",
    "#         C1W2_4;\n",
    "#     }\n",
    "    \n",
    "#     C1W3 -> C1W3_1 -> C1W3_2 -> C1W3_3 -> C1W3_4;\n",
    "#     subgraph cluster_C1W3 {\n",
    "#         C1W3;\n",
    "#         C1W3_1;\n",
    "#         C1W3_2;\n",
    "#         C1W3_3;\n",
    "#         C1W3_4;\n",
    "#     }\n",
    "    \n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# graphs = pydot.graph_from_dot_data(dot_string)\n",
    "# graph = graphs[0]\n",
    "\n",
    "# graph.write_png(\"map.png\")\n",
    "# view_pydot(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3223adca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T23:49:58.571080Z",
     "start_time": "2023-04-03T23:49:58.173815Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dot_string = \"\"\"\n",
    "digraph D {\n",
    "\n",
    "  node [shape=plaintext fontsize=\"12\"];\n",
    "\n",
    "    {rank=same; C1W1; C1W1_1; C1W1_2; C1W1_3}\n",
    "    {rank=same; C1W2; C1W2_1; C1W2_2; C1W2_3; C1W2_4}\n",
    "    {rank=same; C1W3; C1W3_1; C1W3_2; C1W3_3}\n",
    "\n",
    "  C1W1 [ label=<\n",
    "   <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "     <tr><td align=\"center\"><b>C1 W1: Linear regression</b></td></tr>\n",
    "     <tr><td align=\"center\"><b> with single feature</b></td></tr>\n",
    "\n",
    "   </table>>];\n",
    "\n",
    "  C1W2 [ label=<\n",
    "   <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "     <tr><td align=\"center\"><b>C1 W2: Linear regression</b></td></tr>\n",
    "     <tr><td align=\"center\"><b> with multiple features</b></td></tr>\n",
    "   </table>>];\n",
    "\n",
    "  C1W3 [ label=<\n",
    "   <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "     <tr><td align=\"center\"><b>C1 W3: </b></td></tr>\n",
    "     <tr><td align=\"center\"><b>Logistic regression</b></td></tr>\n",
    "   </table>>];\n",
    "\n",
    "      C1W1_1 [ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b>Model, f(x)</b></td></tr>\n",
    "         <tr><td align=\"left\">- Straight line</td></tr>\n",
    "       </table>>];\n",
    "   \n",
    "      C1W1_2 [ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b>Cost function, J(w,b)</b></td></tr>\n",
    "         <tr><td align=\"left\">- Squared error</td></tr>\n",
    "       </table>>];\n",
    "   \n",
    "      C1W1_3 [ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b>Gradient Descent </b></td></tr>\n",
    "         <tr><td align=\"left\">- Learning rate</td></tr>\n",
    "          <tr><td align=\"left\">- Partial derivative</td></tr>\n",
    "          <tr><td align=\"left\">- Types: Batch, Stochastic, Mini-batch </td></tr>\n",
    "       </table>>];\n",
    "   \n",
    "      C1W2_1 [ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b> Feature scaling </b></td></tr>\n",
    "         <tr><td align=\"left\">  - Min-max norm</td></tr>\n",
    "         <tr><td align=\"left\">  - Mean norm</td></tr>\n",
    "         <tr><td align=\"left\">  - Z-norm</td></tr>\n",
    "       </table>>];\n",
    "   \n",
    "      C1W2_2 [ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b>Check gradient descent</b></td></tr>\n",
    "          <tr><td align=\"left\"><b>for convergence</b></td></tr>\n",
    "          <tr><td align=\"left\">- Learning curve</td></tr>\n",
    "          <tr><td align=\"left\">  - Set proper learning rate</td></tr>\n",
    "          <tr><td align=\"left\">  - Scale feature properly</td></tr>\n",
    "          <tr><td align=\"left\">- Auto-Stopping criterian</td></tr>\n",
    "       </table>>];\n",
    "   \n",
    "      C1W2_3 [ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b>Feature engineering</b></td></tr>\n",
    "         <tr><td align=\"left\">- Polynomial regression</td></tr>\n",
    "         <tr><td align=\"left\">- Feature scaling important</td></tr>\n",
    "       </table>>];\n",
    "\n",
    "      C1W2_4 [ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b>Closed form solution</b></td></tr>\n",
    "         <tr><td align=\"left\">- Alternative to gradient descent</td></tr>\n",
    "         <tr><td align=\"left\">- Faster on smaller data</td></tr>\n",
    "         <tr><td align=\"left\">- No need feature normalization</td></tr>\n",
    "       </table>>]\n",
    "       \n",
    "  C1W3_1[ label=<\n",
    "   <table border=\"\" cellborder=\"0\" cellspacing=\"1\">\n",
    "     <tr><td align=\"left\"><b>Model, f(x)</b></td></tr>\n",
    "     <tr><td align=\"left\"><font color=\"red\">- Linear reg won't </font></td></tr>\n",
    "      <tr><td align=\"left\"><font color=\"red\">work for classification</font></td></tr>\n",
    "     <tr><td align=\"left\">- Sigmoid function</td></tr>\n",
    "     <tr><td align=\"left\">- Decision boundary</td></tr>\n",
    "     <tr><td align=\"left\">  - Linear</td></tr>\n",
    "     <tr><td align=\"left\">  - Non-linear</td></tr>\n",
    "   </table>>];\n",
    "   \n",
    "     C1W3_2[ label=<\n",
    "   <table border=\"\" cellborder=\"0\" cellspacing=\"1\">\n",
    "     <tr><td align=\"left\"><b>Cost function, J(w,b)</b></td></tr>\n",
    "     <tr><td align=\"left\"><font color=\"red\">- Squared error cost function won't work</font></td></tr>\n",
    "     <tr><td align=\"left\">- Loss function for classification</td></tr>\n",
    "     <tr><td align=\"left\">- Cost function from loss function</td></tr>\n",
    "   </table>>];\n",
    "\n",
    "     C1W3_3[ label=<\n",
    "   <table border=\"\" cellborder=\"0\" cellspacing=\"1\">\n",
    "     <tr><td align=\"left\"><b>Gradient Descent</b></td></tr>\n",
    "     <tr><td align=\"left\">- Looks similar to Linear regression,</td></tr>\n",
    "     <tr><td align=\"left\">  but different as f(x) is different</td></tr>\n",
    "     <tr><td align=\"left\">- Log reg has some similarities </td></tr>\n",
    "     <tr><td align=\"left\">  to linear regression</td></tr>\n",
    "      <tr><td align=\"left\">  - Monitoring grad descent for convergence</td></tr>\n",
    "      <tr><td align=\"left\">  - Vectorized implementation for faster computation</td></tr>\n",
    "      <tr><td align=\"left\">  - Feature scaling for faster convergence</td></tr>\n",
    "   </table>>];\n",
    "\n",
    "      C1W3_4[ label=<\n",
    "       <table border=\"1\" cellborder=\"0\" cellspacing=\"1\">\n",
    "         <tr><td align=\"left\"><b>Overfitting Problem</b></td></tr>\n",
    "         <tr><td align=\"left\">- More training samples</td></tr>\n",
    "         <tr><td align=\"left\">- Feature selection</td></tr>\n",
    "         <tr><td align=\"left\">- Regularization</td></tr>\n",
    "         <tr><td align=\"left\">  - Intuition behind how it <font color=\"blue\">shrinks parameters to reduce overfitting</font></td></tr>\n",
    "         <tr><td align=\"left\">  - Lambda: Specifies trade off between overfitting and underfitting</td></tr>\n",
    "       </table>>]\n",
    "\n",
    "  C1W1 -> C1W2;\n",
    "  C1W2 -> C1W3;\n",
    "  \n",
    "  C1W1 -> C1W1_1 -> C1W1_2 -> C1W1_3;\n",
    "  C1W2 -> C1W2_1 -> C1W2_2 -> C1W2_3 -> C1W2_4;\n",
    "  C1W3 -> C1W3_1 -> C1W3_2 -> C1W3_3 -> C1W3_4;\n",
    "      \n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "graphs = pydot.graph_from_dot_data(dot_string)\n",
    "graph = graphs[0]\n",
    "\n",
    "graph.write_png(\"mindmap/C1W1_mindmap.png\")\n",
    "view_pydot(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2333d7b1",
   "metadata": {},
   "source": [
    "# C1: Supervised Machine Learning: Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5d07c",
   "metadata": {},
   "source": [
    "## C1 W1: Introduction to ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b0f71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T01:02:07.261717Z",
     "start_time": "2023-03-21T01:02:07.249804Z"
    }
   },
   "source": [
    "**Machine Learning**\n",
    "- Study that gives computers ability to learn withotu being explicitly programmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6e1bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T01:02:33.527194Z",
     "start_time": "2023-03-21T01:02:33.516406Z"
    }
   },
   "source": [
    "**Machine learning types**\n",
    "- Supervised learning\n",
    "- Unsupervised learning\n",
    "- Recommender systems\n",
    "- Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b490fb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T01:03:30.226691Z",
     "start_time": "2023-03-21T01:03:30.211077Z"
    }
   },
   "source": [
    "**Supervised learning**\n",
    "- Used in most real world applications. 99% of economic value created by ML today is through supervised learning\n",
    "- Algo that learn x (input) to y (output) mappings\n",
    "- Learns from data *labelled* with the \"right answers\"\n",
    "- Types\n",
    "    - Classification: Predicts categories \n",
    "    - Regression: Predicts a continuous number\n",
    "- Example\n",
    "    - Classification\n",
    "        - Breast cancer detection\n",
    "    - Regression\n",
    "        - Housing price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbee1b6",
   "metadata": {},
   "source": [
    "**Unspervised learning**\n",
    "- Find something interesting or structure in *unlabelled* data\n",
    "- Types\n",
    "    - Clustering\n",
    "    - Dimensionality Reduction\n",
    "    - Anomaly detection\n",
    "\n",
    "- Example\n",
    "    - Clustering\n",
    "        - Google news\n",
    "        - Grouping customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250d8cb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88222444",
   "metadata": {},
   "source": [
    "### Regression Model\n",
    "\n",
    "**Linear regression model**\n",
    "\n",
    "Model that fits a straight line to our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70486ddb",
   "metadata": {},
   "source": [
    "> Linear regression with one input variable is a.k.a. **Univariate linear regression**\n",
    "\n",
    "For this section, **we will assume that the linear regression has only one input feature (for simplicity)**. In the next section C1 W2, we will explore regression with multiple input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf894f",
   "metadata": {},
   "source": [
    "**Function, $f$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb907ec6",
   "metadata": {},
   "source": [
    "When we train the model with training data, it will provide a function $f$, sometimes called as *hypothesis*. The job of the function is to take, input $x$ and estimate or predict the output $y$. This predicted output is $\\hat{y}$.\n",
    "\n",
    "- $f$ - Function\n",
    "- $x$ - model input/features\n",
    "- $y$ - actual output/target\n",
    "- $\\hat{y}$ - predicted/ estimated output \n",
    "  - $\\hat{y} = f(x)$\n",
    "\n",
    "How to represent, $f$?\n",
    "- $f_{w, b}{(x)} = wx + b$, assuming the function is a straight line\n",
    "  - $f$ is a function that takes $x$ as input, and depending upon the values of $w$ and $b$, $f$  will output some values of the prediction, $\\hat{y}$.\n",
    "  - For simplicity, $f_{w, b}{(x)}$ will be represented as $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38fc738",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c5e9d",
   "metadata": {},
   "source": [
    "**Cost function, $J$** - **Squared error** function for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f84dc",
   "metadata": {},
   "source": [
    "With linear regression, we want to choose value for parameters $w$ and $b$ such that the straight line we get from the function $f$ somehow fits the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a561b8",
   "metadata": {},
   "source": [
    "How to find $w$ and $b$, such that predicted value $\\hat{y} _{i}$ is close to actual value $y _{i}$ for all $(x^{(i)}, y^{(i)})$ ?\n",
    "- To answer this question, let's first measure how well the line fits the training data using the cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c63ab",
   "metadata": {},
   "source": [
    "$$ J(w,b) = \\frac{1}{2m}\\sum_{i=1}^{m}{(\\hat{y}_{i} - y_{i})}^{2}$$  \n",
    "$$ J(w,b) = \\frac{1}{2m}\\sum_{i=1}^{m}{(f_{w, b}{(x^{i})}  - y_{i})}^{2}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d7c41",
   "metadata": {},
   "source": [
    "Since $J$ is the cost function that measures how big the squared errors are, so choosing $w$ that minimizes these squared errors, makes them errors as small as possible, gives us a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b6128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T01:20:20.964026Z",
     "start_time": "2023-03-21T01:20:20.944222Z"
    }
   },
   "source": [
    "Note: \n",
    "- *For linear regression, the cost function is always squared error cost function.*\n",
    "- *However, for **not** linear regressions, the cost function is not squared error cost function.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e802da",
   "metadata": {},
   "source": [
    "> **Goal of linear regression**<br>\n",
    "> - Simplliefed case: $\\underset{w}{\\text{minimize}}\\phantom{1}J(w)$ <br>\n",
    "> - General case: $\\underset{w,b}{\\text{minimize}}\\phantom{1}J(w,b)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd78ec4",
   "metadata": {},
   "source": [
    "| Term | Org. equation | Simplified | |\n",
    "| :---- | :-------------: | :----------: | |\n",
    "| Model | $$f_{w, b}{(x^{(i)})} = wx^{(i)} + b$$ |  $$f_{w}{(x^{(i)})} = wx^{(i)}$$ by setting $b=0$ | |\n",
    "| Parameters | $w,b$ | $w$ | |\n",
    "| Cost function | $$J(w,b) = \\frac{1}{2m}\\sum_{i=1}^{m}{(f_{w, b}{x^{(i)}}-y^{(i)})}^{2}$$ |  $$J(w) = \\frac{1}{2m}\\sum_{i=1}^{m}{(f_{w}{x^{(i)}}-y^{(i)})}^{2}$$ ||\n",
    "| Goal | $\\underset{w,b}{\\text{minimize}}\\phantom{1}J(w,b)$ | $\\underset{w}{\\text{minimize}}\\phantom{1}J(w)$ ||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb1f0ab",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb1764",
   "metadata": {},
   "source": [
    "**Gradient Descent - Intuition and Algo**\n",
    "- Algo that can be used to find the optimal parameters $w$ and $b$ that minimises the cost function, $J(w,b)$.\n",
    "- Is used not only in linear regression, but also for training some of the most advanced neural network models, including deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7117becb",
   "metadata": {},
   "source": [
    "In layman words, our goal is to start somewhere randomly in this cost function and get to the bottom of one of these valleys efficiently as possible.\n",
    "- What the gradient descent algorithm does is we are going to spin around 360 degrees and ask ourself, if I were to take a tiny little baby step in one direction, and I want to go downhill as quickly as possible to one of these valleys. $\\Rightarrow$ Mathematically, this is the **direction of the steepest descent**. It means that when we take a tiny baby little step, this takes us downhill faster than a tiny little baby step we could have taken in any other direction. Eventually, we might end up in the local minima after series of this baby steps. In this local minima, the cost will be small as possible => We would have found $w$ and $b$, such that predicted value $\\hat{y} _{i}$ is close to actual value $y _{i}$ for all $(x^{(i)}, y^{(i)})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ba3df",
   "metadata": {},
   "source": [
    "**Assuming there's only one input feature $x$,** \n",
    "\n",
    "- Linear regression model\n",
    "  $$f _{w,b}(x^{(i)})=wx^{(i)}+b \\tag{1}$$\n",
    "\n",
    "- Cost function for linear regression\n",
    "   $$J(w,b) = \\frac{1}{2m}\\sum _{i=1}^{m}(f _{w,b}(x^{(i)})-y^{(i)})^2 \\tag{2}$$\n",
    "\n",
    "- Gradient descent \n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace \n",
    "\\end{align*}$$\n",
    "    \n",
    "$\\phantom{.....}\\text{where, parameters }w, b \\text{ are updated simultaneously.}$  \n",
    "    \n",
    "- Gradient \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Here *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0f8cf",
   "metadata": {},
   "source": [
    "where \n",
    "- $\\alpha$ is **learning rate** between $0$ and $1$, that controls how big of a step we take downhill $\\Rightarrow$ Huge $\\alpha$ corresponds to very aggressive gradient descent procedure as will take huge steps downhill.\n",
    "- $\\frac{\\partial}{\\partial w}J(w,b)$ is the **partial derivative** term of cost function $J$ with respect to parameter $w$. This derivative is utilized by the gradient descent to update the parameter . In layman terms, it tell us the direction (of the steepest descent) in which we want to take baby steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998474c",
   "metadata": {},
   "source": [
    "**Learning rate**\n",
    "- If $\\alpha$ is too small, gradient descent will work, but may be slow as it will be taking tiny tiny steps every steps  before it reaches the minima\n",
    "- If $\\alpha$ is too large, though the derivative might be a small value that points in the right direction, as the learning rate is multiplied with derivative, it will end up being a very large value and might overshoot. As we will be taking huge steps, we may never reach minimum. $\\Rightarrow$ Fail to converge, and may even diverge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27010206",
   "metadata": {},
   "source": [
    "**Characteristics of Gradient Descent**\n",
    "- In general, the cost $J$ starts large and rapidly declines. The partial derivatives, $\\frac{\\partial}{\\partial{w}}J(w,b)$, and  $\\frac{\\partial}{\\partial{b}}J(w,b)$ also get smaller, rapidly at first and then more slowly. \n",
    "- As we get closer to the **local minimum** (reach the bottom of the bowl in layman terms), the gradient descent starts making **smaller steps**, because as we approach local minimum, the **derivative automatically gets smaller**. $\\Rightarrow$  **Update steps also gets smaller**.\n",
    "    - <span style=\"color:red\">For e.g., say $w$ and $b$ are bouncing back and forth between positive and negative with the absolute value increasing with each iteration. Further, for each iteration $\\frac{\\partial J(w,b)}{\\partial w}$ changes sign and cost is increasing rather than decreasing. Then. this is a clear sign that the *learning rate is too large* and the solution is diverging. </span>\n",
    "\n",
    "\n",
    "**Types of Gradient Descent** \n",
    "- **Batch gradient descent**: **\"Batch\"** implies that for each step of gradient descent uses all the training examples, instead of subset of training data. The name \"batch\" might be not that intuitive though.\n",
    "- There are other versions of gradient descent that do not look at the entire training set, but instead smaller subsets of training data for each update step. However, for linear regression, we will be using linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae017e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da757b8",
   "metadata": {},
   "source": [
    "## C1 W2: Regression with multiple input variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fae43a",
   "metadata": {},
   "source": [
    "Notations for multiple linear regression\n",
    "\n",
    "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalar, non bold                                                      ||\n",
    "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
    "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | training example maxtrix                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` |\n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
    "|  $\\mathbf{x}_{j}$ | $j_{th}$ feature | `X[:,j]`|\n",
    "|  $x_{j}^{(i)}$ | $j_{th}$ feature in $i_{th}$ training example| `X[i,j]`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c33343c",
   "metadata": {},
   "source": [
    "**Gradient descent for multiple linear regression** with and without vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eeb6c6",
   "metadata": {},
   "source": [
    "| Details | without vectorization | Vector notation |\n",
    "| -: | :- | :- |\n",
    "| Parameters | $w_1, ..., w_n$ <br>$b$| $\\vec{w}=[w_1 \\dotsc w_n]$<br>$b$ |\n",
    "| Model | $f_{\\vec{w},b}(\\vec{x}) = w_{1}x_{1} + \\dotsc + w_{n}x_{n} + b$ | $f_{\\vec{w},b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x}+b$ |\n",
    "| Cost function | $J(w_1, \\dotsc, w_n, b)$ | $J(\\vec{w},b)$ |\n",
    "| Gradient descent | repeat {<br>$\\; w_j = w_j -\\alpha\\frac{\\partial }{\\partial w_{j}}J(w_1, \\dotsc, w_n, b)$ <br> $\\; b = b -\\alpha\\frac{\\partial }{\\partial b}J(w_1, \\dotsc, w_n, b)$ <br>}| repeat {<br>$\\; w_j = w_j -\\alpha\\frac{\\partial }{\\partial w_{j}}J(\\vec{w}, b)$ <br> $\\; b = b -\\alpha\\frac{\\partial }{\\partial b}J(\\vec{w}, b)$ <br>} |\n",
    "| $$$$ | $$$$ | $$$$ |\n",
    "\n",
    "where \n",
    "- $\\vec{w}$ is vector of length $n$, and $n$ represents to number of columns/features\n",
    "- $b$ is still a number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab14700",
   "metadata": {},
   "source": [
    "**Gradient descent and its derivative term when we have single feature vs. multiple features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95840aa7",
   "metadata": {},
   "source": [
    "| One feature | $n$ features ($n \\ge 2$) |\n",
    "| :--- | :--- |\n",
    "| repeat{ |  repeat{|\n",
    "| $w = w - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})x^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w}J(w,b)$ | $w_1 = w_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)})x_{1}^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w_1}J(\\vec{w},b)$ <br> $\\vdots$ <br> $w_n = w_n - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)})x_{n}^{(i)} = w - \\alpha\\frac{\\partial}{\\partial w_n}J(\\vec{w},b)$ |\n",
    "| $b = b - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)}) = b - \\alpha\\frac{\\partial}{\\partial b}J(w,b)$ | $b = b - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)}) = b - \\alpha\\frac{\\partial}{\\partial b}J(\\vec{w},b)$ |\n",
    "| simultaneously update <br> $w,b $ <br>}|  simultaneously update <br> $w_j$ for $(j=1, \\dotsc, n)$ and $b$<br>} |\n",
    "| $$$$ | $$$$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f79ca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8baa27d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T02:18:19.396134Z",
     "start_time": "2023-03-21T02:18:19.387449Z"
    }
   },
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8944b65",
   "metadata": {},
   "source": [
    "- When possible range of **feature values are large**, it's more likely that a good model will learn to choose a relatively **small parameter value**, like 0.1.\n",
    "    - e.g., House size, $x_1=2000$ sq. feet and $w_1 = 0.1$\n",
    "- Likewise, when possible **feature values are small**, the reasonable value for these **parameters will be relatively large**\n",
    "    - e.g., No. of bedrooms, $x_2=5$ and $w_2 = 50$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdef258",
   "metadata": {},
   "source": [
    "**Intution: How does feature size relate to or affect gradient descent?**\n",
    "\n",
    "- Let's look at the scatter plot of the features where size (in sq. feet) is the horiontal axis, and the number of bedroom is on the vertical axis. \n",
    "    - If we plot the training data, we notice that *horizontal axis is on a much large scale/range of values compared to vertical axis*\n",
    "- Let's then look at the contour plot of cost function, where $w_1$ is the horizontal axis, and $w_2$ is the vertical axis. \n",
    "    - Here, *horizontal axis has much narrower range, whereas vertical axis takes on much larger values*. $\\Rightarrow$ Contours form ovals/elipses and they are short on one side and long on other.\n",
    "        - This is because a **small change in $w_1$ can have a large impact on estimated price** as it is multiplied with a very large value $x_1$ $\\Rightarrow$ Very large impact on cost $J$\n",
    "        - In contrast, it takes much larger change in $w_2$ in order to change predictions much.  $\\Rightarrow$  **small changes to $w_2$ don't change the cost function nearly that much**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a98bc",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"week2/attachments/feature_scaling_part_1_2.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "    <img src=\"week2/attachments/feature_scaling_part_1_3.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35978be4",
   "metadata": {},
   "source": [
    "The problem with skewed contours is that the gradient descent may end up taking long time before it can find its way to global minima. In this situation, a useful thing to do is to <span style=\"color:green\"><b>scale the features</b></span> by performing some transformation of our training data so that the features range from 0 to 1. If we run gradient descent on a cost function to find on this scaled features, then contours will look more like circles and less skewed $\\Rightarrow$ <span style=\"color:green\"><b>Gradient descent will run faster</b></span> as it can find a much more direct path to global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73edcf81",
   "metadata": {},
   "source": [
    "Other things to note:\n",
    "- When $w$'s are updated unevenly during training, it implies that the features vary significantly in magnitude making some features update much faster than others. \n",
    "    - **Why are the $w$'s updated unevenly**? \n",
    "        - This is because when the gradients are computed, the common error term $f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{i}$ is multiplied by the features $\\color{Red}{x_j^{(i)}}$ for the $w$'s. \n",
    "        - In the lab housing dataset, feature $w_0$ (size in sq.ft) is 2-3 orders of magnitude larger than other features.\n",
    "        - This makes the gradient bigger. When the weights are updated based on learning rate $\\alpha$ and gradient computed from partial derivative with respect to each feature, $w_0$ will make more rapid progress than the other parameters due to its much larger gradient. \n",
    "$$\\frac{\\partial}{\\partial w_j}J(\\vec{w},b) = \\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{i})\\color{Red}{x_{j}^{(i)}}$$\n",
    "- When the features are normalized, the cost contour is much more symmetric. The result is that updates to parameters during gradient descent can make equal progress for each parameter.<span style=\"color:green\"> $\\Rightarrow$ The scaled features get very accurate results much, much faster!. Also, the gradient of each parameter will be tiny by the end of this fairly short run (iterations)</span>. \n",
    "- A learning rate of 0.1 is a good start for regression with normalized features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0062df9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-21T04:33:36.623558Z",
     "start_time": "2023-03-21T04:33:36.612787Z"
    }
   },
   "source": [
    "**Different feature scaling techniques**\n",
    "1. **Divide by maximum** (or) **Min-max normalization**: \n",
    "    - One way to scale the features is to take the maximum of the feature value and divide all the samples by the maximum as shown above.\n",
    "    - **More generally**: Rescale each feature by both its minimum and maximum value as show below\n",
    "$$x_1 = \\frac{x_1 - min(x_1)}{max(x_1)- min(x_1)} \\;\\;\\;\\; x_2 = \\frac{x_2 - min(x_2)}{max(x_2) - min(x_2)}$$\n",
    "    \n",
    "    Both ways normalizes features to the range of -1 and 1, where the former method works for positive features which is simple and serves well for the lecture's example, and the latter method works for any features.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d73d53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-14T00:58:37.315084Z",
     "start_time": "2023-03-14T00:58:37.298439Z"
    }
   },
   "source": [
    "2. **Mean normalization**: \n",
    "    - Rescale the features so that they are cenetered around zero. \n",
    "    - To calculate mean normalization: \n",
    "$$x_1 = \\frac{x_1 - \\mu_{1}}{max(x_1)- min(x_1)} \\;\\;\\;\\; x_2 = \\frac{x_2 - \\mu_{2}}{max(x_2) - min(x_2)}$$\n",
    "\n",
    "    where $u_1$ and $u_2$ are mean of $x_1$ and $x_2$ respectively\n",
    "\n",
    "3. **Z-score normalization**: \n",
    "    - Uses standard deviation $\\sigma$\n",
    "    - To calculate z-score normalization,\n",
    "$$x_1 = \\frac{x_1 - \\mu_{1}}{\\sigma_1} \\;\\;\\;\\; x_2 = \\frac{x_2 - \\mu_{2}}{\\sigma_2}$$\n",
    "\n",
    "    where $u_1$ and $\\sigma_1$ are mean and standard deviation of $x_1$\n",
    "\n",
    "    After z-score normalization, all features will have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351c40d",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><b><u>Rule of thumb for feature scaling</u></b></span>\n",
    "\n",
    "When in doubt, carry out the feature rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22191fc5",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"week2/attachments/feature_scaling_part_2_4.png\" alt=\"Drawing\" style=\"width: 50%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c0577a",
   "metadata": {},
   "source": [
    "### Check gradient descent for convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c744a1",
   "metadata": {},
   "source": [
    "**How to ensure gradient descent is working correctly**\n",
    "\n",
    "- Objective: $\\underset{\\vec{w},b}{min}J(\\vec{w},b)$\n",
    "- Solution: \n",
    "    - We can plot the cost function over the number of iterations of gradient descent. This curve is aka **learning curve**\n",
    "    - <span style=\"color:green\">If gradient descent is working properly, the cost $J$ should decrease every single iteration</span>. <span style=\"color:red\">If the cost is increasing, it implies a bug in code or poor choice of $\\alpha$</span>\n",
    "    \n",
    "Note: The # of iterations needed for gradient descent to converge varies a lot by applications.\n",
    "Then, how do we decide when to stop the iterations? \n",
    "- **Automatic convergence test for gradient descent aka Stopping Criterian**: If $J(\\vec{w},b)$ decreases by $\\le \\epsilon$ in one iteration, declare *convergence* and stop the iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63df106",
   "metadata": {},
   "source": [
    "**How to choose a proper learning rate for gradient descent?**\n",
    "\n",
    "In the learning curve (cost vs. numbe of iterations), <span style=\"color:red\">if cost sometimes goes up and sometimes goes down, it implies that either gradient descent is not working properly or learning rate is too large</span>.\n",
    "\n",
    "- Ensure, that weights are properly updated ($w_1 \\neq w_1 + \\alpha d_1$, instead $w_1 = w_1 - \\alpha d_1$)\n",
    "    - When $w$'s are updated unevenly during training, it may imply that the features vary significantly in magnitude making some features update much faster than others. So, scale of features might be an issue\n",
    "- When the cost goes up or keeps wobbling, try smaller learning rate, and go down gradually.\n",
    "    - One way to debug correct implementation of gradient descent is that with small enough $\\alpha$, $J$ should decrease on every single iteration.\n",
    "    - Remember, setting $\\alpha$ to be very small value means gradient descent can take a lot of iterations to converge.\n",
    "    - Perhaps, we can try range of values for learning rate for every iterations\n",
    "        - Try 3x bigger $\\alpha$ for each new variant to find the largest possible learning rate\n",
    "            - e.g., $0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec12e6c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da367e0",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5940c",
   "metadata": {},
   "source": [
    "**Feature engineering**: Using *intuition* or *knowledge* to design *new features*, by transforming or combining original features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524410a6",
   "metadata": {},
   "source": [
    "Polynomial regression\n",
    "- A flavor of feature engineering that allows us to fit not just straight lines, but curves (non-linear functions) to our data.\n",
    "- Combines the ideas of multiple linear regression and feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cdcbe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-16T23:58:31.980440Z",
     "start_time": "2023-03-16T23:58:31.971077Z"
    }
   },
   "source": [
    "So, how do decide which features to use? Will be addressed in Course 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac928dcb",
   "metadata": {},
   "source": [
    "Note:\n",
    "- Polynomial features will be chosen based on how well they matched the target data. Another way to think about this is to note that <span style=\"color:blue\">we are still using linear regression once we have created new (engineered) features. Given that, <b>the best new (engineered) features will be linear relative to the target</b></span>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85032998",
   "metadata": {},
   "source": [
    "### Closed form solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba15e6e",
   "metadata": {},
   "source": [
    "Note\n",
    "- **Gradient Descent based**\n",
    "    - It is important to apply feature scaling when doing feature engineering with gradient descent based regression\n",
    "- **Closed form (Alternative to gradient descent)** \n",
    "    - The closed-form solution work well and faster on smaller data sets such as these but can be computationally demanding on larger data sets.\n",
    "    - The closed-form solution does not require normalization.\n",
    "    - An example of a closed form solution in linear regression would be the least square equation [Source](https://stats.stackexchange.com/questions/70848/what-does-a-closed-form-solution-mean)\n",
    "\n",
    "$$y = wX$$\n",
    "\n",
    "$$w=(X^{T}X)^{-1}X^{T}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778546a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50bb5c",
   "metadata": {},
   "source": [
    "## C1 W3: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342dd43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T00:09:12.899383Z",
     "start_time": "2023-04-03T00:09:12.890451Z"
    }
   },
   "source": [
    "**Why can't we use linear regression for classification?**\n",
    "\n",
    "- We can try applying linear regression to fit on classification problem by converting the output to categories by choosing a threshold (a.k.a. **decision boundary**). If we are lucky enough, this might even work in some cases. <span style=\"color:red\">However, often, linear regression might learn a worse function and make terrible mistakes when it comes to classification. $\\Rightarrow$ <b>Linear regression is not a good algorithm for classification problem.</b></span>\n",
    "    - For e.g., From the purple straight line below, if we use a threshold of 0.5 for y to draw the decision boundary between two classes, then, we will misclassify two of malignant tumours as benign with linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c4ed1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T00:20:52.939106Z",
     "start_time": "2023-04-03T00:20:52.917308Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"week3/attachments/lab_sigmoid_function_and_logistic_regression_2_edited.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d12d3d",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99b1dc",
   "metadata": {},
   "source": [
    "Instead of fitting a straight line in linear regression, in logistic regression, we fit a S-shaped curve (**sigmoid function**) to the data, such that the outputs stays between 0 (-ve class) and 1 (+ve class) for the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7414db5",
   "metadata": {},
   "source": [
    "Sigmoid function,\n",
    "$$g(z) = \\frac{1}{1+e^{-z}} \\;\\;\\; 0<g(z)<1$$\n",
    "\n",
    "- If the $z$ gets too bigger, the $g(z)$ gets close to $1$\n",
    "- If the $z$ gets too smaller, the $g(z)$ gets close to $0$\n",
    "- If the $z = 0$ , then $g(z)=0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70669e5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T00:18:19.226770Z",
     "start_time": "2023-04-03T00:18:19.210150Z"
    }
   },
   "source": [
    "*Difference between linear regression and logistic regression function* \n",
    "\n",
    "For linear regression,\n",
    "$$f_{\\vec{w},b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x} + b$$\n",
    "\n",
    "For logistic regression,\n",
    "$$f_{\\vec{w},b}(\\vec{x}) = g(\\vec{w}\\cdot\\vec{x} + b) = g(z) = \\frac{1}{1+e^{-(\\vec{w}\\cdot\\vec{x}+b)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7fec36",
   "metadata": {},
   "source": [
    "In a sense, <span style=\"color:blue\"><b>the logistic regression model applies sigmoid to the output of linear regression model.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ce3a28",
   "metadata": {},
   "source": [
    "The output of logistic regression (above) can also be interepreted as probability that the output belongs to class $1$ given input $x$, with parameters $w$ and $b$.\n",
    "\n",
    "$$f_{w,b}(\\vec{x}) = P(y=1|{x};\\vec{w},b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a16e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T00:27:04.564130Z",
     "start_time": "2023-04-03T00:27:04.544034Z"
    }
   },
   "source": [
    "**Decision boundary**\n",
    "- The decision boundary line of logistic regression can be both linear and non-linear (not a straight line such as polynomial) depending on the choice of function/model.\n",
    "- e.g.,\n",
    "    - Linear model    : $f_{\\vec{w},b}(\\vec{x}) = g(z) = g(\\vec{w}\\cdot\\vec{x} + b) = g(w_1x_1+w_2x_2+b)$ $\\Rightarrow$ Linear decision boundary: $w_1x_1+w_2x_2+b = 0$\n",
    "    - Non-linear model: $f_{\\vec{w},b}(\\vec{x}) = g(z) = g(\\vec{w}\\cdot\\vec{x} + b) = g(w_1x_1^2+w_2x_2^2+b)$  $\\Rightarrow$ Non-linear decision boundary: $w_1x_1^2+w_2x_2^2+b = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad708092",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be7040",
   "metadata": {},
   "source": [
    "- The **squared error cost function worked well for linear regression as it's convex**. So, it is natural to consider it for logistic regression as well. However, $f_{wb}(x)$ now has a non-linear component in logistic regression, the sigmoid function: $f_{w,b}(x^{(i)}) = sigmoid(wx^{(i)} + b )$. <span style=\"color:Red\"><b>The squared error cost function does not work for logistic regression as it's non-convex $\\Rightarrow$ Gradient descent might end up in sub-optimal local minima</b></span>.\n",
    "- In order to build a new cost function for logistic regression,  we will modify the cost function of linear regression to make it more suitable to non-linear nature. Logistic Regression, therefore uses a **loss function** more suited to the task of categorization where the target is 0 or 1 rather than any number. Thereby, making the <span style=\"color:green\">**new modified cost function convex**</span>, and thus we can reliably use gradient descent to find the global minimum (best parameters to fit our model on the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9a63b",
   "metadata": {},
   "source": [
    "**New cost function**\n",
    "\n",
    "$$J(\\vec{w},b) = \\frac{1}{m}\\sum_{i=1}^{m}L(f_{\\vec{w},b}(\\vec{x}^{(i)}), y^{(i)})$$\n",
    "\n",
    "where the **loss function** $L(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point (that tells us how well we are doing in predicting the example), which is:\n",
    "\n",
    "$$L(f_{\\vec{w},b}(\\vec{x}^{(i)}) - y^{(i)}) = \\left\\{\\begin{matrix}\n",
    "        \\phantom{1- }-log\\left(f_{\\vec{w},b}(\\vec{x}^{(i)})\\right ) & \\text{ if } y^{(i)}=1 \\\\ \n",
    "        -log\\left(1- f_{\\vec{w},b}(\\vec{x}^{(i)})\\right ) & \\text{ if } y^{(i)}=0 & \n",
    "        \\end{matrix}\\right.$$\n",
    "\n",
    "\n",
    "*  $f_{\\vec{w},b}(\\vec{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value.\n",
    "\n",
    "*  $f_{\\vec{w},b}(\\vec{x}^{(i)}) = g(\\vec{w} \\cdot\\vec{x}^{(i)}+b)$ where function $g$ is the sigmoid function.\n",
    "\n",
    "The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or ($y=0$) and another for when the target is one ($y=1$). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d67241",
   "metadata": {},
   "source": [
    "Simplyfing the logistic loss function and cost function, we get\n",
    "\n",
    "**Simplified logistic loss function**\n",
    "    \\begin{equation}\n",
    "      L(f_{\\vec{w},b}(\\vec{x}^{(i)}), y^{(i)}) = \\begin{cases}\n",
    "        - \\log\\left(f_{\\vec{w},b}(\\vec{x}^{(i)}) \\right) & \\text{if $y^{(i)}=1$}\\\\\n",
    "        - \\log \\left( 1 - f_{\\vec{w},b}( \\vec{x}^{(i)} ) \\right) & \\text{if $y^{(i)}=0$}\n",
    "      \\end{cases}\n",
    "    \\end{equation}\n",
    "    \n",
    "$$L(f_{\\vec{w},b}(\\vec{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\vec{w},b}( \\vec{x}^{(i)} ) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\vec{w},b}( \\vec{x}^{(i)} ) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b3bd29",
   "metadata": {},
   "source": [
    "**Simplified cost function**\n",
    "\n",
    "Loss is defined to apply to one example. Here, we combine the losses to form the cost, which includes all the examples.\n",
    "\n",
    "$$J(\\vec{w},b) = \\frac{1}{m}\\sum_{i=1}^{m}\\left [ L(f_{\\vec{w},b}(\\vec{x}^{(i)}), y^{(i)})\\right ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a94e851",
   "metadata": {},
   "source": [
    "$$J(\\vec{w},b) = -\\frac{1}{m}\\sum_{i=1}^{m}\\left [ y^{(i)} \\log\\left(f_{\\vec{w},b}( \\vec{x}^{(i)} ) \\right) + \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\vec{w},b}( \\vec{x}^{(i)} ) \\right)\\right ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a78acce",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "  f_{\\vec{w},b}(\\vec{x^{(i)}}) &= g(z^{(i)}) \\\\\n",
    "  z^{(i)} &= \\vec{w} \\cdot \\vec{x}^{(i)}+ b \\\\\n",
    "  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7998ce",
   "metadata": {},
   "source": [
    "Why did we choose this particular cost function for logistic regression when we could have chosen tons of other functions?\n",
    "- This cost function is derived from statistics using a statistical principle called **Maximum Likelihood Estimation**, an idea on how to efficiently find parameters for different models. The cost function also is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5eb7d5",
   "metadata": {},
   "source": [
    "### Gradient descent for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afbffd3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Recall the gradient descent algorithm utilizes the gradient calculation:\n",
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "Where each iteration performs simultaneous updates on $w_j$ for all $j$, where\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2} \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e956f",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Though the derivative terms for logistic regression look similar to linear regression, they are not the same as the definition for $f_{w,b}(x)$ is changed</b>. Therefore, they are two very different algorithms.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54886d8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-19T02:26:47.744055Z",
     "start_time": "2023-03-19T02:26:47.720046Z"
    }
   },
   "source": [
    "**Besides many concepts are same for linear and logistic regression**\n",
    "- Monitoring gradient decent (a.k.a. **learning curve**) for convergence\n",
    "- Vectorized implementation for faster computation\n",
    "- Feature scaling speeds up gradient descent convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd0e25f",
   "metadata": {},
   "source": [
    "### Overfitting Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d24f2",
   "metadata": {},
   "source": [
    "3 possible scenarios:\n",
    "- <span style=\"color:red\"><b>Underfitting (or) High bias</b></span>\n",
    "- <span style=\"color:red\"><b>Overfitting (or) High variance</b></span>\n",
    "- <span style=\"color:green\"><b>Generalization</b></span>\n",
    "    - Make good predictions even on brand new examples never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9febf46",
   "metadata": {},
   "source": [
    "**Options to address Overfitting**\n",
    "1. Provide more training example\n",
    "2. Selected features to include/exclude\n",
    "    - <span style=\"color:blue\"><b>Feature selection </b></span>- Covered in Course 2\n",
    "3. Reduce size of parameters\n",
    "    - <span style=\"color:blue\"><b>Regularization</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3aac33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-19T04:07:06.793040Z",
     "start_time": "2023-03-19T04:07:06.772404Z"
    }
   },
   "source": [
    "**Regularization**\n",
    "\n",
    "- For a overfitted model, there's a high chance that it has large values for $w_j$. \n",
    "- <span style=\"color:blue\">Regularization is way to more gently reduce the impacts of some of the features without doing something as harsh as eliminating it outright. <b>Regularization encourage the learning algorithm to shrink the value of the parameters $w_j$ without necessarily demanding that it is set to exactly 0, thereby reducing overfitting</b></span>.\n",
    "    - Used very frequently, including in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1aedc",
   "metadata": {},
   "source": [
    "**Small values for our parameters ($w_1$, $...$, $w_n$, $b$) will lead to simpler models $\\Rightarrow$ Less prone to overfitting**.\n",
    "\n",
    "Because we don't know which of the parameters are going to be important ones, let's penalize all of them a bit and shrink all of them by adding a new term $\\frac{\\lambda}{2m}\\sum_{j=1}^{n}w_j^2$\n",
    "\n",
    "$$\\text{Modified linear regression cost function, }J(\\vec{w},b) = \\frac{1}{2m}\\sum_{i=1}^{m}\\left (f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)}\\right)^2 + \\mathbf{\\frac{\\lambda}{2m}\\sum_{j=1}^{n}w_j^2}$$\n",
    "\n",
    "where $\\lambda$ is called regularization parameter.\n",
    "\n",
    "By convention, we can exclude the $b$ term from being penalised. However, some ML practitioners might include it as well as  $\\frac{\\lambda}{2m}\\sum_{j=1}^{n}b^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d3b579",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">In summary, in the modified cost function, we want to minimize the mean squared error cost with additionally, the regularization term $\\Rightarrow$ This <b>new cost function trades of two goals</b> we have: </span>\n",
    "1. <span style=\"color:blue\">Try to <b>minimize the first term (squared error of pred vs. actual values)</b> encourages algo to fit the training data well </span>\n",
    "2. <span style=\"color:blue\">Try to <b>minimize the second term by keeping parameters $w_j$ small</b>, thereby reducing overfitting\n",
    "    - **The value of $\\lambda$ we choose will specify the relative trade off or importance between these two goals** \n",
    "        - If <span style=\"color:red\">$\\lambda=0$</span>, no regularization happens $\\Rightarrow$  <span style=\"color:red\">Overfitting</span>\n",
    "        - If <span style=\"color:red\">$\\lambda=10^{10}$ (a very large value)</span>, we are placing heavy weight on regularization term, and only way to minimise the cost function is by forcing all values of $w$ very close to $0$. Thus, $f(x)\\approx b$ and learning algo fits a horizontal straigth line $\\Rightarrow$  <span style=\"color:red\">Underfitting</span> </span>\n",
    "    \n",
    "So, we have to choose $\\lambda$ such that it properly trades-off between overfitting and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d29b8",
   "metadata": {},
   "source": [
    "**Regularized linear regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8b3c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T00:00:11.980587Z",
     "start_time": "2023-03-20T00:00:11.959882Z"
    }
   },
   "source": [
    "| Linear regression  | Without regularization | With regularization | \n",
    "| :- | :------------------- | :---------------------- | \n",
    "| Cost function | $$J(\\vec{w},b) = \\frac{1}{2m}\\sum_{i=1}^{m}\\left (f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)}\\right)^2 $$ | $$J(\\vec{w},b) = \\frac{1}{2m}\\sum_{i=1}^{m}\\left (f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)}\\right)^2 + \\mathbf{\\frac{\\color{Red}\\lambda}{\\color{Red}{2m}}\\sum_{j=1}^{n}\\color{Red}{w_j^2}}$$ |\n",
    "| Derivatives for gradient descent | | | \n",
    "| $$\\frac{\\partial}{\\partial{w_j}}J(\\vec{w},b)=$$ | $$\\frac{1}{m}\\sum_{i=1}^{m}\\left( f_{\\vec{w},b}(x^{(i)}) -y^{(i)}\\right) x_j^{(i)} $$ |  $$\\frac{1}{m}\\sum_{i=1}^{m}\\left( f_{\\vec{w},b}(x^{(i)}) -y^{(i)}\\right) x_j^{(i)} + \\mathbf{\\frac{\\color{Red}\\lambda}{\\color{Red}m}\\color{Red}{w_{j}}} --- (1) $$ |\n",
    "| $$\\frac{\\partial}{\\partial{b}}J(\\vec{w},b)=$$ | $$\\frac{1}{m}\\sum_{i=1}^{m}\\left( f_{\\vec{w},b}(x^{(i)}) -y^{(i)}\\right)  $$ |  $$\\frac{1}{m}\\sum_{i=1}^{m}\\left( f_{\\vec{w},b}(x^{(i)}) -y^{(i)}\\right)  $$ | \n",
    "| Gradient descent <br> repeat { <br> $$$$<br> $$$$ <br>$$$$} simulataneous update | <br><br> $$ w_j = w_j -\\alpha \\frac{\\partial}{\\partial{w_j}}J(\\vec{w},b)  $$ <br>$$ b = b -\\alpha \\frac{\\partial}{\\partial{b}}J(\\vec{w},b)$$<br>| <br><br> $$ w_j = w_j -\\alpha \\frac{\\partial}{\\partial{w_j}}J(\\vec{w},b) --- (2) $$ <br>$$ b = b -\\alpha \\frac{\\partial}{\\partial{b}}J(\\vec{w},b) $$<br> | \n",
    "|Plugging $(1)$ in $(2)$| | | \n",
    "| $$w_j=$$ | $$w_j - \\alpha \\left[ \\frac{1}{m}\\sum_{i=1}^{m}\\left( f_{\\vec{w},b}(x^{(i)}) -y^{(i)}\\right) x_j^{(i)} \\right]$$ | $$w_j - \\alpha \\left[ \\frac{1}{m}\\sum_{i=1}^{m}\\left( f_{\\vec{w},b}(x^{(i)}) -y^{(i)}\\right) x_j^{(i)} + \\mathbf{\\frac{\\color{Red}\\lambda}{\\color{Red}m}\\color{Red}{w_{j}}}\\right] $$ <br>  $$w_j  - \\alpha\\mathbf{\\frac{\\color{Red}\\lambda}{\\color{Red}m}\\color{Red}{w_{j}}} - \\alpha \\left[ \\frac{1}{m}\\sum_{i=1}^{m}\\left( f_{\\vec{w},b}(x^{(i)}) -y^{(i)}\\right) x_j^{(i)}\\right] $$ <br> $$\\underbrace{w_j \\left (1 - \\alpha\\mathbf{\\frac{\\color{Red}\\lambda}{\\color{Red}m}} \\right )} - \\underbrace{\\alpha \\left[ \\frac{1}{m}\\sum_{i=1}^{m}\\left( f_{\\vec{w},b}(x^{(i)}) -y^{(i)}\\right) x_j^{(i)}\\right]} $$ <br> $$NewPart - OriginalPart$$ | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c8df3",
   "metadata": {},
   "source": [
    "Say, $\\alpha=0.01$,  $\\lambda=1$ and $m=50$, then\n",
    "$$\\alpha\\frac{\\lambda}{m} = 0.01\\frac{1}{50} = 0.0002$$\n",
    "\n",
    "Plugging it into gradient descent\n",
    "\n",
    "$$w_j = w_j\\left( 1-\\alpha \\frac{\\lambda}{m} \\right) - \\alpha \\left[ ... \\right] $$\n",
    "$$w_j = w_j\\left(1-.0002\\right) - \\alpha \\left[ ... \\right] $$\n",
    "$$w_j = w_j\\left(0.9998\\right) - \\alpha \\left[ ... \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5665a9a3",
   "metadata": {},
   "source": [
    "**What regularization term in cost function is doing to the gradient descent update?**\n",
    "- <span style=\"color:blue\">On every single iteration, we are multiplying $w_j$ by a number slightly less than 1 in the <i>new part</i>, and that has the effect of shrinking the value of $w_j$ just a tiny little bit (on every iteration).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d3f6c3",
   "metadata": {},
   "source": [
    "**Regularized logistic regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673c6cb",
   "metadata": {},
   "source": [
    "For logistic regression <u>without</u> regularization, \n",
    "\n",
    "- Cost function is\n",
    "     $$J(\\vec{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\vec{w},b}( \\vec{x}^{(i)} ) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\vec{w},b}( \\vec{x}^{(i)}) \\right) \\right] $$\n",
    "     \n",
    "- Gradient is\n",
    "    $$\n",
    "\\frac{\\partial J(\\vec{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\vec{w},b}(\\vec{x}^{(i)}) - \\vec{y}^{(i)})x_{j}^{(i)}\n",
    "$$\n",
    "    $$\n",
    "\\frac{\\partial J(\\vec{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\vec{w},b}(\\vec{x}^{(i)}) - \\vec{y}^{(i)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ccd12",
   "metadata": {},
   "source": [
    "For logistic regression <u>with</u> regularization,\n",
    "- Cost function is\n",
    "    $$J(\\vec{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\vec{w},b}( \\vec{x}^{(i)} ) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\vec{w},b}( \\vec{x}^{(i)}) \\right) \\right] + \\frac{\\color{Red}\\lambda}{\\color{Red}{2m}}  \\sum_{j=0}^{n-1} \\color{Red}{w_j^2}$$\n",
    "\n",
    "- The gradient is \n",
    "    $$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\left( \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \\right) + \\frac{\\color{Red}\\lambda}{\\color{Red}m} \\color{Red}{w_j}  \\quad\\$$\n",
    "    \n",
    "    $$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cd4a02",
   "metadata": {},
   "source": [
    "- Remember, the gradient descent update (or derivative of cost function) for linear regression and logistic regression looked very similar.\n",
    "- Likewise, the gradient descent update for regularized linear and logistic regression look similar too.\n",
    "- <span style=\"color:red\"><b> Note: Though, gradient descent update for linear and logistic regression look very similar, but $f(x)$ is not the same.</b></span>\n",
    "    - For linear regression, $f(x)$ is a linear function\n",
    "    - For logistic regression, $f(x)$ is a sigmoid (logistic) function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1749d0",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ga]",
   "language": "python",
   "name": "conda-env-ga-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "311.225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
