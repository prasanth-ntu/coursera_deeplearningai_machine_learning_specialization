{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fd86250",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Course-2-Week-2:-Neural-network-training\" data-toc-modified-id=\"Course-2-Week-2:-Neural-network-training-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Course 2 Week 2: Neural network training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Neural-network-training\" data-toc-modified-id=\"Neural-network-training-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Neural network training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tensorflow-implementation\" data-toc-modified-id=\"Tensorflow-implementation-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Tensorflow implementation</a></span></li><li><span><a href=\"#Training-Details\" data-toc-modified-id=\"Training-Details-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Training Details</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Neural-network-training\" data-toc-modified-id=\"Practice-quiz:-Neural-network-training-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Practice quiz: Neural network training</a></span></li><li><span><a href=\"#Activation-functions\" data-toc-modified-id=\"Activation-functions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Activation functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Alternatives-to-the-sigmoid-function\" data-toc-modified-id=\"Alternatives-to-the-sigmoid-function-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Alternatives to the sigmoid function</a></span></li><li><span><a href=\"#Choosing-activation-functions\" data-toc-modified-id=\"Choosing-activation-functions-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Choosing activation functions</a></span></li><li><span><a href=\"#Why-do-we-need-activation-functions?\" data-toc-modified-id=\"Why-do-we-need-activation-functions?-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Why do we need activation functions?</a></span></li><li><span><a href=\"#Lab:-ReLU-activation\" data-toc-modified-id=\"Lab:-ReLU-activation-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Lab: ReLU activation</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Activation-functions\" data-toc-modified-id=\"Practice-quiz:-Activation-functions-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Practice quiz: Activation functions</a></span></li><li><span><a href=\"#Multiclass-Classification\" data-toc-modified-id=\"Multiclass-Classification-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Multiclass Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multiclass\" data-toc-modified-id=\"Multiclass-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Multiclass</a></span></li><li><span><a href=\"#Softmax\" data-toc-modified-id=\"Softmax-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Softmax</a></span></li><li><span><a href=\"#Neural-network-with-softmax-output\" data-toc-modified-id=\"Neural-network-with-softmax-output-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Neural network with softmax output</a></span></li><li><span><a href=\"#Improved-implementation-with-Softmax\" data-toc-modified-id=\"Improved-implementation-with-Softmax-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>Improved implementation with Softmax</a></span></li><li><span><a href=\"#Classification-with-multiple-outputs-(Optional)\" data-toc-modified-id=\"Classification-with-multiple-outputs-(Optional)-1.5.5\"><span class=\"toc-item-num\">1.5.5&nbsp;&nbsp;</span>Classification with multiple outputs (Optional)</a></span></li><li><span><a href=\"#Lab:-Softmax\" data-toc-modified-id=\"Lab:-Softmax-1.5.6\"><span class=\"toc-item-num\">1.5.6&nbsp;&nbsp;</span>Lab: Softmax</a></span></li><li><span><a href=\"#Lab:-Multiclass\" data-toc-modified-id=\"Lab:-Multiclass-1.5.7\"><span class=\"toc-item-num\">1.5.7&nbsp;&nbsp;</span>Lab: Multiclass</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Additional-neural-network-concepts\" data-toc-modified-id=\"Practice-quiz:-Additional-neural-network-concepts-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Practice quiz: Additional neural network concepts</a></span></li><li><span><a href=\"#Back-propagation-(Optional)\" data-toc-modified-id=\"Back-propagation-(Optional)-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Back propagation (Optional)</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-a-derivative-(Optional)\" data-toc-modified-id=\"What-is-a-derivative-(Optional)-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>What is a derivative (Optional)</a></span></li><li><span><a href=\"#Computation-graph-(Optional)\" data-toc-modified-id=\"Computation-graph-(Optional)-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>Computation graph (Optional)</a></span></li><li><span><a href=\"#Larger-neural-network-example-(Optional)\" data-toc-modified-id=\"Larger-neural-network-example-(Optional)-1.7.3\"><span class=\"toc-item-num\">1.7.3&nbsp;&nbsp;</span>Larger neural network example (Optional)</a></span></li><li><span><a href=\"#Optional-Lab:-Derivatives\" data-toc-modified-id=\"Optional-Lab:-Derivatives-1.7.4\"><span class=\"toc-item-num\">1.7.4&nbsp;&nbsp;</span>Optional Lab: Derivatives</a></span></li><li><span><a href=\"#Optional-Lab:-Back-propagation\" data-toc-modified-id=\"Optional-Lab:-Back-propagation-1.7.5\"><span class=\"toc-item-num\">1.7.5&nbsp;&nbsp;</span>Optional Lab: Back propagation</a></span></li></ul></li><li><span><a href=\"#Practice-lab:-Neural-network-training\" data-toc-modified-id=\"Practice-lab:-Neural-network-training-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Practice lab: Neural network training</a></span></li><li><span><a href=\"#Miscellaneous\" data-toc-modified-id=\"Miscellaneous-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Miscellaneous</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea2f80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T02:23:35.809829Z",
     "start_time": "2023-04-25T02:23:35.796262Z"
    }
   },
   "source": [
    "# Course 2 Week 2: Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616acd6",
   "metadata": {},
   "source": [
    "- Coursera course link: https://www.coursera.org/learn/advanced-learning-algorithms/home/week/2\n",
    "- Blog link: https://community.deeplearning.ai/c/mls/mls-course-2/mls-course-2-week-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fc172f",
   "metadata": {},
   "source": [
    "This week, we'll learn how to train our model in TensorFlow, and also learn about other important activation functions (besides the sigmoid function), and where to use each type in a neural network. We'll also learn how to go beyond binary classification to multiclass classification (3 or more categories). Multiclass classification will introduce us to a new activation function and a new loss function. Optionally, we can also learn about the difference between multiclass classification and multi-label classification. We'll learn about the Adam optimizer, and why it's an improvement upon regular gradient descent for neural network training. Finally, we will get a brief introduction to other layer types besides the one we've seen thus far.\n",
    "\n",
    "**Learning Objectives**\n",
    "- Train a neural network on data using TensorFlow\n",
    "- Understand the difference between various activation functions (sigmoid, ReLU, and linear)\n",
    "- Understand which activation functions to use for which type of layer\n",
    "- Understand why we need non-linear activation functions\n",
    "- Understand multiclass classification\n",
    "- Calculate the softmax activation for implementing multiclass classification\n",
    "- Use the categorical cross entropy loss function for multiclass classification\n",
    "- Use the recommended method for implementing multiclass classification in code\n",
    "- (Optional): Explain the difference between **multi-label** and **multiclass** classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815a505",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T02:23:46.855270Z",
     "start_time": "2023-04-25T02:23:46.836890Z"
    }
   },
   "source": [
    "## Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae7ecb",
   "metadata": {},
   "source": [
    "### Tensorflow implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682d47e",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/tensorflow_implementation.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab512660",
   "metadata": {},
   "source": [
    "**Steps involved in training a NN**\n",
    "1. Define the **NN architecture** (No. of layers, No. of units in each layer, and activatiion function)\n",
    "2. Specify the **loss function** (e.g., `BinaryCrossentropy()`, `MeanSquaredError()`) to compile the model\n",
    "3. **Fit** the NN model defined in step 1 using loss of the cost function specified in step 2 to the training data X, Y\n",
    "    - `epochs` will control the number of steps in gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eac9b7",
   "metadata": {},
   "source": [
    "### Training Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d63484",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_training_details_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed7426",
   "metadata": {},
   "source": [
    "<span style='color:blue'><b>Steps involved in training a logistic regression (Recap of Course 1) and comparison to training a NN</b></span>\n",
    "\n",
    "| Steps | Logistic Regression (Numpy based) | Neural Network (Tensorflow based) |\n",
    "|:-------|:---------------------|:----------------|\n",
    "| 1. Specify how to compute output, $f_{\\vec{w},b}(\\vec{x})$ given input $x$ and parameters $w$,$b$ <br> (a.k.a.) Define the model | `z = np.dot(w,x) + b`<br><br> `f_x = 1/(1+np.exp(-z))` |  `model = Sequential([`<br>`Dense(...),`<br>`Dense(...),`<br>`Dense(...),`<br>`])`  |\n",
    "| 2. Specify loss and cost function  <br><br> $L(f_{\\vec{w},b}(\\vec{x}),y)$  for 1 example  <br><br> $J(\\vec{w},b) = \\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)}),y^{(i)})$   |  logistic loss<br><br>`loss = -y * np.log(f_x)`<br>`-(1-y) * np.log(1-f_x)`   | binary cross entropy<br><br>`model.compile(`<br>`loss=BinaryCrossentropy())` |\n",
    "| 3. Train on data to minimize  $J(\\vec{w},b)$ |  `w = w - alpha * dj_dw`<br>`b = b - alpha * dj_db`  | `model.fit(X,y, epochs=100)` |\n",
    "\n",
    "Let's go through each of these steps in detail in the context of handwritten digit classification problem with NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab614d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_training_details_p2.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0896c7b5",
   "metadata": {},
   "source": [
    "Step 1: Create the NN model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352f352",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_training_details_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16160917",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T22:10:19.145786Z",
     "start_time": "2023-04-25T22:10:19.125616Z"
    }
   },
   "source": [
    "Step 2: Define Loss and Cost functions in the NN\n",
    "- Classification\n",
    "    - <span style=\"color:blue\"><b>Logistic Loss function (a.k.a.) Binary Cross Entropy </b></span> \n",
    "        - Most commonly used loss function for binary classification\n",
    "        - Same loss function that we had for logistic regression\n",
    "\n",
    "        - $L(f_{\\vec{w},b}(\\vec{x}),y) = -ylog(f(\\vec{x})) - (1-y)log(1-f(\\vec{x}))$\n",
    "\n",
    "- Regression\n",
    "    - <span style=\"color:blue\"><b> Squared Error Loss  (a.k.a.) Mean Squared Error </b></span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a266501",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_training_details_p4.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89268dce",
   "metadata": {},
   "source": [
    "Step 3: Gradient descent to minimise the cost function\n",
    "- We repeatedly update weights $w_{j}^{[l]}$ (i.e. weights in every layer $[l]$ and unit $j$) \n",
    "    - In order to do gradient descent, the key thing to compute is the partial derivatives using algorithm called back propagation\n",
    "- `model.fit(X,Y, epoch=100)` will take care of all of this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f39661",
   "metadata": {},
   "source": [
    "## Practice quiz: Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e440c23",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Practice-Quiz-Neural-Network-Training) screenshots for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862fc75b",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd35cd1a",
   "metadata": {},
   "source": [
    "### Alternatives to the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80da502",
   "metadata": {},
   "source": [
    "So far, we have been using sigmoid as the activation function in the hidden layers of NN, and even in output layer of NN (for binary classification).\n",
    "- Infact, we just created lot of logistic regression units (neurons) and connected them together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b38b8",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/alternatives_to_the_sigmoid_activation_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60652215",
   "metadata": {},
   "source": [
    "Rather than modelling awareness as a probability values (between 0 and 1) using sigmoid function, may be awareness should be any non-negative value (0 to a very large value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81772c64",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/alternatives_to_the_sigmoid_activation_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446cd8bb",
   "metadata": {},
   "source": [
    "**Commonly used activation functions in NN**\n",
    "- **Sigmoid**: $g(z) = \\large \\frac{1}{1+e^{-z}}$\n",
    "- Rectified Linear Unit (**ReLU**): $g(z) = \\text{max}(0,z)$\n",
    "- Linear activation function (**No activation function**): $g(z) = z$\n",
    "- Softmax activation function: *Covered later*\n",
    "\n",
    "**Other activation functions**\n",
    "- tanh\n",
    "- Leaky ReLU\n",
    "- Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edae8b1",
   "metadata": {},
   "source": [
    "### Choosing activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debed5a4",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/choosing_activation_functions_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f0f342",
   "metadata": {},
   "source": [
    "**Activation function $g(z)$ for output layer**\n",
    "- **Sigmoid** activation function for binary classification `y = 0/1`\n",
    "    - e.g., Handwritten digit 0 or 1 recognition\n",
    "- **Linear** activation function for regression `y = +/-`\n",
    "    - e.g., Profit\n",
    "- **ReLU** activation function for regression `y = 0 or +`\n",
    "    - e.g., House price, awareness of a brand/product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88458bf2",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/choosing_activation_functions_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e9a2a",
   "metadata": {},
   "source": [
    "**Activation function $g(z)$ for hiddden layers**\n",
    "- ReLU is the most common choice in NNs\n",
    "- <span style=\"color:red\">In early history of development of NN, sigmoid was more widely used</span>. But, <span style=\"color:green\">recently, ReLU is more widely adopted</span>. **Why?**\n",
    "    1. Computationally, ReLU is faster to compute, whereas sigmoid is slower as we need to compute exponentiation, inverse, etc.\n",
    "    2. <span style=\"color:blue\">Intuitive explanation: <b>ReLU goes flat only on one part of graph, whereas sigmoid goes flat on both sides of the graph. When we use gradient descent to train NN, if a function is flat in lot of places, gradient descent will be very slow (as the derivatives will be close to 0, weight updates will be very small) </b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a533458",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/choosing_activation_functions_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c885a55",
   "metadata": {},
   "source": [
    "### Why do we need activation functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab55d88",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/why_do_we_need_activation_functions_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d29af",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/why_do_we_need_activation_functions_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815dc582",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/why_do_we_need_activation_functions_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa0926f",
   "metadata": {},
   "source": [
    "**Why we should not use linear activations in hidden layers**\n",
    "- <span style=\"color:red\">If we use linear activation function for all hidden layers and output layer, then this NN model will be equivalent to linear regression</span>\n",
    "- <span style=\"color:red\">If we use linear activation function for all hidden layers, and sigmoid function for output layer, then this NN model will be equivalent to logistic regression</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d747197e",
   "metadata": {},
   "source": [
    "### Lab: ReLU activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0947cb7",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/optional-labs/C2_W2_Relu.ipynb) solutions and [my practice](greyhatguy007/optional-labs/C2_W2_Relu_Practice_TP.ipynb) for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9115cf",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Note: We re-create a <u>piecewise</u> linear function using a simple NN with the help of ReLU by leveraging it's non-linear behaviour  thus allowing us to turn off functions until they are needed</b></span>\n",
    "- with ReLU as the activation function of hidden layer 1 with 3 units\n",
    "- with linear activation function (or no activation function) for hidden layer 2  with 1 unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5697800e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T01:03:10.229788Z",
     "start_time": "2023-04-26T01:03:10.214385Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/c2_w2_lab_relu_activation_p1.png\" alt=\"Drawing\" style=\"width: 85%;\">\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "    <img src=\"attachments/c2_w2_lab_relu_activation_p2.png\" alt=\"Drawing\" style=\"width: 75%;\">\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "    <img src=\"attachments/c2_w2_lab_relu_activation_p3.png\" alt=\"Drawing\" style=\"width: 85%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4139b08",
   "metadata": {},
   "source": [
    "## Practice quiz: Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74bc3a",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Practice-Quiz-Activation-Functions) screenshots for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd6fcd",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a10ba59",
   "metadata": {},
   "source": [
    "### Multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a1f19b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/multiclass_p1.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1106aa1",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/multiclass_p2.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af38e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T22:41:52.092918Z",
     "start_time": "2023-04-28T22:41:52.070569Z"
    }
   },
   "source": [
    "Binary classification\n",
    "- We will predict probability of $y$ being 1 given $\\vec{x}$, $P(y=1|\\vec{x})$\n",
    "\n",
    "Multiclass classification\n",
    "- Given $\\vec{x}$, We will predict\n",
    "    - probability of $y$ being 1, $P(y=1|\\vec{x})$\n",
    "    - probability of $y$ being 2, $P(y=2|\\vec{x})$\n",
    "    - probability of $y$ being 3, $P(y=3|\\vec{x})$\n",
    "    - probability of $y$ being 4, $P(y=4|\\vec{x})$\n",
    "    \n",
    ">  The multiclass classification algo can learn a decision boundary that maybe looks like this that divides the space exploded into four categories rather than just two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e96ed1",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5700fe",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/softmax_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf0360",
   "metadata": {},
   "source": [
    "Softmax logistic regression (multi-class classification) is a generlization of logistic regeression (binary classification algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5cfd3a",
   "metadata": {},
   "source": [
    "| Notation  | Logistic Regression ($2$ possible outputs) | Softmax regeression ($N$ possible outputs) |\n",
    "|:---|:--------------------------------------------|:--------------------------------------------|\n",
    "| $y$ | $y=0,1$ | $y=0,1,2,3, ...\\text{N}$ |\n",
    "| $z$  | $z=\\vec{w}\\cdot\\vec{x}+b$  | $z_{j} = \\vec{w_{j}}\\cdot\\vec{x}+b_{j}$ |\n",
    "|  $a$ |  $a=\\large\\frac{1}{1+e^{-z}}$ | $a_{j}= \\large\\frac{e^{z_{j}}}{\\sum_{k=1}^{N}{e^{z_{j}}}}$ |\n",
    "| $a_{j}$ | $\\text{For 2 possible outputs}$ <br> $a_{1}=\\frac{1}{1+e^{-z_{1}}} = P(y=1\\mid \\vec{x})$ <br> $a_{2}=1-a_{1} = P(y=0\\mid \\vec{x})$ |  $\\text{For 4 possible outputs, y=1,2,3,4}$ <br> $a_{1} = \\large\\frac{e^{z_{1}}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}} \\small = P(y=1\\mid \\vec{x})$ <br> $a_{2} = \\large\\frac{e^{z_{2}}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}} \\small = P(y=2\\mid \\vec{x})$ <br> $a_{3} = \\large\\frac{e^{z_{3}}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}} \\small = P(y=3\\mid \\vec{x})$ <br> $a_{4} = \\large\\frac{e^{z_{4}}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}} \\small = P(y=4\\mid \\vec{x})$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee47a82e",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/softmax_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1d552",
   "metadata": {},
   "source": [
    "### Neural network with softmax output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e801f15",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_with_softmax_output_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f87ade",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1b51c22",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_with_softmax_output_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c2f48c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c639aeac",
   "metadata": {},
   "source": [
    "### Improved implementation with Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc980b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/improved_implementation_of_softmax_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481bb96",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75bfd6cc",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/improved_implementation_of_softmax_p4.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d8b41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38efd3d8",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/improved_implementation_of_softmax_p5.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402d7f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3efeb94e",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/improved_implementation_of_softmax_p6.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015a43f",
   "metadata": {},
   "source": [
    "### Classification with multiple outputs (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3efd21",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/classification_with_multiple_outputs_p1.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6068cd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0decd115",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/classification_with_multiple_outputs_p2.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb12f5d",
   "metadata": {},
   "source": [
    "### Lab: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716141cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "126bbd68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8dc3fde",
   "metadata": {},
   "source": [
    "### Lab: Multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc0523",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "111c92ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ac1e468",
   "metadata": {},
   "source": [
    "## Practice quiz: Additional neural network concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358bc084",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffcb1e52",
   "metadata": {},
   "source": [
    "## Back propagation (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da2544",
   "metadata": {},
   "source": [
    "### What is a derivative (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3e24e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32b5f4dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ebfeccd",
   "metadata": {},
   "source": [
    "### Computation graph (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9676ac3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14084379",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf307520",
   "metadata": {},
   "source": [
    "### Larger neural network example (Optional) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b07f8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a744cc15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15a98f28",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8a05f00",
   "metadata": {},
   "source": [
    "### Optional Lab: Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9554e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88636a6a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b034958a",
   "metadata": {},
   "source": [
    "### Optional Lab: Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841d9aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3af7b5a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edaad3af",
   "metadata": {},
   "source": [
    "## Practice lab: Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3296760e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbcc2004",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "171f10db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1d01199",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e51df4dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T00:41:26.029599Z",
     "start_time": "2023-04-26T00:41:26.014383Z"
    }
   },
   "outputs": [],
   "source": [
    "# Kernel crashes half-way through. Found issue by running the codes in python command line \n",
    "# https://stackoverflow.com/questions/20554074/sklearn-omp-error-15-initializing-libiomp5md-dll-but-found-mk2iomp5md-dll-a\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f33a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ga]",
   "language": "python",
   "name": "conda-env-ga-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
