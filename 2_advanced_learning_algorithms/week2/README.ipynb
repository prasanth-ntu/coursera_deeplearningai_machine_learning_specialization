{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fd86250",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Course-2-Week-2:-Neural-network-training\" data-toc-modified-id=\"Course-2-Week-2:-Neural-network-training-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Course 2 Week 2: Neural network training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Neural-network-training\" data-toc-modified-id=\"Neural-network-training-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Neural network training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tensorflow-implementation\" data-toc-modified-id=\"Tensorflow-implementation-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Tensorflow implementation</a></span></li><li><span><a href=\"#Training-Details\" data-toc-modified-id=\"Training-Details-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Training Details</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Neural-network-training\" data-toc-modified-id=\"Practice-quiz:-Neural-network-training-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Practice quiz: Neural network training</a></span></li><li><span><a href=\"#Activation-functions\" data-toc-modified-id=\"Activation-functions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Activation functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Alternatives-to-the-sigmoid-function\" data-toc-modified-id=\"Alternatives-to-the-sigmoid-function-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Alternatives to the sigmoid function</a></span></li><li><span><a href=\"#Choosing-activation-functions\" data-toc-modified-id=\"Choosing-activation-functions-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Choosing activation functions</a></span></li><li><span><a href=\"#Why-do-we-need-activation-functions?\" data-toc-modified-id=\"Why-do-we-need-activation-functions?-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Why do we need activation functions?</a></span></li><li><span><a href=\"#Lab:-ReLU-activation\" data-toc-modified-id=\"Lab:-ReLU-activation-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Lab: ReLU activation</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Activation-functions\" data-toc-modified-id=\"Practice-quiz:-Activation-functions-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Practice quiz: Activation functions</a></span></li><li><span><a href=\"#Multiclass-Classification\" data-toc-modified-id=\"Multiclass-Classification-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Multiclass Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Multiclass\" data-toc-modified-id=\"Multiclass-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Multiclass</a></span></li><li><span><a href=\"#Softmax\" data-toc-modified-id=\"Softmax-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Softmax</a></span></li><li><span><a href=\"#Neural-network-with-softmax-output\" data-toc-modified-id=\"Neural-network-with-softmax-output-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Neural network with softmax output</a></span></li><li><span><a href=\"#Improved-implementation-with-Softmax\" data-toc-modified-id=\"Improved-implementation-with-Softmax-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>Improved implementation with Softmax</a></span></li><li><span><a href=\"#Classification-with-multiple-outputs-(Optional)\" data-toc-modified-id=\"Classification-with-multiple-outputs-(Optional)-1.5.5\"><span class=\"toc-item-num\">1.5.5&nbsp;&nbsp;</span>Classification with multiple outputs (Optional)</a></span></li><li><span><a href=\"#Lab:-Softmax\" data-toc-modified-id=\"Lab:-Softmax-1.5.6\"><span class=\"toc-item-num\">1.5.6&nbsp;&nbsp;</span>Lab: Softmax</a></span></li><li><span><a href=\"#Lab:-Multiclass\" data-toc-modified-id=\"Lab:-Multiclass-1.5.7\"><span class=\"toc-item-num\">1.5.7&nbsp;&nbsp;</span>Lab: Multiclass</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Multiclass-Classification\" data-toc-modified-id=\"Practice-quiz:-Multiclass-Classification-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Practice quiz: Multiclass Classification</a></span></li><li><span><a href=\"#Additional-Neural-Network-Concepts\" data-toc-modified-id=\"Additional-Neural-Network-Concepts-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Additional Neural Network Concepts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Advanced-Optimization\" data-toc-modified-id=\"Advanced-Optimization-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Advanced Optimization</a></span></li><li><span><a href=\"#Additional-Layer-Types\" data-toc-modified-id=\"Additional-Layer-Types-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>Additional Layer Types</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Additional-neural-network-concepts\" data-toc-modified-id=\"Practice-quiz:-Additional-neural-network-concepts-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Practice quiz: Additional neural network concepts</a></span></li><li><span><a href=\"#Back-propagation-(Optional)\" data-toc-modified-id=\"Back-propagation-(Optional)-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Back propagation (Optional)</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-a-derivative-(Optional)\" data-toc-modified-id=\"What-is-a-derivative-(Optional)-1.9.1\"><span class=\"toc-item-num\">1.9.1&nbsp;&nbsp;</span>What is a derivative (Optional)</a></span></li><li><span><a href=\"#Computation-graph-(Optional)\" data-toc-modified-id=\"Computation-graph-(Optional)-1.9.2\"><span class=\"toc-item-num\">1.9.2&nbsp;&nbsp;</span>Computation graph (Optional)</a></span></li><li><span><a href=\"#Larger-neural-network-example-(Optional)\" data-toc-modified-id=\"Larger-neural-network-example-(Optional)-1.9.3\"><span class=\"toc-item-num\">1.9.3&nbsp;&nbsp;</span>Larger neural network example (Optional)</a></span></li><li><span><a href=\"#Optional-Lab:-Derivatives\" data-toc-modified-id=\"Optional-Lab:-Derivatives-1.9.4\"><span class=\"toc-item-num\">1.9.4&nbsp;&nbsp;</span>Optional Lab: Derivatives</a></span></li><li><span><a href=\"#Optional-Lab:-Back-propagation\" data-toc-modified-id=\"Optional-Lab:-Back-propagation-1.9.5\"><span class=\"toc-item-num\">1.9.5&nbsp;&nbsp;</span>Optional Lab: Back propagation</a></span></li></ul></li><li><span><a href=\"#Practice-lab:-Neural-network-training\" data-toc-modified-id=\"Practice-lab:-Neural-network-training-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Practice lab: Neural network training</a></span></li><li><span><a href=\"#Miscellaneous\" data-toc-modified-id=\"Miscellaneous-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Miscellaneous</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea2f80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T02:23:35.809829Z",
     "start_time": "2023-04-25T02:23:35.796262Z"
    }
   },
   "source": [
    "# Course 2 Week 2: Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616acd6",
   "metadata": {},
   "source": [
    "- Coursera course link: https://www.coursera.org/learn/advanced-learning-algorithms/home/week/2\n",
    "- Blog link: https://community.deeplearning.ai/c/mls/mls-course-2/mls-course-2-week-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fc172f",
   "metadata": {},
   "source": [
    "This week, we'll learn how to train our model in TensorFlow, and also learn about other important activation functions (besides the sigmoid function), and where to use each type in a neural network. We'll also learn how to go beyond binary classification to multiclass classification (3 or more categories). Multiclass classification will introduce us to a new activation function and a new loss function. Optionally, we can also learn about the difference between multiclass classification and multi-label classification. We'll learn about the Adam optimizer, and why it's an improvement upon regular gradient descent for neural network training. Finally, we will get a brief introduction to other layer types besides the one we've seen thus far.\n",
    "\n",
    "**Learning Objectives**\n",
    "- Train a neural network on data using TensorFlow\n",
    "- Understand the difference between various activation functions (sigmoid, ReLU, and linear)\n",
    "- Understand which activation functions to use for which type of layer\n",
    "- Understand why we need non-linear activation functions\n",
    "- Understand multiclass classification\n",
    "- Calculate the softmax activation for implementing multiclass classification\n",
    "- Use the categorical cross entropy loss function for multiclass classification\n",
    "- Use the recommended method for implementing multiclass classification in code\n",
    "- (Optional): Explain the difference between **multi-label** and **multiclass** classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815a505",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T02:23:46.855270Z",
     "start_time": "2023-04-25T02:23:46.836890Z"
    }
   },
   "source": [
    "## Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae7ecb",
   "metadata": {},
   "source": [
    "### Tensorflow implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682d47e",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/tensorflow_implementation.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab512660",
   "metadata": {},
   "source": [
    "**Steps involved in training a NN**\n",
    "1. Define the **NN architecture** (No. of layers, No. of units in each layer, and activatiion function)\n",
    "2. Specify the **loss function** (e.g., `BinaryCrossentropy()`, `MeanSquaredError()`) to compile the model\n",
    "3. **Fit** the NN model defined in step 1 using loss of the cost function specified in step 2 to the training data X, Y\n",
    "    - `epochs` will control the number of steps in gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eac9b7",
   "metadata": {},
   "source": [
    "### Training Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d63484",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_training_details_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed7426",
   "metadata": {},
   "source": [
    "<span style='color:blue'><b>Steps involved in training a logistic regression (Recap of Course 1) and comparison to training a NN</b></span>\n",
    "\n",
    "| Steps | Logistic Regression (Numpy based) | Neural Network (Tensorflow based) |\n",
    "|:-------|:---------------------|:----------------|\n",
    "| 1. Specify how to compute output, $f_{\\vec{w},b}(\\vec{x})$ given input $x$ and parameters $w$,$b$ <br> (a.k.a.) Define the model | `z = np.dot(w,x) + b`<br><br> `f_x = 1/(1+np.exp(-z))` |  `model = Sequential([`<br>`Dense(...),`<br>`Dense(...),`<br>`Dense(...),`<br>`])`  |\n",
    "| 2. Specify loss and cost function  <br><br> $L(f_{\\vec{w},b}(\\vec{x}),y)$  for 1 example  <br><br> $J(\\vec{w},b) = \\frac{1}{m}\\sum_{i=1}^{m}(f_{\\vec{w},b}(\\vec{x}^{(i)}),y^{(i)})$   |  logistic loss<br><br>`loss = -y * np.log(f_x)`<br>`-(1-y) * np.log(1-f_x)`   | binary cross entropy<br><br>`model.compile(`<br>`loss=BinaryCrossentropy())` |\n",
    "| 3. Train on data to minimize  $J(\\vec{w},b)$ |  `w = w - alpha * dj_dw`<br>`b = b - alpha * dj_db`  | `model.fit(X,y, epochs=100)` |\n",
    "\n",
    "Let's go through each of these steps in detail in the context of handwritten digit classification problem with NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab614d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_training_details_p2.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0896c7b5",
   "metadata": {},
   "source": [
    "Step 1: Create the NN model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352f352",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_training_details_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16160917",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T22:10:19.145786Z",
     "start_time": "2023-04-25T22:10:19.125616Z"
    }
   },
   "source": [
    "Step 2: Define Loss and Cost functions in the NN\n",
    "- Classification\n",
    "    - <span style=\"color:blue\"><b>Logistic Loss function (a.k.a.) Binary Cross Entropy </b></span> \n",
    "        - Most commonly used loss function for binary classification\n",
    "        - Same loss function that we had for logistic regression\n",
    "\n",
    "        - $L(f_{\\vec{w},b}(\\vec{x}),y) = -ylog(f(\\vec{x})) - (1-y)log(1-f(\\vec{x}))$\n",
    "\n",
    "- Regression\n",
    "    - <span style=\"color:blue\"><b> Squared Error Loss  (a.k.a.) Mean Squared Error </b></span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a266501",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_training_details_p4.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89268dce",
   "metadata": {},
   "source": [
    "Step 3: Gradient descent to minimise the cost function\n",
    "- We repeatedly update weights $w_{j}^{[l]}$ (i.e. weights in every layer $[l]$ and unit $j$) \n",
    "    - In order to do gradient descent, the key thing to compute is the partial derivatives using algorithm called back propagation\n",
    "- `model.fit(X,Y, epoch=100)` will take care of all of this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f39661",
   "metadata": {},
   "source": [
    "## Practice quiz: Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e440c23",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Practice-Quiz-Neural-Network-Training) screenshots for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862fc75b",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd35cd1a",
   "metadata": {},
   "source": [
    "### Alternatives to the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80da502",
   "metadata": {},
   "source": [
    "So far, we have been using sigmoid as the activation function in the hidden layers of NN, and even in output layer of NN (for binary classification).\n",
    "- Infact, we just created lot of logistic regression units (neurons) and connected them together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b38b8",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/alternatives_to_the_sigmoid_activation_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60652215",
   "metadata": {},
   "source": [
    "Rather than modelling awareness as a probability values (between 0 and 1) using sigmoid function, may be awareness should be any non-negative value (0 to a very large value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81772c64",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/alternatives_to_the_sigmoid_activation_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446cd8bb",
   "metadata": {},
   "source": [
    "**Commonly used activation functions in NN**\n",
    "- **Sigmoid**: $g(z) = \\large \\frac{1}{1+e^{-z}}$\n",
    "- Rectified Linear Unit (**ReLU**): $g(z) = \\text{max}(0,z)$\n",
    "- Linear activation function (**No activation function**): $g(z) = z$\n",
    "- Softmax activation function: *Covered later*\n",
    "\n",
    "**Other activation functions**\n",
    "- tanh\n",
    "- Leaky ReLU\n",
    "- Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edae8b1",
   "metadata": {},
   "source": [
    "### Choosing activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debed5a4",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/choosing_activation_functions_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f0f342",
   "metadata": {},
   "source": [
    "**Activation function $g(z)$ for output layer**\n",
    "- **Sigmoid** activation function for binary classification `y = 0/1`\n",
    "    - e.g., Handwritten digit 0 or 1 recognition\n",
    "- **Linear** activation function for regression `y = +/-`\n",
    "    - e.g., Profit\n",
    "- **ReLU** activation function for regression `y = 0 or +`\n",
    "    - e.g., House price, awareness of a brand/product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88458bf2",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/choosing_activation_functions_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e9a2a",
   "metadata": {},
   "source": [
    "**Activation function $g(z)$ for hiddden layers**\n",
    "- ReLU is the most common choice in NNs\n",
    "- <span style=\"color:red\">In early history of development of NN, sigmoid was more widely used</span>. But, <span style=\"color:green\">recently, ReLU is more widely adopted</span>. **Why?**\n",
    "    1. Computationally, ReLU is faster to compute, whereas sigmoid is slower as we need to compute exponentiation, inverse, etc.\n",
    "    2. <span style=\"color:blue\">Intuitive explanation: <b>ReLU goes flat only on one part of graph, whereas sigmoid goes flat on both sides of the graph. When we use gradient descent to train NN, if a function is flat in lot of places, gradient descent will be very slow (as the derivatives will be close to 0, weight updates will be very small) </b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a533458",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/choosing_activation_functions_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c885a55",
   "metadata": {},
   "source": [
    "### Why do we need activation functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab55d88",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/why_do_we_need_activation_functions_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d29af",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/why_do_we_need_activation_functions_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815dc582",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/why_do_we_need_activation_functions_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa0926f",
   "metadata": {},
   "source": [
    "**Why we should not use linear activations in hidden layers**\n",
    "- <span style=\"color:red\">If we use linear activation function for all hidden layers and output layer, then this NN model will be equivalent to linear regression</span>\n",
    "- <span style=\"color:red\">If we use linear activation function for all hidden layers, and sigmoid function for output layer, then this NN model will be equivalent to logistic regression</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d747197e",
   "metadata": {},
   "source": [
    "### Lab: ReLU activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0947cb7",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/optional-labs/C2_W2_Relu.ipynb) solutions and [my practice](greyhatguy007/optional-labs/C2_W2_Relu_Practice_TP.ipynb) for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9115cf",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Note: We re-create a <u>piecewise</u> linear function using a simple NN with the help of ReLU by leveraging it's non-linear behaviour  thus allowing us to turn off functions until they are needed</b></span>\n",
    "- with ReLU as the activation function of hidden layer 1 with 3 units\n",
    "- with linear activation function (or no activation function) for hidden layer 2  with 1 unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5697800e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T01:03:10.229788Z",
     "start_time": "2023-04-26T01:03:10.214385Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/c2_w2_lab_relu_activation_p1.png\" alt=\"Drawing\" style=\"width: 85%;\">\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "    <img src=\"attachments/c2_w2_lab_relu_activation_p2.png\" alt=\"Drawing\" style=\"width: 75%;\">\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "    <img src=\"attachments/c2_w2_lab_relu_activation_p3.png\" alt=\"Drawing\" style=\"width: 85%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4139b08",
   "metadata": {},
   "source": [
    "## Practice quiz: Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d74bc3a",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Practice-Quiz-Activation-Functions) screenshots for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd6fcd",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a10ba59",
   "metadata": {},
   "source": [
    "### Multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a1f19b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/multiclass_p1.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1106aa1",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/multiclass_p2.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2817d8df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T22:41:52.092918Z",
     "start_time": "2023-04-28T22:41:52.070569Z"
    }
   },
   "source": [
    "Binary classification\n",
    "- We will predict probability of $y$ being 1 given $\\vec{x}$, $P(y=1|\\vec{x})$\n",
    "\n",
    "Multiclass classification\n",
    "- Given $\\vec{x}$, We will predict\n",
    "    - probability of $y$ being 1, $P(y=1|\\vec{x})$\n",
    "    - probability of $y$ being 2, $P(y=2|\\vec{x})$\n",
    "    - probability of $y$ being 3, $P(y=3|\\vec{x})$\n",
    "    - probability of $y$ being 4, $P(y=4|\\vec{x})$\n",
    "    \n",
    ">  The multiclass classification algo can learn a decision boundary that maybe looks like this that divides the space exploded into four categories rather than just two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e96ed1",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5700fe",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/softmax_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf0360",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Softmax logistic regression (multi-class classification) is a generlization of logistic regeression (binary classification algo)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d20d66c",
   "metadata": {},
   "source": [
    "| Notation  | Logistic Regression ($2$ possible outputs) | Softmax regeression ($N$ possible outputs) |\n",
    "|:---|:--------------------------------------------|:--------------------------------------------|\n",
    "| $y$ | $y=0,1$ | $y=0,1,2,3, ...\\text{N}$ |\n",
    "| $z$  | $z=\\vec{w}\\cdot\\vec{x}+b$  | $z_{j} = \\vec{w_{j}}\\cdot\\vec{x}+b_{j}$ <br><br> parameters: $w_1, w_2, \\cdots, w_N$  and $b_1, b_2, \\cdots, b_N$|\n",
    "|  $a$ |  $a=g(z) = \\large\\frac{1}{1+e^{-z}} \\small = P(y=1\\mid \\vec{x}) $ | $a_{j}= \\large\\frac{e^{z_{j}}}{\\sum_{k=1}^{N}{e^{z_{k}}}} \\small = P(y=j\\mid \\vec{x})$ <br><br> $\\text{Note: } a_1 + a_2 + \\cdots + a_N = 1$  |\n",
    "| $a_{j}$ | $\\text{For 2 possible outputs}$ <br><br> $a_{1} = \\large \\frac{1}{1+e^{-z}} \\small = P(y=1\\mid \\vec{x}) = 0.71$ <br> $a_{2}=1-a_{1} = P(y=0\\mid \\vec{x}) = 0.29$ |  $\\text{For 4 possible outputs, y=1,2,3,4}$ <br><br> $a_{1} = \\large\\frac{e^{z_{1}}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}} \\small = P(y=1\\mid \\vec{x}) = 0.30$ <br><br> $a_{2} = \\large\\frac{e^{z_{2}}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}} \\small = P(y=2\\mid \\vec{x}) = 0.20$ <br><br> $a_{3} = \\large\\frac{e^{z_{3}}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}} \\small = P(y=3\\mid \\vec{x}) = 0.15$ <br><br> $a_{4} = \\large\\frac{e^{z_{4}}}{e^{z_1}+e^{z_2}+e^{z_3}+e^{z_4}} \\small = P(y=4\\mid \\vec{x}) = 0.35$ |\n",
    "| loss function <br> of a single training example | $\\text{loss} = -y\\text{log}(a_1) -(1-y)\\text{log}(1-a_1) $ <br> $\\text{loss} = -y\\text{log}(a_1) - (1-y)\\text{log}(a_2) $ <br> $\\text{loss} = \\begin{cases} -log(a_1) & \\text{ if } y=1 \\\\  -log(a_2) & \\text{ if } y=0 \\end{cases}$ | <span style=\"color:blue\"><b>Cross-entropy loss</b> - Loss fn used for multi-class classification</span><br>$\\text{loss}(a_1,a_2, \\cdots, a_N, y) = \\begin{cases} -log(a_1) & \\text{ if } y=1 \\\\  -log(a_2) & \\text{ if } y=2 \\\\ & \\vdots \\\\ -log(a_N) & \\text{ if } y=N \\\\ \\end{cases}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be3dd58",
   "metadata": {},
   "source": [
    "Softmax regression loss function, $loss = -\\text{log}(a_j) \\text{ if } y=j$\n",
    "- Example,\n",
    "    - if $a_j \\approx  1$, then loss will be very small\n",
    "    - if $a_j \\approx  0.5$, then loss will be slightly bigger\n",
    "    - <span style=\"color:blue\">smaller the $a_j$ value is, bigger the loss</span>\n",
    "- **The above property incentivises the softmax algorithm to make a $a_j$ as large (as close to $1$) as possible**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee47a82e",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/softmax_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d12ce",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><i>Note</i>: If we apply softmax regression where $\\text{N}=2$, then softmax regression ends up computing  logistic regression, <b>though parameters end up being slightly different</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1d552",
   "metadata": {},
   "source": [
    "### Neural network with softmax output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e801f15",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "     <img src=\"attachments/neural_network_with_softmax_output_p0.png\" alt=\"Drawing\" style=\"width: 25%;\">\n",
    "    <img src=\"attachments/neural_network_with_softmax_output_p1.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f87ade",
   "metadata": {},
   "source": [
    "Let's take the handwritten digit recognition example, and convert from 2-class classification to 10-class classificaiton.\n",
    "- For 2-class classification, the final layer had sigmoid activation function\n",
    "- For 10-class classification, the final layer needs softmax activation function. In other words, this **NN has softmax output layer**\n",
    "    - Layer 1: Activation will be computed similar to NN with sigmoid output layer\n",
    "    - Layer 2: Activation will be computed similar to NN with sigmoid output layer\n",
    "    - Layer 3:\n",
    "        - Notations\n",
    "            - $z_1^{[3]} = \\vec{w}_1^{[3]} \\cdot a^{[2]} + b_1^{[3]} \\rightarrow a_1^{[3]} = \\large \\frac{e^{z_1^{[3]}}}{e^{z_1^{[3]}}+\\cdots+e^{z_{10}^{[3]}}} \\small = P(y=1|\\vec{x})$\n",
    "            - $\\vdots$\n",
    "            - $z_{10}^{[3]} = \\vec{w}_{10}^{[3]} \\cdot a^{[2]} + b_{10}^{[3]} \\rightarrow a_{10}^{[3]} = \\large \\frac{e^{z_{10}^{[3]}}}{e^{z_1^{[3]}}+\\cdots+e^{z_{10}^{[3]}}} \\small = P(y=10|\\vec{x})$\n",
    "        - **Interestingly**\n",
    "            - **In Logistic regression**: \n",
    "                - $a_1^{[3]} = g(z_1^{[3]})$ and  $a_2^{[3]} = g(z_2^{[3]}) \\Rightarrow$ **Each of these activation values, $\\mathbf{a_j}$ depends only on corresponding values of $\\mathbf{z_j}$**\n",
    "                - **For sigmoid, relu and linear activation functions, we can perform element-wise operation**  \n",
    "                    - activation $a_1$ is only a function of $z_1$, and \n",
    "                    - activation $a_2$ is only a function of $z_2$\n",
    "            - **In Softmax**\n",
    "                - $a_1^{[3]}$ is a function of $z_1, z_2, \\cdots, z_{10} \\Rightarrow$ **Each of these activation values depends on of the values of $\\mathbf{z}$**, a unique property of softmax. \n",
    "                - $\\vec{a^{[3]}} = (a_1^{[3]}, \\cdots, a_{10}^{[3]}) = g(z_1^{[3]}, \\cdots, z{10}^{[3]}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b51c22",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_with_softmax_output_p2.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c2f48c",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><b>Don't use this version of code. Better version can be found in next section</b></span>\n",
    "\n",
    "- The softmax based cost function is called as `SparseCategoricalCrossentropy` by TensorFlow, whereas \n",
    "    - `Categorical` - Because, we are classifying $y$ into categories $1 \\cdots 10$\n",
    "    - `Sparse` - $y$ will be a sparse matrix, as it can take on only one of the 10 values\n",
    "- For logistic regression, it's called as `BinaryCrossentropy`. \n",
    "\n",
    "For more details, refer https://www.tensorflow.org/api_docs/python/tf/keras/losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639aeac",
   "metadata": {},
   "source": [
    "### Improved implementation with Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603cef87",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/improved_implementation_of_softmax_p1.png\" alt=\"Drawing\" style=\"width: 35%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99cc3e34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T00:35:50.872051Z",
     "start_time": "2023-04-29T00:35:50.858063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1=0.000200000000000000\n",
      "x2=0.000199999999999978\n",
      "x1==x2: False\n"
     ]
    }
   ],
   "source": [
    "# This is equivalent to passing the formula directly into TF\n",
    "x1 = 2.0/10000\n",
    "print (f\"x1={x1:.18f}\")\n",
    "\n",
    "# This is equivalent to computing activation function, a as a intermediate quantity in TF\n",
    "x2 = (1 + 1/10000) - (1 - 1/10000) \n",
    "print (f\"x2={x2:.18f}\")\n",
    "\n",
    "print (f\"x1==x2: {x1==x2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895d342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T00:35:22.191580Z",
     "start_time": "2023-04-29T00:35:22.177562Z"
    }
   },
   "source": [
    "This example show cases the round off error with floating point numbers. <span style=\"color:red\">As computer uses finite memory to store them, <b>depending on how we compute the values, we migth end up with a different numerical round off errors</b>.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481bb96",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Instead of computing activation value and loss in separate steps</span><span style=\"color:green\">, if we compute them together in a single expression, <b>Tensorflow can come up wth a more numerically accurate way to compute this loss function.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc980b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/improved_implementation_of_softmax_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014adf6",
   "metadata": {},
   "source": [
    "| Logistic Regression | Less accurate  | More accurate  |\n",
    "| :-- | :---------------- | :---------------- |\n",
    "| Logistic loss |  $a = g(z) = \\large \\frac{1}{1+e^{-z}}$ <br> $loss = -y\\text{log}(a) - (1-y)\\text{log}(1-a)$ | $loss = \\large -\\text{log}\\left(\\frac{1}{1+e^{-z}}\\right) - (1-y)\\text{log}\\left(1-\\frac{1}{1+e^{-z}}\\right)$  |\n",
    "| TF: Output layer | `Dense(units=1, activation='sigmoid)` <br><br> **Computes probability** | `Dense(units-1, activation='linear')`  <br><br> **Computes $\\mathbf{z}$**|\n",
    "| TF: Loss function | `model.compile(loss=BinaryCrossEntropy()`  | `model.compile(loss=BinaryCrossEntropy(from_logits=True)`<br><br> where `logit:` $z$|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bfd6cc",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/improved_implementation_of_softmax_p4.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000c92b",
   "metadata": {},
   "source": [
    "**TF Code Example with MNIST**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efeb94e",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/improved_implementation_of_softmax_p6.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd10aa",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Since, we have changed the activation of the last layer to `linear` function, the logistic regression model now outputs <u>logits or log-odds</u> ($z$) and not probability ($a$).</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38efd3d8",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/improved_implementation_of_softmax_p5.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbf69a",
   "metadata": {},
   "source": [
    "Since, we have changed the activation of the last layer to `linear` function, the NN model now outputs $z_1, \\cdots, z_{10}$ and not $a_1, \\cdots, a_{10}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015a43f",
   "metadata": {},
   "source": [
    "### Classification with multiple outputs (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c908aaac",
   "metadata": {},
   "source": [
    "**Multi-class vs. multi-label**\n",
    "- Multi-class: Predict $1$ of $N$ classes\n",
    "- Multi-label: Predict $1$ or more classes of $N$ classes\n",
    "    - e.g., Detect the items in front of the car (e.g., car, bus, pedestrian, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3efd21",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/classification_with_multiple_outputs_p1.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0decd115",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/classification_with_multiple_outputs_p2.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba85e15",
   "metadata": {},
   "source": [
    "**How to build multi-label classification?**\n",
    "- Approach 1: Treat this as 3 seperate ML problems\n",
    "- Approach 2: Train one neural network to predict all the three outputs (labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb12f5d",
   "metadata": {},
   "source": [
    "### Lab: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff51ff2",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/optional-labs/C2_W2_SoftMax.ipynb) solutions and [my practice](greyhatguy007/optional-labs/C2_W2_SoftMax_Practice_TP.ipynb) for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716141cb",
   "metadata": {},
   "source": [
    "**Note**\n",
    "In both **softmax regression** and **neural networks with Softmax outputs**, N outputs are generated and one output is selected as the predicted category. In both cases a vector $\\mathbf{z}$ is generated by a linear function which is applied to a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. **After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as probabilities**. The larger inputs  will correspond to larger output probabilities.\n",
    "<center>  <img  src=\"greyhatguy007/optional-labs/images/C2_W2_SoftmaxReg_NN.png\" width=\"700\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126bbd68",
   "metadata": {},
   "source": [
    "The softmax function can be written:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }} \\tag{1}$$\n",
    "The output $\\mathbf{a}$ is a vector of length N, so for softmax regression, you could also write:\n",
    "\\begin{align}\n",
    "\\mathbf{a}(x) =\n",
    "\\begin{bmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = N | \\mathbf{x}; \\mathbf{w},b)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{k=1}^{N}{e^{z_k} }}\n",
    "\\begin{bmatrix}\n",
    "e^{z_1} \\\\\n",
    "\\vdots \\\\\n",
    "e^{z_{N}} \\\\\n",
    "\\end{bmatrix} \\tag{2}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202edcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T03:47:02.809520Z",
     "start_time": "2023-04-29T03:47:02.795560Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/lab_softmax_properties_p1.png\" alt=\"Drawing\" style=\"width: 95%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff5409",
   "metadata": {},
   "source": [
    "<center> <img  src=\"greyhatguy007/optional-labs/images/C2_W2_SoftMaxCost.png\" width=\"600\" />    <center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e8463",
   "metadata": {},
   "source": [
    "The loss function associated with Softmax, the cross-entropy loss, is:\n",
    "\\begin{equation}\n",
    "  L(\\mathbf{a},y)=\\begin{cases}\n",
    "    -log(a_1), & \\text{if $y=1$}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(a_N), & \\text{if $y=N$}\n",
    "  \\end{cases} \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Where y is the target category for this example and $\\mathbf{a}$ is the output of a softmax function. In particular, the values in $\\mathbf{a}$ are probabilities that sum to one.\n",
    ">**Recall:** In this course, Loss is for one example while Cost covers all examples. \n",
    " \n",
    " \n",
    "<span style=\"color:blue\">Note in (3) above, <b>only the line that corresponds to the target contributes to the loss, other lines are zero</b>. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise.</span> \n",
    "    $$\\mathbf{1}\\{y == n\\} = =\\begin{cases}\n",
    "    1, & \\text{if $y==n$}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}$$\n",
    "Now the cost is:\n",
    "\\begin{align}\n",
    "J(\\mathbf{w},b) = -\\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right] \\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "Where $m$ is the number of examples, $N$ is the number of outputs. This is the average of all the losses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90d939",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746bdda7",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">This lab discussed two ways of implementing the softmax, cross-entropy loss in Tensorflow, </span>\n",
    "1. the **'obvious'** method - most straightforward \n",
    "2. the **'preferred'** method - more numerically stable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8bec3b",
   "metadata": {},
   "source": [
    "**1. Obvious**\n",
    "\n",
    "The model is implemented with the `softmax` as an activation in the final Dense layer.\n",
    "The loss function is separately specified in the `compile` directive. \n",
    "\n",
    "The loss function is `SparseCategoricalCrossentropy`. This loss is described in (3) above. In this model, the softmax takes place in the last layer. The loss function takes in the softmax output which is a vector of probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e7328a",
   "metadata": {},
   "source": [
    "**2. Preferred**\n",
    "\n",
    "<span style=\"color:blue\">In the preferred method, the final layer has a `linear` activation. For historical reasons, the outputs in this form are referred to as *logits*. The loss function has an additional argument: `from_logits = True`. This informs the loss function that the softmax operation should be included in the loss calculation. This allows for an optimized implementation.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e661b5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20bd69",
   "metadata": {},
   "source": [
    "**`SparseCategorialCrossentropy` or `CategoricalCrossEntropy`**\n",
    "\n",
    "<span style=\"color:blue\"><b>Tensorflow has two potential formats for target values and the selection of the loss defines which is expected.</b></span>\n",
    "- **`SparseCategorialCrossentropy`**: expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9.  An example with 10 potential target values, where the target is 2 would be `y=2`\n",
    "- **`CategoricalCrossEntropy`**: Expects the target value of an example to be **one-hot encoded** where the value at the target index is 1 while the other N-1 entries are zero. An example with 10 potential target values, where the target is 2 would be `y=[0,0,1,0,0,0,0,0,0,0]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc3fde",
   "metadata": {},
   "source": [
    "### Lab: Multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc0523",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "111c92ad",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "<div align=\"center\">\n",
    "    <img src=\"attachments/lab_multiclass_nn_architecture.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "</div>\n",
    "\n",
    "This lab used a 2-layer network as shown.\n",
    "<b>Unlike the binary classification networks, this network has four outputs, one for each class</b>. Given an input example, the output with the highest value is the predicted class of the input.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8f95b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/lab_softmax_output_exp_p1.png\" alt=\"Drawing\" style=\"width: 95%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b65713",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T04:35:45.582895Z",
     "start_time": "2023-04-29T04:35:45.561953Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/lab_softmax_output_exp_p2_layer_1.png\" alt=\"Drawing\" style=\"width: 95%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b8dd6d",
   "metadata": {},
   "source": [
    "From layer 1 activations plot above, we can observe that\n",
    "- For $a_0^{[1]}$, classes 0 and 1 have 0, classes 2 and 3 have >0\n",
    "- For $a_1^{[1]}$, classes 0 and 2 have 0, classes 1 and 3 have >0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf03b47",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/lab_softmax_output_exp_p2_layer_2_output.png\" alt=\"Drawing\" style=\"width: 95%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27726c8",
   "metadata": {},
   "source": [
    "*Note: The intensity of background color that corresponds to layers output/activation*\n",
    "\n",
    "From layer 1 activation plots above, we can observe that\n",
    "- For $a_0^{[1]}$, classes 0 and 1 have 0, classes 2 and 3 have >0\n",
    "- For $a_1^{[1]}$, classes 0 and 2 have 0, classes 1 and 3 have >0\n",
    "\n",
    "From layer 2 activation plots (with layer 1 outputs as axis), we can observe that\n",
    "- For $a_0^{[2]}$, class 0 will have highest value as $a_0^{[1]}=0$ and $a_1^{[1]}=0$\n",
    "- For $a_1^{[2]}$, class 1 will have highest value as $a_0^{[1]}=0$ and $a_1^{[1]}>0$\n",
    "- For $a_2^{[2]}$, class 2 will have highest value as $a_0^{[1]}>0$ and $a_1^{[1]}=0$\n",
    "- For $a_3^{[2]}$, class 3 will have highest value as $a_0^{[1]}>0$ and $a_1^{[1]}>0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac1e468",
   "metadata": {},
   "source": [
    "## Practice quiz: Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be42fc7c",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Practice-quiz-Multiclass-Classification) screenshots for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc7152",
   "metadata": {},
   "source": [
    "## Additional Neural Network Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298da3bf",
   "metadata": {},
   "source": [
    "### Advanced Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337e16aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T06:12:25.608577Z",
     "start_time": "2023-04-29T06:12:25.583151Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/advanced_optimization_p1.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91830450",
   "metadata": {},
   "source": [
    "**How can we make gradient descent work even better?**\n",
    "- *Image on the left*: Since every single small step of gradient descent is pretty much going in the same direction over and over, why don't we make $\\alpha$ bigger steps to reach the global minimum faster by making the learning rate bigger?\n",
    "- *Image on the right*: Since in every step of gradient descent, it is oscillating back and forth, why don't we make the learning rate smaller?\n",
    "\n",
    "**Adam algorithm: Adaptive Moment estimation**\n",
    "- Can handle both the above mentioned scenario by <span style=\"color:blue\"><b>adjusting the learning rate automatically</b></span>.\n",
    "- Depending on how the gradient descent is proceeding, sometimes, we wish for a bigger learning rate $\\alpha$, and othertimes, for a smaller learning rate $\\alpha$\n",
    "- Interestingly, it uses different learning rates for every single parameter of our model\n",
    "    - If we have $w_1, \\cdots, w_{10}$ and $b$, we will have $11$ different learning rates, namely $\\alpha_1, \\cdots, \\alpha_{10}, \\alpha_{11}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8285b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T06:12:25.608577Z",
     "start_time": "2023-04-29T06:12:25.583151Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/advanced_optimization_p2.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92819d0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T06:12:25.608577Z",
     "start_time": "2023-04-29T06:12:25.583151Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/advanced_optimization_p3.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e685b01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T06:12:25.608577Z",
     "start_time": "2023-04-29T06:12:25.583151Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/advanced_optimization_p4.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3e2b5",
   "metadata": {},
   "source": [
    "> Adam optimizer typically works much faster than gradient descent. If in doubt, use Adam optimizer for NN training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc55c1",
   "metadata": {},
   "source": [
    "### Additional Layer Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a648007f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T06:15:32.755628Z",
     "start_time": "2023-04-29T06:15:32.739730Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/additional_layer_types_p1.png\" alt=\"Drawing\" style=\"width: 35%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5bd9db",
   "metadata": {},
   "source": [
    "**Dense layer**\n",
    "- Every neuron in a layer get its input from all the activations from the previous layer\n",
    "\n",
    "**Convolutional layer**\n",
    "- Each neuron in a layer only looks at part/region of the previous layer's input\n",
    "- Why?Benefits?\n",
    "    - Faster computation\n",
    "    - Need less training data (and less prone to overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1e705",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T06:12:25.608577Z",
     "start_time": "2023-04-29T06:12:25.583151Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/additional_layer_types_p2.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b94124",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T06:12:25.608577Z",
     "start_time": "2023-04-29T06:12:25.583151Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/additional_layer_types_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ace036e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T06:12:25.608577Z",
     "start_time": "2023-04-29T06:12:25.583151Z"
    }
   },
   "source": [
    "**Convolution Neural Network (CNN)**\n",
    "- e.g., Classification of 1D EKG signals to diagnose whether patients have heart disease\n",
    "    - Input layer: 100 features\n",
    "    - Convolutional layer 1: 9 units, window size = 20, stride=10\n",
    "        - Unit 1: Looks only at $x_{1} - x_{20}$\n",
    "        - Unit 2: Looks only at $x_{11} - x_{30}$\n",
    "        - $\\vdots$\n",
    "        - Unit 9: Looks only at $x_{81} - x_{100}$\n",
    "    - Convolution layer 2: 3 units, window size = 5, stride=2\n",
    "        - Unit 1:  Looks only at $a_{1} - a_{5}$\n",
    "        - Unit 2:  Looks only at $a_{3} - a_{7}$\n",
    "        - Unit 2:  Looks only at $a_{5} - a_{10}$\n",
    "    - Output Layer 3: \n",
    "        - Unit 1: Sigmoid function to make binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e5daf8",
   "metadata": {},
   "source": [
    "## Practice quiz: Additional neural network concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb25396",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Practice-quiz-Multiclass-Classification) screenshots for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb1e52",
   "metadata": {},
   "source": [
    "## Back propagation (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da2544",
   "metadata": {},
   "source": [
    "### What is a derivative (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4cb3be",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/what_is_a_derivative_p1.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb54091a",
   "metadata": {},
   "source": [
    "In the above example, \n",
    "- if $w$ goes up by $\\epsilon$, then $J(w)$ goes up by $6 \\times \\epsilon$.\n",
    "- In calculus, we denote it as $\\frac{\\partial}{\\partial{w}}J(w)=6$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ce8e6",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/what_is_a_derivative_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3e24e",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/what_is_a_derivative_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30604f05",
   "metadata": {},
   "source": [
    "In calculus, the slope of the lines correspond to the derivative of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b5f4dd",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/what_is_a_derivative_p4.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de686f",
   "metadata": {},
   "source": [
    "**Another way of computing derivative for specific value of $w$**\n",
    "- If $w$ goes up $\\epsilon$, how much does $J(w)$ goes up by interms of $k \\times \\epsilon$, where $k$ will be the derivative value for that particualr $w$\n",
    "- The value of $k$ will depend both on\n",
    "    - what's the function $J(w)$ and\n",
    "    - what's the value $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22f8f541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T07:11:22.635982Z",
     "start_time": "2023-04-29T07:11:22.000763Z"
    }
   },
   "outputs": [],
   "source": [
    "import sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec654590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T07:14:17.555463Z",
     "start_time": "2023-04-29T07:14:17.544367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{w}$"
      ],
      "text/plain": [
       "1/w"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J, w = sympy.symbols('J,w')\n",
    "J = [1/w, w, w**2, w**3][0]\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb79caa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T07:14:17.741143Z",
     "start_time": "2023-04-29T07:14:17.719236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{1}{w^{2}}$"
      ],
      "text/plain": [
       "-1/w**2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ_dw = sympy.diff(J,w)\n",
    "dJ_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e25a4968",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T07:14:17.943637Z",
     "start_time": "2023-04-29T07:14:17.927274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{1}{4}$"
      ],
      "text/plain": [
       "-1/4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJ_dw.subs([(w,2)]) # When w = 2, dJ/dw = 4 as its 2w for J=w^2, and dJ/dw = 12 as its 3*w^2 for J=w^3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab26d867",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/what_is_a_derivative_p5.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebfeccd",
   "metadata": {},
   "source": [
    "### Computation graph (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0934eb4",
   "metadata": {},
   "source": [
    "**Computation graph**\n",
    "- Key idea in DL where we implement FP and BP by step-by-step computations\n",
    "- Is how programming langugages like TenforFlow compute derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9676ac3",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/computation_graph_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a3ba0d",
   "metadata": {},
   "source": [
    "The above example illustrates how computation graph is used to compute $J$ for a simple/small NN with 1 layer and 1 neuron, and linear activation function. The set of nodes (boxes e.g., `c`, `a`, `d`, `J`) are connected by edges (arrows) in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab96129f",
   "metadata": {},
   "source": [
    "**Example**\n",
    "- $x=-2$\n",
    "- $y=2$\n",
    "- $w=2$\n",
    "- $b=8$\n",
    "\n",
    "**Computation Graph: Forward Prop (left to right) to compute J**\n",
    "- $c = wx$\n",
    "- $a = c + b$\n",
    "- $d = a - y$\n",
    "- $J = \\frac{1}{2}d^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1001b1e",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/computation_graph_p2.png\" alt=\"Drawing\" style=\"width: 75%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc4ba9",
   "metadata": {},
   "source": [
    "**Computation Graph: Backward Prop (left to right) to compute derivates of J, $\\large \\frac{\\partial J}{\\partial w}$ and $\\large  \\frac{\\partial J}{\\partial b}$**\n",
    "\n",
    "Using chain rule (calculus), we can compute how much change in ___ something affects change in J \n",
    "\n",
    "-  $\\large \\frac{\\partial J}{\\partial d} \\small = 2$ \n",
    "    - $\\large \\frac{\\partial J}{\\partial a} = \\frac{\\partial J}{\\partial d} \\times \\frac{\\partial d}{\\partial a} \\small = 2 \\times 1 = 2$ \n",
    "        -  $\\large \\frac{\\color{Blue} \\partial \\color{Blue} J}{\\color{Blue} \\partial \\color{Blue} b} = \\frac{\\partial J}{\\partial a} \\times \\frac{\\partial a}{\\partial b} \\small = 2 \\times 1 = 2$ \n",
    "        -  $\\large \\frac{\\partial J}{\\partial c} = \\frac{\\partial J}{\\partial a} \\times \\frac{\\partial a}{\\partial c} \\small = 2 \\times 1 = 2$ \n",
    "            -  $\\large \\frac{\\color{Blue} \\partial \\color{Blue} J}{\\color{Blue} \\partial \\color{Blue} w} = \\frac{\\partial J}{\\partial c} \\times \\frac{\\partial c}{\\partial w} \\small = 2 \\times x = 2 \\times -2 = -4$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ec09f",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/computation_graph_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430056bb",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/computation_graph_p4.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14084379",
   "metadata": {},
   "source": [
    "**Why do we use Backprop using computation graph to compute derivatives?**\n",
    "- Efficient algo\n",
    "- To compute $ \\frac{\\partial J}{\\partial w}$ and $\\frac{\\partial J}{\\partial b}$, we just need to compute $ \\frac{\\partial J}{\\partial a}$ once\n",
    "    - If there are $N$ nodes (boxes) and $P$ parameters, this allows us to compute everything in $N+P$ steps rather than $N \\times P$ steps. For example\n",
    "        - $N = 10,000$ nodes\n",
    "        - $P = 100,000$ parameters\n",
    "        - <span style=\"color:blue\"><b>$N+P = 1.1 \\times 10^{5} \\Rightarrow$ Using back propagation is very efficient to compute derivatives </b></span>\n",
    "        - $N\\times P = 10^{9} \\Rightarrow$ If we compute one parameter at a time, then very inefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf307520",
   "metadata": {},
   "source": [
    "### Larger neural network example (Optional) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b07f8",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/larger_neural_network_p1.png\" alt=\"Drawing\" style=\"width: 70%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3d31a",
   "metadata": {},
   "source": [
    "The above example illustrates how computation graph is used to compute $J$ for a NN with \n",
    "- 1 hidden layer and 1 hidden unit $\\vec{a}^{[1]} = a_1^{[1]}$ with ReLU activation, and \n",
    "- output layer with 1 output unit $\\vec{a}^{[2]} = a_1^{[2]}$ with ReLU activation. \n",
    "\n",
    "The set of nodes (boxes e.g., `c`, `a`, `d`, `J`) are connected by edges (arrows) in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739f7870",
   "metadata": {},
   "source": [
    "**Example with single training example**\n",
    "- $x=1$\n",
    "- $y=5$\n",
    "- $w^{[1]}=2, b^{[1]}=0$\n",
    "- $w^{[2]}=3, b^{[2]}=1$\n",
    "- $g(z) = \\text{max}(0,z)$\n",
    "\n",
    "**Forward propagation calculation**\n",
    "- $a^{[1]} = g(w^{[1]}x + b^{[1]}) = \\text{max}(0, w^{[1]}x + b^{[1]}) = \\text{max}(2\\times 1 + 0) = 2$\n",
    "- $a^{[2]} = g(w^{[2]}a^{[1]} + b^{[2]}) = \\text{max}(0, w^{[2]}a^{[1]} + b^{[2]}) = \\text{max}(3\\times 2 + 1) = 7$\n",
    "- $J(w,b) = \\frac{1}{2}(a^{[2]}-y)^2 = \\frac{1}{2}(7-5)^{2} = 2$\n",
    " \n",
    "**Computation Graph: Forward Prop (left to right) to compute J**\n",
    "- $t^{[1]} = w^{[1]}\\times x$\n",
    "- $z^{[1]} = t^{[1]} + b^{[1]}$\n",
    "- $a^{[1]} = g(z^{[1]})$\n",
    "- $t^{[2]} = w^{[2]} \\times a^{[1]}$\n",
    "- $z^{[2]} = t^{[2]} + b^{[2]}$\n",
    "- $a^{[2]} = g(z^{[2]})$\n",
    "- $J = \\frac{1}{2}(a^{[2]}-y)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd75ec79",
   "metadata": {},
   "source": [
    "**Computation Graph: Backward Prop (left to right) to compute derivates of J, $\\large \\frac{\\partial J}{\\partial w}$ and $\\large  \\frac{\\partial J}{\\partial b}$**\n",
    "\n",
    "Using chain rule (calculus), we can compute how much change in ___ something affects change in J \n",
    "\n",
    "-  $\\large \\frac{\\partial J}{\\partial a^{[2]}} \\small = a^{[2]}-y = 7-5 = 2 $ (Note: There's inner chain rule)\n",
    "    - $\\large \\frac{\\partial J}{\\partial z^{[2]}} = \\frac{\\partial J}{\\partial a^{[2]}} \\times \\frac{\\partial a^{[2]}}{\\partial z^{[2]}} \\small = 2 \\times 1 = 2$ (Note: As $z^{[2]}$ goes up by $\\epsilon$, $a^{[2]}$ will go up by $\\epsilon$ for current setting with ReLU activation)\n",
    "        -  $\\large \\frac{\\color{Blue} \\partial \\color{Blue} J}{\\color{Blue} \\partial \\color{Blue}{b^{[2]}}} = \\frac{\\partial J}{\\partial z^{[2]}} \\times \\frac{\\partial z^{[2]}}{\\partial b^{[2]}} \\small = 2 \\times 1 = 2$ \n",
    "        -  $\\large \\frac{\\partial J}{\\partial t^{[2]}} = \\frac{\\partial J}{\\partial z^{[2]}} \\times \\frac{\\partial z^{[2]}}{\\partial t^{[2]}} \\small = 2 \\times 1 = 2$ \n",
    "            -  $\\large \\frac{\\color{Blue} \\partial \\color{Blue} J}{\\color{Blue} \\partial \\color{Blue}{w^{[2]}}} = \\frac{\\partial J}{\\partial t^{[2]}} \\times \\frac{\\partial t^{[2]}}{\\partial w^{[2]}} \\small = 2 \\times a^{[1]} = 2 \\times 2 = 4$ \n",
    "            -  $\\large \\frac{\\partial J}{\\partial a^{[1]}} = \\frac{\\partial J}{\\partial t^{[2]}} \\times \\frac{\\partial t^{[2]}}{\\partial a^{[1]}} \\small = 2 \\times w^{[2]} = 2 \\times 3 = 6$ \n",
    "                -  $\\large \\frac{\\partial J}{\\partial z^{[1]}} = \\frac{\\partial J}{\\partial a^{[1]}} \\times \\frac{\\partial a^{[1]}}{\\partial z^{[1]}} \\small = 6 \\times 1 = 6$ \n",
    "                    -  $\\large \\frac{\\color{Blue}\\partial \\color{Blue}J}{\\color{Blue}\\partial \\color{Blue}{b^{[1]}}} = \\frac{\\partial J}{\\partial z^{[1]}} \\times \\frac{\\partial z^{[1]}}{\\partial b^{[1]}} \\small = 6 \\times 1 = 6$ \n",
    "                    -  $\\large \\frac{\\partial J}{\\partial t^{[1]}} = \\frac{\\partial J}{\\partial z^{[1]}} \\times \\frac{\\partial z^{[1]}}{\\partial t^{[1]}} \\small = 6 \\times 1 = 6$     \n",
    "                        -  $\\large \\frac{\\color{Blue}\\partial \\color{Blue}J}{\\color{Blue}\\partial \\color{Blue}{w^{[1]}}} = \\frac{\\partial J}{\\partial t^{[1]}} \\times \\frac{\\partial t^{[1]}}{\\partial w^{[1]}} \\small = 6 \\times x = 6 \\times 1 = 6$ (Note: if $w^{[1]}$ goes by up $\\epsilon$, $J$ would go up by $6 \\times \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a98f28",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/larger_neural_network_p2.png\" alt=\"Drawing\" style=\"width: 60%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a05f00",
   "metadata": {},
   "source": [
    "### Optional Lab: Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9554e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88636a6a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b034958a",
   "metadata": {},
   "source": [
    "### Optional Lab: Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841d9aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3af7b5a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edaad3af",
   "metadata": {},
   "source": [
    "## Practice lab: Neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3296760e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbcc2004",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "171f10db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1d01199",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e51df4dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T06:42:25.548747Z",
     "start_time": "2023-04-29T06:42:25.535867Z"
    }
   },
   "outputs": [],
   "source": [
    "# Kernel crashes half-way through. Found issue by running the codes in python command line \n",
    "# https://stackoverflow.com/questions/20554074/sklearn-omp-error-15-initializing-libiomp5md-dll-but-found-mk2iomp5md-dll-a\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f33a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ga]",
   "language": "python",
   "name": "conda-env-ga-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
