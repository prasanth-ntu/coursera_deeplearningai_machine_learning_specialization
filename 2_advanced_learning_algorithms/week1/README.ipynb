{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451da473",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Course-2-Week-1:-Neural-Networks\" data-toc-modified-id=\"Course-2-Week-1:-Neural-Networks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Course 2 Week 1: Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Neural-networks-intuition\" data-toc-modified-id=\"Neural-networks-intuition-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Neural networks intuition</a></span><ul class=\"toc-item\"><li><span><a href=\"#Welcome\" data-toc-modified-id=\"Welcome-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Welcome</a></span></li><li><span><a href=\"#Neurons-and-the-brain\" data-toc-modified-id=\"Neurons-and-the-brain-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Neurons and the brain</a></span></li><li><span><a href=\"#Demand-Prediction\" data-toc-modified-id=\"Demand-Prediction-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Demand Prediction</a></span></li><li><span><a href=\"#Example:-Recognizing-images\" data-toc-modified-id=\"Example:-Recognizing-images-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Example: Recognizing images</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Neural-networks-intuition\" data-toc-modified-id=\"Practice-quiz:-Neural-networks-intuition-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Practice quiz: Neural networks intuition</a></span></li><li><span><a href=\"#Neural-network-model\" data-toc-modified-id=\"Neural-network-model-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Neural network model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Neural-network-layer\" data-toc-modified-id=\"Neural-network-layer-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Neural network layer</a></span></li><li><span><a href=\"#More-complex-neural-networks\" data-toc-modified-id=\"More-complex-neural-networks-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>More complex neural networks</a></span></li><li><span><a href=\"#Inference:-making-predictions-(forward-propagation)\" data-toc-modified-id=\"Inference:-making-predictions-(forward-propagation)-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Inference: making predictions (forward propagation)</a></span></li><li><span><a href=\"#Neurons-and-Layers\" data-toc-modified-id=\"Neurons-and-Layers-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Neurons and Layers</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Neural-network-model\" data-toc-modified-id=\"Practice-quiz:-Neural-network-model-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Practice quiz: Neural network model</a></span></li><li><span><a href=\"#Tensorflow-implementation\" data-toc-modified-id=\"Tensorflow-implementation-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Tensorflow implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inference-in-Code\" data-toc-modified-id=\"Inference-in-Code-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Inference in Code</a></span></li><li><span><a href=\"#Data-in-TensorFlow\" data-toc-modified-id=\"Data-in-TensorFlow-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Data in TensorFlow</a></span></li><li><span><a href=\"#Building-a-neural-network\" data-toc-modified-id=\"Building-a-neural-network-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Building a neural network</a></span></li><li><span><a href=\"#Cofee-Roasting-in-Tensorflow\" data-toc-modified-id=\"Cofee-Roasting-in-Tensorflow-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>Cofee Roasting in Tensorflow</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Tensorflow-implementation\" data-toc-modified-id=\"Practice-quiz:-Tensorflow-implementation-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Practice quiz: Tensorflow implementation</a></span></li><li><span><a href=\"#Neural-network-implementation-in-Python\" data-toc-modified-id=\"Neural-network-implementation-in-Python-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Neural network implementation in Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#Forward-prop-in-a-single-layer\" data-toc-modified-id=\"Forward-prop-in-a-single-layer-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Forward prop in a single layer</a></span></li><li><span><a href=\"#General-implementation-of-forward-propagation\" data-toc-modified-id=\"General-implementation-of-forward-propagation-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>General implementation of forward propagation</a></span></li><li><span><a href=\"#CofeeRoastingNumPy\" data-toc-modified-id=\"CofeeRoastingNumPy-1.7.3\"><span class=\"toc-item-num\">1.7.3&nbsp;&nbsp;</span>CofeeRoastingNumPy</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Neural-network-implementation-in-Python\" data-toc-modified-id=\"Practice-quiz:-Neural-network-implementation-in-Python-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Practice quiz: Neural network implementation in Python</a></span></li><li><span><a href=\"#Speculation-on-artificial-general-intelligence-(AGI)\" data-toc-modified-id=\"Speculation-on-artificial-general-intelligence-(AGI)-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Speculation on artificial general intelligence (AGI)</a></span></li><li><span><a href=\"#Vectorization-(optional)\" data-toc-modified-id=\"Vectorization-(optional)-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Vectorization (optional)</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-neural-networks-are-implemented-efficiently\" data-toc-modified-id=\"How-neural-networks-are-implemented-efficiently-1.10.1\"><span class=\"toc-item-num\">1.10.1&nbsp;&nbsp;</span>How neural networks are implemented efficiently</a></span></li><li><span><a href=\"#Matrix-multiplication\" data-toc-modified-id=\"Matrix-multiplication-1.10.2\"><span class=\"toc-item-num\">1.10.2&nbsp;&nbsp;</span>Matrix multiplication</a></span></li><li><span><a href=\"#Matrix-multiplication-rules\" data-toc-modified-id=\"Matrix-multiplication-rules-1.10.3\"><span class=\"toc-item-num\">1.10.3&nbsp;&nbsp;</span>Matrix multiplication rules</a></span></li></ul></li><li><span><a href=\"#Practice-Lab:-Neural-networks\" data-toc-modified-id=\"Practice-Lab:-Neural-networks-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Practice Lab: Neural networks</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f932ab82",
   "metadata": {},
   "source": [
    "# Course 2 Week 1: Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a635a3",
   "metadata": {},
   "source": [
    "## Neural networks intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb128686",
   "metadata": {},
   "source": [
    "### Welcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67767ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T23:30:03.781542Z",
     "start_time": "2023-04-14T23:30:03.752310Z"
    }
   },
   "source": [
    "Plan for this 4 weeks course\n",
    "- Wk 1: NN Inference (Prediction)\n",
    "- Wk 2: NN Training\n",
    "- Wk 3: Practical advice for building machine laerning systems\n",
    "- Wk 4: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025d83e",
   "metadata": {},
   "source": [
    "### Neurons and the brain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b4f653",
   "metadata": {},
   "source": [
    "> NN Origins: Algorithms/Software that try to mimic the human biological brain\n",
    "\n",
    "- Work started way back in 1950s, and fell out of favor for a while.\n",
    "- NNs were used again in 1980's and early 1990's and gained traction in applications like handwritten digit recognition.\n",
    "- However, fell out of favor in the late 1990's.\n",
    "- NNs Resurgence: From around 2005, and became rebranded little bit with **deep learning**\n",
    "- Since them, NNs have revolutionized one application area after another\n",
    "    - speech recognition $\\rightarrow$ compute vision $\\rightarrow$ NLP $\\rightarrow$ and so on...\n",
    "    \n",
    "> <span style=\"color:red\">Though today's NN have almost nothing to do with how the brain learns, there was the early motivation to trying to build algorithms to mimic the brain</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805ea6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T00:03:46.944621Z",
     "start_time": "2023-04-15T00:03:46.939215Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neurons_in_the_brain.png\" alt=\"Drawing\" style=\"width: 35%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58aec8f",
   "metadata": {},
   "source": [
    "All of human thought is from neurons in our brain, sending electrical pulses and forming new connections of other neurons. \n",
    "\n",
    "As shown in figure above,\n",
    "- <span style=\"color:blue\"><b>a neuron </b></span>(top right)\n",
    "    - **Dendrites** (input wires): has **number of inputs where it receives electrical impulses from many other neurons**\n",
    "    - aggregates inputs **carries out some computations**\n",
    "    - **Axons** (output wires): and **send this outputs to other neurons** by electrical impulses\n",
    "        - the ouput of this neuron becomes the input of the neuron (on bottom left), which again aggregates inputs from multiple othr neurons to then may be send its own output to yet other neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21aaaf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T00:13:35.824550Z",
     "start_time": "2023-04-15T00:13:35.813819Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neurons_in_artificial_neural_networks_and_the_brain.png\" alt=\"Drawing\" style=\"width: 75%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c13f4",
   "metadata": {},
   "source": [
    "The artifical neural network (ANN) uses a very simplified math model of what biological neuron does.\n",
    "\n",
    "Let's look at a **simplified math model of few neurons**.\n",
    "What does these neurons do?\n",
    "- Takes one or more inputs (which are just some numbers)\n",
    "- Does some computation\n",
    "- Outputs a number\n",
    "    - Which then could be an input to another neuron\n",
    "    \n",
    "Rather than building one neuron at a time, we often want to simulate many such neurons at the same time $\\Rightarrow$ We have more neurons, which collectively input a few numbers, carry out some computation, and output some other numbers.\n",
    "\n",
    "> <span style=\"color:red\">Caveat: Till date, we have almost no idea how the human brain works.</span>\n",
    "\n",
    ">  <span style=\"color:blue\">Though the NN origins were biologically motivates, the researchers are now just using engineering principles to figure out how to build algorithms that are more effective.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54fdfe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T00:27:32.293757Z",
     "start_time": "2023-04-15T00:27:32.281813Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_networks_why_now.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef538e19",
   "metadata": {},
   "source": [
    "**Why now than ever before?**\n",
    "1. With rise of interent, mobile phones and digitalization of the society, the amout of data we have for lot of applications has exploded.\n",
    "2. Performance of AI and NN with increased amount of data\n",
    "    - With traditional ML algorithms (like linear and logistic regression), they were not able to scale with anount of data \n",
    "$\\Rightarrow$ They were not able to take effective advantage of all of this data we had for different applications\n",
    "    - With small NN, the performance become slightly better than traditional ML algos\n",
    "    - With medium NN (i.e., more neurons), performance improved further\n",
    "    - With large NN, for some applications, the performance will keep on going up\n",
    "        - <span style=\"color:green\"><b>With large NN trained on big data, we could attain superior performance</b></span><span style=\"color:red\"> which were not possible with earlier generations of learning algorithms</span>. This caused deep learning algos to take off along with rise of faster computer processors (like GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9847fb87",
   "metadata": {},
   "source": [
    "### Demand Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671220ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T01:03:21.788720Z",
     "start_time": "2023-04-15T01:03:21.777372Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/demand_prediction_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b1db4d",
   "metadata": {},
   "source": [
    "**To illustrate how NNs work, let's use a simple example: Demand prediction (predict whether the product will be a top seller or not)**.\n",
    "\n",
    "- Input feature, $x$: Price of the T-shirt\n",
    "- Output, $y$: Top seller or not\n",
    "    - Using logistic regression, $\\large\\hat{y} = f(x) = \\frac{1}{1+e^{-(wx+b)}}$ \n",
    "        - We are going to call the output of logistic regression as $a$ instead of $\\hat{y}$ or $f(x)$ moving forward to set us up to build a NN.\n",
    "        - **$a$ stands for activation, a term from neuroscience that refers to how much a neuron is sending a high output (electrical impulses) to other neurons downstream from it**.\n",
    "        \n",
    "    - <span><b>The above logsitic regression can be thought of as a <u>single nueron model</u></b></span>, where\n",
    "        - takes the input $x$ (e.g., price), \n",
    "        - and the neuron computes the formula (e.g., sigmoid function) and \n",
    "        - outputs the value $a$ (e.g., probability of T-shirt being a top seller)\n",
    "        \n",
    "<span style=\"color:blue\"><b>Now that we described a single neuron, building a NN now just requires taking a bunch of these nurons and wiring them together</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71fb79",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/demand_prediction_p2_nn_intuition.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43638316",
   "metadata": {},
   "source": [
    "**Let's now use a complex example: Demand prediction (predict whether the product will be a top seller or not)**.\n",
    "\n",
    "- Input features $x$ (**input layer** in the context of NN)\n",
    "    - price\n",
    "    - shipping cost\n",
    "    - marketing\n",
    "    - material\n",
    "- Manual feature engineering (intermediate step): \n",
    "    - We might suspect the probability of T-shirt being a top seller depends on 3 factors\n",
    "        - Affordability \n",
    "        - Awareness \n",
    "        - Perceived quality\n",
    "    - Let's say we create one artificial neuron to estimate the probability of each of the above 3 factors $\\Rightarrow$ We will have 3 neurons, which we will group together and call as **layer** (to be specific, **hidden layers** in context of NN). In the context of NN, each of these 3 estimations are called as **activations** of the hidden layer\n",
    "        - Affordability = f(price, shipping_cost)\n",
    "        - Awareness = f(marketing)\n",
    "        - Perceived quality = f(price, material)\n",
    "- Output: Probability of being a Top seller\n",
    "    - Now that we have an probability estimate of 3 intermediate factors (affordability, awareness and perceived quality), we can wire the outputs of these 3 neurons to another neuron on the right (i.e., another logistic regression unit).\n",
    "    - This single nueron unit (a.k.a **output layer** in the context of NN) will then finally estimate the probability of t-shirt being a top-seller, which is the **final activation (or final output)** of the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8854d014",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">In the above example, we had to go through the neurons one at a time and decice what inputs it would take from previous layer (e.g., affordability as f(price, shipping_costs), awareness as f(marketing), etc.. However, if we are building a large NN, it would be impractical to go through and manually decide which neurons should take which features as inputs.</span><span style=\"color:green\">So, <b>the way a NN is implemented in practice is each neuron will have access to every features/value from previous layer.</b></span> \n",
    "\n",
    "e.g., We can imagine that if we're trying to predict affordability and we know what's the price, shipping cost, marketing and material, like us, may be the NN will learn to ignore marketing and material and just figure out through setting the parameters appropriately to only focus on the subset of features (price and shipping cost) that are most relevant to affordability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3946c4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T01:31:24.680838Z",
     "start_time": "2023-04-15T01:31:24.660307Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/demand_prediction_p3_with_vector_notations.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af1278",
   "metadata": {},
   "source": [
    "**Let's look at another intuition/way for NNs**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2eb343",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/demand_prediction_p4_further_nn_intuition.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb32a4",
   "metadata": {},
   "source": [
    "If we hide the input layer, what we essentially have is a logistic regression algo with\n",
    "- inputs: affordability, awareness, perceived quality\n",
    "- output: probability of t-shirt being a top seller\n",
    "\n",
    "However, the cool thing is rather than using original 4 input features, the network is using a new and maybe better set of 3 features, that are hopefully more predictive whether or not the t-shirt will be a top seller.\n",
    "\n",
    "<span style=\"color:green\"><b>One way to think of this NN is just a verion of logistic regresion that can learn its own features in the hidden layers that makes it easier to make accurate predictions.</b></span>\n",
    "\n",
    "Recall, that in C1, we manually feature engineered complex features from length and width of the lot. Instead of hand engineering features, with NN, it learns its own features thereby making learning problem more easier for us, thus making the NN very powerful learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d5888",
   "metadata": {},
   "source": [
    "**Layer**\n",
    "- Grouping of neurons that same or similar features as input, and in turn outputs a few number together\n",
    "- A layer can have single nueron or multiple neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433b61e3",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/multiple_hidden_layers_example.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ce106",
   "metadata": {},
   "source": [
    "Above, we have examples of NN with multiple hidden layers. As a DS/MLE, we need to choose the right NN architecture (no. of hidden layers, and no. of units in each hidden layer). The above mentioned NNs can sometimes be called as Multi Layer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1249af",
   "metadata": {},
   "source": [
    "### Example: Recognizing images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963243d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90e79e73",
   "metadata": {},
   "source": [
    "## Practice quiz: Neural networks intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b31a8",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3517fe",
   "metadata": {},
   "source": [
    "### Neural network layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d9468",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fc287dc",
   "metadata": {},
   "source": [
    "### More complex neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761ada7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef4bfb82",
   "metadata": {},
   "source": [
    "### Inference: making predictions (forward propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec92d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a8886f6",
   "metadata": {},
   "source": [
    "### Neurons and Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa53f24f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea92cab3",
   "metadata": {},
   "source": [
    "## Practice quiz: Neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff542d",
   "metadata": {},
   "source": [
    "## Tensorflow implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb6fe5",
   "metadata": {},
   "source": [
    "### Inference in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544cd3ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "918872bf",
   "metadata": {},
   "source": [
    "### Data in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e63a4ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "642b9db4",
   "metadata": {},
   "source": [
    "### Building a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638446c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c700943",
   "metadata": {},
   "source": [
    "### Cofee Roasting in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ad518",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1c32135",
   "metadata": {},
   "source": [
    "## Practice quiz: Tensorflow implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a10c1",
   "metadata": {},
   "source": [
    "## Neural network implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b429944",
   "metadata": {},
   "source": [
    "### Forward prop in a single layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac831a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a92babc",
   "metadata": {},
   "source": [
    "### General implementation of forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1ed61",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70ed85a3",
   "metadata": {},
   "source": [
    "### CofeeRoastingNumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68154be9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "990bcb71",
   "metadata": {},
   "source": [
    "## Practice quiz: Neural network implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2837b",
   "metadata": {},
   "source": [
    "## Speculation on artificial general intelligence (AGI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d76736",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4112181c",
   "metadata": {},
   "source": [
    "## Vectorization (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0f153",
   "metadata": {},
   "source": [
    "### How neural networks are implemented efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88194311",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51e6553a",
   "metadata": {},
   "source": [
    "### Matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c6702c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b09d0ac0",
   "metadata": {},
   "source": [
    "### Matrix multiplication rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39f513",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85692177",
   "metadata": {},
   "source": [
    "## Practice Lab: Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5a62d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee9d5b10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52ac9793",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd761728",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3969221f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cfd958c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d56d5464",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ga]",
   "language": "python",
   "name": "conda-env-ga-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
