{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699be61c",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Course-2-Week-1:-Neural-Networks\" data-toc-modified-id=\"Course-2-Week-1:-Neural-Networks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Course 2 Week 1: Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Neural-networks-intuition\" data-toc-modified-id=\"Neural-networks-intuition-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Neural networks intuition</a></span><ul class=\"toc-item\"><li><span><a href=\"#Welcome\" data-toc-modified-id=\"Welcome-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Welcome</a></span></li><li><span><a href=\"#Neurons-and-the-brain\" data-toc-modified-id=\"Neurons-and-the-brain-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Neurons and the brain</a></span></li><li><span><a href=\"#Demand-Prediction\" data-toc-modified-id=\"Demand-Prediction-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Demand Prediction</a></span></li><li><span><a href=\"#Example:-Recognizing-images\" data-toc-modified-id=\"Example:-Recognizing-images-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Example: Recognizing images</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Neural-networks-intuition\" data-toc-modified-id=\"Practice-quiz:-Neural-networks-intuition-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Practice quiz: Neural networks intuition</a></span></li><li><span><a href=\"#Neural-network-model\" data-toc-modified-id=\"Neural-network-model-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Neural network model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Neural-network-layer\" data-toc-modified-id=\"Neural-network-layer-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Neural network layer</a></span></li><li><span><a href=\"#More-complex-neural-networks\" data-toc-modified-id=\"More-complex-neural-networks-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>More complex neural networks</a></span></li><li><span><a href=\"#Inference:-making-predictions-(forward-propagation)\" data-toc-modified-id=\"Inference:-making-predictions-(forward-propagation)-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Inference: making predictions (forward propagation)</a></span></li><li><span><a href=\"#Lab:-Neurons-and-Layers\" data-toc-modified-id=\"Lab:-Neurons-and-Layers-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Lab: Neurons and Layers</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Neural-network-model\" data-toc-modified-id=\"Practice-quiz:-Neural-network-model-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Practice quiz: Neural network model</a></span></li><li><span><a href=\"#Tensorflow-implementation\" data-toc-modified-id=\"Tensorflow-implementation-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Tensorflow implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inference-in-Code\" data-toc-modified-id=\"Inference-in-Code-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Inference in Code</a></span></li><li><span><a href=\"#Data-in-TensorFlow\" data-toc-modified-id=\"Data-in-TensorFlow-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Data in TensorFlow</a></span></li><li><span><a href=\"#Building-a-neural-network\" data-toc-modified-id=\"Building-a-neural-network-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Building a neural network</a></span></li><li><span><a href=\"#Lab:-Cofee-Roasting-in-Tensorflow\" data-toc-modified-id=\"Lab:-Cofee-Roasting-in-Tensorflow-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>Lab: Cofee Roasting in Tensorflow</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Tensorflow-implementation\" data-toc-modified-id=\"Practice-quiz:-Tensorflow-implementation-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Practice quiz: Tensorflow implementation</a></span></li><li><span><a href=\"#Neural-network-implementation-in-Python\" data-toc-modified-id=\"Neural-network-implementation-in-Python-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Neural network implementation in Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#Forward-prop-in-a-single-layer\" data-toc-modified-id=\"Forward-prop-in-a-single-layer-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Forward prop in a single layer</a></span></li><li><span><a href=\"#General-implementation-of-forward-propagation\" data-toc-modified-id=\"General-implementation-of-forward-propagation-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>General implementation of forward propagation</a></span></li><li><span><a href=\"#CofeeRoastingNumPy\" data-toc-modified-id=\"CofeeRoastingNumPy-1.7.3\"><span class=\"toc-item-num\">1.7.3&nbsp;&nbsp;</span>CofeeRoastingNumPy</a></span></li></ul></li><li><span><a href=\"#Practice-quiz:-Neural-network-implementation-in-Python\" data-toc-modified-id=\"Practice-quiz:-Neural-network-implementation-in-Python-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Practice quiz: Neural network implementation in Python</a></span></li><li><span><a href=\"#Speculation-on-artificial-general-intelligence-(AGI)\" data-toc-modified-id=\"Speculation-on-artificial-general-intelligence-(AGI)-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Speculation on artificial general intelligence (AGI)</a></span></li><li><span><a href=\"#Vectorization-(optional)\" data-toc-modified-id=\"Vectorization-(optional)-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Vectorization (optional)</a></span><ul class=\"toc-item\"><li><span><a href=\"#How-neural-networks-are-implemented-efficiently\" data-toc-modified-id=\"How-neural-networks-are-implemented-efficiently-1.10.1\"><span class=\"toc-item-num\">1.10.1&nbsp;&nbsp;</span>How neural networks are implemented efficiently</a></span></li><li><span><a href=\"#Matrix-multiplication\" data-toc-modified-id=\"Matrix-multiplication-1.10.2\"><span class=\"toc-item-num\">1.10.2&nbsp;&nbsp;</span>Matrix multiplication</a></span></li><li><span><a href=\"#Matrix-multiplication-rules\" data-toc-modified-id=\"Matrix-multiplication-rules-1.10.3\"><span class=\"toc-item-num\">1.10.3&nbsp;&nbsp;</span>Matrix multiplication rules</a></span></li></ul></li><li><span><a href=\"#Practice-Lab:-Neural-networks\" data-toc-modified-id=\"Practice-Lab:-Neural-networks-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Practice Lab: Neural networks</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb408ed",
   "metadata": {},
   "source": [
    "# Course 2 Week 1: Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50619623",
   "metadata": {},
   "source": [
    "## Neural networks intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d87038",
   "metadata": {},
   "source": [
    "### Welcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff096b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T23:30:03.781542Z",
     "start_time": "2023-04-14T23:30:03.752310Z"
    }
   },
   "source": [
    "Plan for this 4 weeks course\n",
    "- Wk 1: NN Inference (Prediction)\n",
    "- Wk 2: NN Training\n",
    "- Wk 3: Practical advice for building machine laerning systems\n",
    "- Wk 4: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b9447",
   "metadata": {},
   "source": [
    "### Neurons and the brain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f748665",
   "metadata": {},
   "source": [
    "> NN Origins: Algorithms/Software that try to mimic the human biological brain\n",
    "\n",
    "- Work started way back in 1950s, and fell out of favor for a while.\n",
    "- NNs were used again in 1980's and early 1990's and gained traction in applications like handwritten digit recognition.\n",
    "- However, fell out of favor in the late 1990's.\n",
    "- NNs Resurgence: From around 2005, and became rebranded little bit with **deep learning**\n",
    "- Since them, NNs have revolutionized one application area after another\n",
    "    - speech recognition $\\rightarrow$ compute vision $\\rightarrow$ NLP $\\rightarrow$ and so on...\n",
    "    \n",
    "> <span style=\"color:red\">Though today's NN have almost nothing to do with how the brain learns, there was the early motivation to trying to build algorithms to mimic the brain</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261dec3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T00:03:46.944621Z",
     "start_time": "2023-04-15T00:03:46.939215Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neurons_in_the_brain.png\" alt=\"Drawing\" style=\"width: 35%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9113cfae",
   "metadata": {},
   "source": [
    "All of human thought is from neurons in our brain, sending electrical pulses and forming new connections of other neurons. \n",
    "\n",
    "As shown in figure above,\n",
    "- <span style=\"color:blue\"><b>a neuron </b></span>(top right)\n",
    "    - **Dendrites** (input wires): has **number of inputs where it receives electrical impulses from many other neurons**\n",
    "    - aggregates inputs **carries out some computations**\n",
    "    - **Axons** (output wires): and **send this outputs to other neurons** by electrical impulses\n",
    "        - the ouput of this neuron becomes the input of the neuron (on bottom left), which again aggregates inputs from multiple othr neurons to then may be send its own output to yet other neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a201a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T00:13:35.824550Z",
     "start_time": "2023-04-15T00:13:35.813819Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neurons_in_artificial_neural_networks_and_the_brain.png\" alt=\"Drawing\" style=\"width: 75%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67099356",
   "metadata": {},
   "source": [
    "The artifical neural network (ANN) uses a very simplified math model of what biological neuron does.\n",
    "\n",
    "Let's look at a **simplified math model of few neurons**.\n",
    "What does these neurons do?\n",
    "- Takes one or more inputs (which are just some numbers)\n",
    "- Does some computation\n",
    "- Outputs a number\n",
    "    - Which then could be an input to another neuron\n",
    "    \n",
    "Rather than building one neuron at a time, we often want to simulate many such neurons at the same time $\\Rightarrow$ We have more neurons, which collectively input a few numbers, carry out some computation, and output some other numbers.\n",
    "\n",
    "> <span style=\"color:red\">Caveat: Till date, we have almost no idea how the human brain works.</span>\n",
    "\n",
    ">  <span style=\"color:blue\">Though the NN origins were biologically motivates, the researchers are now just using engineering principles to figure out how to build algorithms that are more effective.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8037a30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T00:27:32.293757Z",
     "start_time": "2023-04-15T00:27:32.281813Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_networks_why_now.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8759b27",
   "metadata": {},
   "source": [
    "**Why now than ever before?**\n",
    "1. With rise of interent, mobile phones and digitalization of the society, the amout of data we have for lot of applications has exploded.\n",
    "2. Performance of AI and NN with increased amount of data\n",
    "    - With traditional ML algorithms (like linear and logistic regression), they were not able to scale with anount of data \n",
    "$\\Rightarrow$ They were not able to take effective advantage of all of this data we had for different applications\n",
    "    - With small NN, the performance become slightly better than traditional ML algos\n",
    "    - With medium NN (i.e., more neurons), performance improved further\n",
    "    - With large NN, for some applications, the performance will keep on going up\n",
    "        - <span style=\"color:green\"><b>With large NN trained on big data, we could attain superior performance</b></span><span style=\"color:red\"> which were not possible with earlier generations of learning algorithms</span>. This caused deep learning algos to take off along with rise of faster computer processors (like GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df8468",
   "metadata": {},
   "source": [
    "### Demand Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd16fcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T01:03:21.788720Z",
     "start_time": "2023-04-15T01:03:21.777372Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/demand_prediction_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5cb123",
   "metadata": {},
   "source": [
    "**To illustrate how NNs work, let's use a simple example: Demand prediction (predict whether the product will be a top seller or not)**.\n",
    "\n",
    "- Input feature, $x$: Price of the T-shirt\n",
    "- Output, $y$: Top seller or not\n",
    "    - Using logistic regression, $\\large\\hat{y} = f(x) = \\frac{1}{1+e^{-(wx+b)}}$ \n",
    "        - We are going to call the output of logistic regression as $a$ instead of $\\hat{y}$ or $f(x)$ moving forward to set us up to build a NN.\n",
    "        - **$a$ stands for activation, a term from neuroscience that refers to how much a neuron is sending a high output (electrical impulses) to other neurons downstream from it**.\n",
    "        \n",
    "    - <span><b>The above logsitic regression can be thought of as a <u>single nueron model</u></b></span>, where\n",
    "        - takes the input $x$ (e.g., price), \n",
    "        - and the neuron computes the formula (e.g., sigmoid function) and \n",
    "        - outputs the value $a$ (e.g., probability of T-shirt being a top seller)\n",
    "        \n",
    "<span style=\"color:blue\"><b>Now that we described a single neuron, building a NN now just requires taking a bunch of these nurons and wiring them together</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb62d4",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/demand_prediction_p2_nn_intuition.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546768df",
   "metadata": {},
   "source": [
    "**Let's now use a complex example: Demand prediction (predict whether the product will be a top seller or not)**.\n",
    "\n",
    "- Input features $x$ (**input layer** in the context of NN)\n",
    "    - price\n",
    "    - shipping cost\n",
    "    - marketing\n",
    "    - material\n",
    "- Manual feature engineering (intermediate step): \n",
    "    - We might suspect the probability of T-shirt being a top seller depends on 3 factors\n",
    "        - Affordability \n",
    "        - Awareness \n",
    "        - Perceived quality\n",
    "    - Let's say we create one artificial neuron to estimate the probability of each of the above 3 factors $\\Rightarrow$ We will have 3 neurons, which we will group together and call as **layer** (to be specific, **hidden layers** in context of NN). In the context of NN, each of these 3 estimations are called as **activations** of the hidden layer\n",
    "        - Affordability = f(price, shipping_cost)\n",
    "        - Awareness = f(marketing)\n",
    "        - Perceived quality = f(price, material)\n",
    "- Output: Probability of being a Top seller\n",
    "    - Now that we have an probability estimate of 3 intermediate factors (affordability, awareness and perceived quality), we can wire the outputs of these 3 neurons to another neuron on the right (i.e., another logistic regression unit).\n",
    "    - This single nueron unit (a.k.a **output layer** in the context of NN) will then finally estimate the probability of t-shirt being a top-seller, which is the **final activation (or final output)** of the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e69ed0",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">In the above example, we had to go through the neurons one at a time and decice what inputs it would take from previous layer (e.g., affordability as f(price, shipping_costs), awareness as f(marketing), etc.. However, if we are building a large NN, it would be impractical to go through and manually decide which neurons should take which features as inputs.</span><span style=\"color:green\">So, <b>the way a NN is implemented in practice is each neuron will have access to every features/value from previous layer.</b></span> \n",
    "\n",
    "e.g., We can imagine that if we're trying to predict affordability and we know what's the price, shipping cost, marketing and material, like us, may be the NN will learn to ignore marketing and material and just figure out through setting the parameters appropriately to only focus on the subset of features (price and shipping cost) that are most relevant to affordability. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0531bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T01:31:24.680838Z",
     "start_time": "2023-04-15T01:31:24.660307Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/demand_prediction_p3_with_vector_notations.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613ebf1",
   "metadata": {},
   "source": [
    "**Let's look at another intuition/way for NNs**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1100027",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/demand_prediction_p4_further_nn_intuition.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7dcd4e",
   "metadata": {},
   "source": [
    "If we hide the input layer, what we essentially have is a logistic regression algo with\n",
    "- inputs: affordability, awareness, perceived quality\n",
    "- output: probability of t-shirt being a top seller\n",
    "\n",
    "However, the cool thing is rather than using original 4 input features, the network is using a new and maybe better set of 3 features, that are hopefully more predictive whether or not the t-shirt will be a top seller.\n",
    "\n",
    "<span style=\"color:green\"><b>One way to think of this NN is just a verion of logistic regresion that can learn its own features in the hidden layers that makes it easier to make accurate predictions.</b></span>\n",
    "\n",
    "Recall, that in C1, we manually feature engineered complex features from length and width of the lot. Instead of hand engineering features, with NN, it learns its own features thereby making learning problem more easier for us, thus making the NN very powerful learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f23017",
   "metadata": {},
   "source": [
    "**Layer**\n",
    "- Grouping of neurons that same or similar features as input, and in turn outputs a few number together\n",
    "- A layer can have single nueron or multiple neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d565ab20",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/multiple_hidden_layers_example.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44364cea",
   "metadata": {},
   "source": [
    "Above, we have examples of NN with multiple hidden layers. As a DS/MLE, we need to choose the right NN architecture (no. of hidden layers, and no. of units in each hidden layer). The above mentioned NNs can sometimes be called as Multi Layer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f44a32",
   "metadata": {},
   "source": [
    "### Example: Recognizing images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1909e1",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/face_recognition_p1.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea6628",
   "metadata": {},
   "source": [
    "The 1000x1000 pixel brightness values $(0-255)$ can be transformed to 1D input feature vector, $\\vec{x}$ of size 1 million (1000 rows x 1000 cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58bc077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T02:50:11.814116Z",
     "start_time": "2023-04-15T02:50:11.803376Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/face_recognition_p2.png\" alt=\"Drawing\" style=\"width: 75%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23980d3",
   "metadata": {},
   "source": [
    "Each of the square nox above is a visulization of what the nueron is trying to detect\n",
    "\n",
    "One interesting thing would be to visualize what are these neurons in each of the hidden layers trying to compute (after trained with tons of face images). It turns out, in the\n",
    "-  First hidden layer\n",
    "    - The first neuron is looking for very short edges/lines in the image\n",
    "    - The second neuron is looking for a line or oriented edge in the image\n",
    "    - The third neuron is looking for very short edges/lines in a slightly different orientation\n",
    "    - ...\n",
    "- Second hidden layer\n",
    "   - The first neuron looks like trying to detect the presence or absence of an eye in a certain position of the image\n",
    "   - The second neuron looks like trying to detect like a corner of a nose\n",
    "   - The neuron in row 4, col 5 looks like trying to detect bottom of an ear\n",
    "- Third hidden layer\n",
    "    - ...\n",
    "    \n",
    "In summary \n",
    "-  **First hidden layer**\n",
    "    - We might find that neurons are looking for very short lines/**edges** in the image\n",
    "- **Second hidden layer** \n",
    "    - Neurons might learn to group together lots of little short lines/edge segments to look for **parts of faces**   \n",
    "- **Third hidden layer**\n",
    "    - Aggregates different parts of faces and tries to detect presense or absence of large, corser **complete face shapes**.\n",
    "-  **Final layer**\n",
    "    - Detecting how much the face corresponds to different face shapes creates a rich set of features that then helps the output layer try to determine the identity of the person in the picture.\n",
    "    \n",
    "*Note: In this visualization, the neurons in the first hidden layer are shown looking at relatively small windows to look for these edges. In the second hidden layer is looking at bigger window, and the third hidden layer is looking at even bigger window. These little neurons visualizations actually correspond to differently sized regions in the image.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70169920",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/face_recognition_p3_cars_example.png\" alt=\"Drawing\" style=\"width: 75%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c591b47",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>The same learning algorithm now is asked to detect cars, will then</b></span>\n",
    "- <span style=\"color:blue\"><b>learn edges in the first layer</b></span>\n",
    "- <span style=\"color:blue\"><b>learn to detect parts of cars in the second hidden layer and </b></span>\n",
    "- <span style=\"color:blue\"><b>more complete car shapes in the third hidden layer</b></span>\n",
    "\n",
    "Just by feeding it different data, the neural network automatically learns to detect very different features so as to try to make the predictions of car detection or person recognition or whether there's a particular given task that is trained on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38ab1d",
   "metadata": {},
   "source": [
    "## Practice quiz: Neural networks intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06eff5",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde8822",
   "metadata": {},
   "source": [
    "### Neural network layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7993db23",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_layer_p1.png\" alt=\"Drawing\" style=\"width: 25%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f21530",
   "metadata": {},
   "source": [
    "**NN architecture e.g., with 2 layers**\n",
    "- Input layer $0$: $\\vec{x}$ has 4 features: price, shipping cost, marketing, material\n",
    "- Hidden layer $1$: 3 neurons\n",
    "- Output layer $2$: 1 neuron (Predicts the prob of T-shirt being a top seller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca846e",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_layer_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30169eca",
   "metadata": {},
   "source": [
    "**Layer 1: Hidden layer**\n",
    "\n",
    "This input feature $\\vec{x}$ of 4 numbers is input to each of the 3 neurons. **Each of the neuron is implementing a simple logistic regression function**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ac325",
   "metadata": {},
   "source": [
    "Output of layer 1, $\\vec{a}^{[1]} = \n",
    "\\begin{bmatrix}\n",
    "a_1^{[1]} \\\\\n",
    "a_2^{[1]} \\\\\n",
    "a_3^{[1]}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "g(\\vec{w_1}^{[1]} \\cdot \\vec{x} + b_1^{[1]}) \\\\ \n",
    "g(\\vec{w_2}^{[1]} \\cdot \\vec{x} + b_2^{[1]}) \\\\ \n",
    "g(\\vec{w_3}^{[1]} \\cdot \\vec{x} + b_3^{[1]})\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "0.3 \\\\ \n",
    "0.7 \\\\ \n",
    "0.2\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "where \n",
    "- $\\large g(z) = \\frac{1}{1+e^{-(z)}}$ is a sigmoid function\n",
    "- $a$ represents activation value\n",
    "    - $\\vec{a}^{[1]}$ refers to activation value of $1^{\\text{st}}$ hidden layer\n",
    "        - $a_1^{[1]}$ represents a $1^{\\text{st}}$ activation value in $1^{\\text{st}}$ layer\n",
    "        - $a_2^{[1]}$ represents a $2^{\\text{nd}}$ activation value in $1^{\\text{st}}$ layer \n",
    "- $w$ and $b$ refers to parameters\n",
    "    - $\\vec{w}^{[1]}$ refers to parameters of 1st hidden layer\n",
    "        - $w_1^{[1]}$  refers to parmeters of the $1^{\\text{st}}$ unit or neuron in $1^{\\text{st}}$ hidden layer\n",
    "        - $w_2^{[1]}$  refers to parmeters of the $2^{\\text{nd}}$  unit or neuron in $1^{\\text{st}}$ hidden layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff529e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T22:28:13.525052Z",
     "start_time": "2023-04-15T22:28:13.502111Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_layer_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a7fcf",
   "metadata": {},
   "source": [
    "**Layer 2: Output layer**\n",
    "\n",
    "Input to layer 2 is output of layer 1 $\\vec{a}^{[1]}$\n",
    "\n",
    "Output of layer 2, $\\vec{a}^{[2]} = \n",
    "\\begin{bmatrix}\n",
    "a_1^{[2]} \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "g(\\vec{w_1}^{[2]} \\cdot \\vec{a}^{[1]} + b_1^{[2]}) \n",
    "\\end{bmatrix} = 0.84\n",
    "$\n",
    "\n",
    "Since the output layer is single nueron, the output $\\vec{a}^{[2]} = a_1^{[2]}$ is rather a scalar than a vector of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7ecc0",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/neural_network_layer_p4.png\" alt=\"Drawing\" style=\"width: 50%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52bc50d",
   "metadata": {},
   "source": [
    "**Optional final step**:\n",
    "- If we want a binary prediction 1 or 0 (top seller or not), we can pass set a threshold 0f $0.5$ for $a^{[2]}$ value (similar to thresholding a logistic regression output)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fa1548",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\"><b>In a nutshell, every layer in NN inputs a vector of numbers and applies a bunch of logistic regression units to it, and then computes another vector of numbers that then gets passed from layer to layer until we get to the final output layers computation, which is the final prediction of the NN.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4b970",
   "metadata": {},
   "source": [
    "### More complex neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cbf063",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/more_complex_neural_nework_p1.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61170515",
   "metadata": {},
   "source": [
    "**NN architecture e.g., 4 layers**\n",
    "- Layer 0: Input layer\n",
    "- Layer 1,2,3: Hidden layers\n",
    "- Layer 4: Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c9b3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T23:19:02.464246Z",
     "start_time": "2023-04-15T23:19:02.440334Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/more_complex_neural_nework_p2.png\" alt=\"Drawing\" style=\"width: 75%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc6e29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-15T23:13:59.711585Z",
     "start_time": "2023-04-15T23:13:59.693200Z"
    }
   },
   "source": [
    "**Layer 3: Hidden layer**\n",
    "    \n",
    "Output of layer 3, $\\vec{a}^{[3]} = \n",
    "\\begin{bmatrix}\n",
    "a_1^{[3]} \\\\\n",
    "a_2^{[3]} \\\\\n",
    "a_3^{[3]}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "g(\\vec{w_1}^{[3]} \\cdot \\vec{a}^{[2]} + b_1^{[3]}) \\\\ \n",
    "g(\\vec{w_2}^{[3]} \\cdot \\vec{a}^{[2]} + b_2^{[3]}) \\\\ \n",
    "g(\\vec{w_3}^{[3]} \\cdot \\vec{a}^{[2]} + b_3^{[3]})\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c79bef",
   "metadata": {},
   "source": [
    "**More generally**,\n",
    "\n",
    "$$\\large a_j^{[l]} = g(\\vec{w_j}^{[l]} \\cdot \\vec{a}^{[l-1]} + b^{[l]})$$\n",
    "\n",
    "where\n",
    "- $a_j^{[l]}$ - activation value of layer $l$, unit (neuron) $j$\n",
    "- $a^{[l-1]}$ - activation value or output of layer $l-1$ (a.k.a. previous layer)\n",
    "- $\\vec{w_j}^{[l]}$ and $b^{[l]}$ are parameters of layer $l$, unit (neuron) $j$\n",
    "- $g$ is sigmoid/activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7629da08",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/multiple_hidden_layers_example_p2_invideo_quiz.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35cfb2",
   "metadata": {},
   "source": [
    "### Inference: making predictions (forward propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d56554",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/inference_making_predictions_FP_handwritten_digit_rec_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a871d16",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/inference_making_predictions_FP_handwritten_digit_rec_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e552fd48",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/inference_making_predictions_FP_handwritten_digit_rec_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d580ec",
   "metadata": {},
   "source": [
    "**NN Forward propagation e.g., using handwritten digit recognition**\n",
    "\n",
    "For simplicity, let's assume that this is a binary classification (predict the digits as $0$ or $1$)\n",
    "\n",
    "- Sequence of computation for inference (a.k.a. **forward propagation**)\n",
    "    - $\\vec{x} \\rightarrow \\vec{a}^{[1]} \\rightarrow \\vec{a}^{[2]} \\rightarrow \\vec{a}^{[3]} = f(x)$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d7a3a1",
   "metadata": {},
   "source": [
    "**Layer 0: Input layer** (64 features)\n",
    "- Input: 8x8 pixel image $\\Rightarrow$ 64 pixels (input features)\n",
    "    - Pixel values: 0 (dark/black) to 255 (bright/white)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d19c95a",
   "metadata": {},
   "source": [
    "**Layer 1: Hidden layer** ($\\color{Red}{25}$ units)\n",
    "\n",
    "$$\\vec{a}^{[1]} = \\begin{bmatrix}\n",
    "g(\\vec{w_{\\color{Red}1}}^{[1]} \\cdot \\vec{x} + b_{\\color{Red}1}^{[1]})\\\\\n",
    " \\vdots \\\\\n",
    "g(\\vec{w_{\\color{Red}{25}}}^{[1]} \\cdot \\vec{x} + b_{\\color{Red}{25}}^{[1]})\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae44c72",
   "metadata": {},
   "source": [
    "**Layer 2: Hidden layer** ($\\color{Red}{15}$ units)\n",
    "\n",
    "$$\\vec{a}^{[2]} = \\begin{bmatrix}\n",
    "g(\\vec{w_{\\color{Red}1}}^{[2]} \\cdot \\vec{a}^{[1]} + b_{\\color{Red}1}^{[2]})\\\\\n",
    " \\vdots \\\\\n",
    "g(\\vec{w_{\\color{Red}{15}}}^{[2]} \\cdot \\vec{a}^{[1]} + b_{\\color{Red}{15}}^{[2]})\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561f1a6",
   "metadata": {},
   "source": [
    "**Layer 3: Output layer** ($\\color{Red}1$ unit)\n",
    "\n",
    "$$\\vec{a}^{[3]} = \\begin{bmatrix}\n",
    "g(\\vec{w_{\\color{Red}1}}^{[3]} \\cdot \\vec{a}^{[2]} + b_{\\color{Red}1}^{[3]})\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb20b4",
   "metadata": {},
   "source": [
    "Note, $\\vec{x}$ can also be mentioned as $\\vec{a}^{[0]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d629e16",
   "metadata": {},
   "source": [
    "### Lab: Neurons and Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14e78db",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/optional-labs/C2_W1_Lab01_Neurons_and_Layers.ipynb) solutions and [my practice](greyhatguy007/optional-labs/C2_W1_Lab01_Neurons_and_Layers_Practice_TP.ipynb) for now. I also have another stripped down version of [my practice_v2](greyhatguy007/optional-labs/C2_W1_Lab01_Neurons_and_Layers_Practice_TP_v2.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188d732",
   "metadata": {},
   "source": [
    "## Practice quiz: Neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e440c23",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Practice%20quiz-%20Neural%20network%20model) screenshots for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128c4f0",
   "metadata": {},
   "source": [
    "## Tensorflow implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ddd987",
   "metadata": {},
   "source": [
    "### Inference in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274d38c3",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"attachments/inference_in_code_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74094341",
   "metadata": {},
   "source": [
    "**Let's look at coffee roasting problem as an example**\n",
    "\n",
    "There are 4 possible scenarios\n",
    "1. [Bad] If the temperature is too low, the cofeee beans will be undercooked\n",
    "2. [Bad] If the duration is too short, the coffee beans will be undercooked\n",
    "3. [Bad] If the temperature is too high and duration is too long, the cofee beans will be overcooked\n",
    "4. [Good] If the temperature and duration is just right, coffee beans will be perfectly roasted (highlighted triangle region in yellow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd9fe3",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"attachments/inference_in_code_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad6936c",
   "metadata": {},
   "source": [
    "The task now is given a feature vector x with both temperature and duration, say 200 degrees Celsius for 17 minutes, how can we do inference in a neural network to get it to tell us whether or not this temperature and duration setting will result in good coffee or not? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feff79cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T00:50:31.267340Z",
     "start_time": "2023-04-18T00:50:31.238938Z"
    }
   },
   "source": [
    "Model building using Tensorflow\n",
    "- **Layer 0 - Input layer**: Input vector with 2 features\n",
    "- **Layer 1 - Hidden layer**: Say, dense layer of 3 units with sigmoid activation function | `a1`\n",
    "- **Layer 2 - Hidden layer**: Say, dense layer of 1 unit with sigmoid activaiton function | `a2`\n",
    "    - Then, we can treshold the layer 2 output as `a2>=0.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a6e9b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/inference_in_code_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac285b",
   "metadata": {},
   "source": [
    "**Let's look at another example of digit classification**\n",
    "\n",
    "Model building using Tensorflow\n",
    "- **Layer 0 - Input layer**: Input vector/ list of pixel intensity values\n",
    "- **Layer 1 - Hidden layer**: Dense layer with 25 units and sigmoid activation function | `a1`\n",
    "- **Layer 2 - Hidden layer**: Dense layer with 15 units and sigmoid activation function | `a2`\n",
    "- **Layer 3 - Hidden layer**: Dense layer with 1 unit and sigmoid activation function | `a3`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7acc8d",
   "metadata": {},
   "source": [
    "### Data in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865edb64",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/data_in_tensorflow_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d364edd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T01:00:33.233957Z",
     "start_time": "2023-04-18T01:00:33.225167Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/data_in_tensorflow_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc59b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T01:00:33.233957Z",
     "start_time": "2023-04-18T01:00:33.225167Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/data_in_tensorflow_p3.png\" alt=\"Drawing\" style=\"width: 55%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "639fa2f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T01:00:33.233957Z",
     "start_time": "2023-04-18T01:00:33.225167Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5af5bf15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T01:04:00.507811Z",
     "start_time": "2023-04-18T01:04:00.487619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(4, 2)\n",
      "(1, 2)\n",
      "(2, 1)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3], \n",
    "              [4,5,6]])\n",
    "print (x.shape)\n",
    "\n",
    "x = np.array([[0.1, 0.2],\n",
    "              [-3, -4], \n",
    "              [-.5, -.6], \n",
    "              [7, 8]])\n",
    "print (x.shape)\n",
    "\n",
    "x = np.array([[200.0, 17.0]]) # 2D array \n",
    "print (x.shape)\n",
    "\n",
    "x = np.array([[200.0], \n",
    "              [17.0]]) # 2D array\n",
    "print (x.shape)\n",
    "\n",
    "x = np.array([200.0, 17.0]) # 1D vector (no rows or cols)\n",
    "print (x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88168bc5",
   "metadata": {},
   "source": [
    "**With Tensorflow, the convention is to use 2D matrices to represent the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb40d59",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/data_in_tensorflow_p4.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "    <img src=\"attachments/data_in_tensorflow_p5.png\" alt=\"Drawing\" style=\"width: 40%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2b1e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-18T01:06:42.397575Z",
     "start_time": "2023-04-18T01:06:42.380538Z"
    }
   },
   "source": [
    "- When we compute `a1 = layer_1(x)`, the shape of `Tensor` a1 is going to be `1x3` matrix.\n",
    "- When we compute `a2 = layer_2(a1)`, the shape of `Tensor` a2 is going to be `1x1` matrix.\n",
    "    - To convert the `Tensor` to `np.array`, just call `a2.numpy()`\n",
    "\n",
    "> Whenever we see a `Tensor`, think of it as a way of representing matrices (Oversimplied generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7c4b3",
   "metadata": {},
   "source": [
    "### Building a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c4703",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/building_a_neural_network_p1.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "    <img src=\"attachments/building_a_neural_network_p2.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ecacc1",
   "metadata": {},
   "source": [
    "**Few variants of building a NN model in Tensorflow**\n",
    "\n",
    "- **Variant 1**\n",
    "```\n",
    "x = np.array([[200,0, 17.0]])\n",
    "layer_1 = Dense(units=3, activation=\"sigmoid\")\n",
    "a1 = layer_1(x)\n",
    "layer_2 = Dense(units=1, activation=\"sigmoid\")\n",
    "a2 = layer_2(a1)\n",
    "```\n",
    "\n",
    "    Instead of manually passing input `x` to `layer_1`, and activation `a1` to `layer_2`, we can automate it using `Sequential(...)` framework.\n",
    "\n",
    "\n",
    "- **Variant 2**\n",
    "```\n",
    "layer_1 = Dense(units=3, activation=\"sigmoid\")\n",
    "layer_2 = Dense(units=1, activation=\"sigmoid\")\n",
    "model = Sequential([layer_1, layer_2]) \n",
    "```\n",
    "\n",
    "    Instead of defining each `Dense` layer, we can define it straight away in `Sequential` framework.\n",
    "    \n",
    "\n",
    "- **Variant 3**\n",
    "```\n",
    "model = Sequential([\n",
    "    Dense(units=3, activation=\"sigmoid\"), \n",
    "    Dense(units=1, activation=\"sigmoid\")\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050899a3",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/building_a_neural_network_p3.png\" alt=\"Drawing\" style=\"width: 65%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18261af9",
   "metadata": {},
   "source": [
    "### Lab: Cofee Roasting in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7db26",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/optional-labs/C2_W1_Lab02_CoffeeRoasting_TF.ipynb) solutions and [my practice](greyhatguy007/optional-labs/C2_W1_Lab02_CoffeeRoasting_TF_Practice_TP.ipynb) for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9115cf",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Note</b><span>: \n",
    "- The weights $W$ should be of size `(number of features in input, number of units in the layer)` while \n",
    "- the bias $b$ size should match the `number of units in the layer`. \n",
    "- Collectively, the `parameter count of the layer $L$` correspond to the `number of elements in the weight $W$ array` and `bias $b$ array`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b11e994",
   "metadata": {},
   "source": [
    "**Important sections of the lab**\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"attachments/lab02_coffee_roasting_p1.png\" alt=\"Drawing\" style=\"width: 45%;\">\n",
    "</div>\n",
    "\n",
    "**Input and Output**\n",
    "- Input: 2 features: \n",
    "    - 1) Temperature (in C), \n",
    "    - 2) Duration (in mins)\n",
    "- Output: Good/Bad roast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877595f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T00:46:40.868700Z",
     "start_time": "2023-04-24T00:46:40.858455Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/lab02_coffee_roasting_p2.png\" alt=\"Model\" style=\"width: 25%;\">\n",
    "</div>\n",
    "\n",
    "**Model**\n",
    "- Layer 1: 3 neurons\n",
    "- Layer 2: 1 neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696383d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T00:48:40.936790Z",
     "start_time": "2023-04-24T00:48:40.922537Z"
    }
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/lab02_coffee_roasting_p3.png\" alt=\"Model\" style=\"width: 75%;\">\n",
    "</div>\n",
    "\n",
    "**Model Params**\n",
    "- Layer L: `n input features + n neurons/units in layer + n neurons` = `Weight array + bias array`\n",
    "- Layer 1: `2*3 + 3 = 9` \n",
    "- Layer 2: `3*1 + 1 = 3`\n",
    "- Total params: `9 + 3 = 13`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec68fd39",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/lab02_coffee_roasting_p4.png\" alt=\"Model\" style=\"width: 85%;\">\n",
    "</div>\n",
    "\n",
    "**Visualizing the output of the 3 neurons or units in Layer 1**\n",
    "- From the above plot, we can infer that when we overlap these three output units, the area where the output values of each neurons/units are lower will form a triangle region of interest that correspond to good roast areas. $\\Rightarrow$ **Low output value of neurons/units indicates good roast**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616b897c",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/lab02_coffee_roasting_p5.png\" alt=\"Model\" style=\"width: 85%;\">\n",
    "</div>\n",
    "\n",
    "**Visualizing the output of the single neuron or unit in Layer 2**\n",
    "- From the above plot, we can infer that in the region where the output of 3 neurons/units in layer 1 are lower, the output value of layer 2 will be the highest $\\Rightarrow$ Good roast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795286ea",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/lab02_coffee_roasting_p6.png\" alt=\"Model\" style=\"width: 85%;\">\n",
    "</div>\n",
    "\n",
    "**Visualizing the final output of the NN**\n",
    "- Left graph: Raw output of the final layer a.k.a. predicted probability (represented by blue shading) \n",
    "- Right graph: Output of the final layer after a decision threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b3231",
   "metadata": {},
   "source": [
    "## Practice quiz: Tensorflow implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6631c8c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T01:56:40.889778Z",
     "start_time": "2023-04-24T01:56:40.872856Z"
    }
   },
   "source": [
    "Refer to [greyhat](greyhatguy007/Practice%20quiz-%20TensorFlow%20implementation) screenshots for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375bd099",
   "metadata": {},
   "source": [
    "## Neural network implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be6a96",
   "metadata": {},
   "source": [
    "### Forward prop in a single layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a102863",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/forward_prop_in_a_single_layer_1.png\" alt=\"Model\" style=\"width: 65%;\">\n",
    "</div>\n",
    "\n",
    "**Let's see how to implement forward prop manually.**\n",
    "\n",
    "- *Note: For this example, we are going to use 1D arrays to represent the inputs and weights, instead of 2D matrix*\n",
    "- Layer 2 Unit 1 weights, $w_{1}^{[2]}$ is represented as `w2_1` in the code below\n",
    "- Activation of layer 1, $a^{[1]} = \\begin{bmatrix}\n",
    "a_{1}^{[1]}\\\\ \n",
    "a_{2}^{[1]}\\\\ \n",
    "a_{3}^{[1]}\n",
    "\\end{bmatrix}$\n",
    "    - $a_{1}^{[1]} = g(\\vec{w_{1}}^{[1]} \\cdot \\vec{x} + b_{1}^{[1]})$\n",
    "    - $a_{2}^{[1]} = g(\\vec{w_{2}}^{[1]} \\cdot \\vec{x} + b_{2}^{[1]})$\n",
    "    - $a_{3}^{[1]} = g(\\vec{w_{3}}^{[1]} \\cdot \\vec{x} + b_{3}^{[1]})$\n",
    "- Activation of layer 2, $a^{[2]} = \\begin{bmatrix}\n",
    "a_{1}^{[2]}\n",
    "\\end{bmatrix}$\n",
    "    - $a_{1}^{[2]} = g(\\vec{w_{1}}^{[2]} \\cdot \\vec{a}^{[1]} + b_{1}^{[2]})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3a4f9e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T01:14:02.620701Z",
     "start_time": "2023-04-24T01:14:02.603501Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ee55e7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T01:17:35.254351Z",
     "start_time": "2023-04-24T01:17:35.246879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=(2,)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([200, 17]) \n",
    "print (f\"x.shape={x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "136b9145",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T01:17:35.382621Z",
     "start_time": "2023-04-24T01:17:35.369391Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f5f4142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T01:20:10.890306Z",
     "start_time": "2023-04-24T01:20:10.871280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1_1.shape=(2,) | b1_1.shape=(1,)\n",
      "a1_1.shape=(1,)\n",
      "w1_2.shape=(2,) | b1_2.shape=(1,)\n",
      "a1_2.shape=(1,)\n",
      "w1_3.shape=(2,) | b1_3.shape=(1,)\n",
      "a1_3.shape=(1,)\n",
      "a1.shape=(3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Layer 1 (3 neurons: Each neuron will take 2 input feature + a bias term)\n",
    "# Layer 1: Neuron 1\n",
    "w1_1 = np.array([1, 2])\n",
    "b1_1 = np.array([-1])\n",
    "print (f\"w1_1.shape={w1_1.shape} | b1_1.shape={b1_1.shape}\")\n",
    "z1_1 = np.dot(w1_1, x) + b1_1\n",
    "a1_1 = sigmoid(z1_1)\n",
    "print (f\"a1_1.shape={a1_1.shape}\")\n",
    "\n",
    "# Layer 1: Neuron 2\n",
    "w1_2 = np.array([-3, 4])\n",
    "b1_2 = np.array([1])\n",
    "print (f\"w1_2.shape={w1_2.shape} | b1_2.shape={b1_2.shape}\")\n",
    "z1_2 = np.dot(w1_2, x) + b1_2\n",
    "a1_2 = sigmoid(z1_2)\n",
    "print (f\"a1_2.shape={a1_2.shape}\")\n",
    "\n",
    "# Layer 1: Neuron 3\n",
    "w1_3 = np.array([5, -6])\n",
    "b1_3 = np.array([2])\n",
    "print (f\"w1_3.shape={w1_3.shape} | b1_3.shape={b1_3.shape}\")\n",
    "z1_3 = np.dot(w1_3, x) + b1_3\n",
    "a1_3 = sigmoid(z1_3)\n",
    "print (f\"a1_3.shape={a1_3.shape}\")\n",
    "\n",
    "a1 = np.array([a1_1, a1_2, a1_3])\n",
    "print (f\"a1.shape={a1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3805beab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T01:24:11.079650Z",
     "start_time": "2023-04-24T01:24:11.069675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2_1.shape=(3,) | b2_1.shape=(1,)\n",
      "a2.shape=(1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Layer 2 (1 neuron: Will take 3 input feature + a bias term)\n",
    "w2_1 = np.array([-7, 8, 9])\n",
    "b2_1 = np.array([3])\n",
    "print (f\"w2_1.shape={w2_1.shape} | b2_1.shape={b2_1.shape}\")\n",
    "\n",
    "z2_1 = np.dot(w2_1, a1) + b2_1\n",
    "a2_1 = sigmoid(z2_1)\n",
    "\n",
    "a2 = np.array([a2_1])\n",
    "print (f\"a2.shape={a2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b55a6",
   "metadata": {},
   "source": [
    "### General implementation of forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087bc322",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"attachments/general_implementation_of_forward_prop_p1.png\" alt=\"Model\" style=\"width: 65%;\">\n",
    "</div>\n",
    "\n",
    "<span style=\"color:blue\"><b>Note</b><span>: \n",
    "- `W` refers to a matrix (2D), whereas `w` refers to array (1D)\n",
    "- Also, the `w` parameters for each neuron will be placed as a col in `W`.\n",
    "    - **The `w` parameters of neuron 1 will be in col 1 of `W`.**\n",
    "    - For each neuron in a layer, there will be one col in numpy array `W`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "337c51ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T01:35:14.240995Z",
     "start_time": "2023-04-24T01:35:14.228207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.shape=(2, 3) | b.shape=(3,)\n"
     ]
    }
   ],
   "source": [
    "# Layer 1 W and b that has 3 neurons => W.shape = (2 input features, 3 neurons in this layer)\n",
    "W = np.array([\n",
    "    [-1,-3, 5],\n",
    "    [ 3, 4,-6]\n",
    "])\n",
    "b = np.array([-1,1,2])\n",
    "print (f\"W.shape={W.shape} | b.shape={b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16098c02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T01:37:12.898850Z",
     "start_time": "2023-04-24T01:37:12.887877Z"
    }
   },
   "outputs": [],
   "source": [
    "def g(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f20d590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T01:37:32.800168Z",
     "start_time": "2023-04-24T01:37:32.778941Z"
    }
   },
   "outputs": [],
   "source": [
    "def dense(a_in, W, b):\n",
    "    units = W.shape[1] # No. of neurons in a layer\n",
    "    a_out = np.zeros(units)\n",
    "    for j in range(units):\n",
    "        w = W[:,j]\n",
    "        z = np.dot(w, a_in) + b[j]\n",
    "        a_out[j] = g(z)\n",
    "    return a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86d292bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T01:38:38.304995Z",
     "start_time": "2023-04-24T01:38:38.285221Z"
    }
   },
   "outputs": [],
   "source": [
    "def sequential(x):\n",
    "    a1 = dense(x, W1, b1)  # Layer 1 activation\n",
    "    a2 = dense(a1, W2, b2) # Layer 2 activaiton \n",
    "    a3 = dense(a2, W3, b3) # Layer 3 activation\n",
    "    a4 = dense(a3, W4, b4) # Layer 4 activation\n",
    "    f_x = a4\n",
    "    return f_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820fc20",
   "metadata": {},
   "source": [
    "### CofeeRoastingNumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a54da",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/optional-labs/C2_W1_Lab03_CoffeeRoasting_Numpy.ipynb) solutions and [my practice](greyhatguy007/optional-labs/C2_W1_Lab03_CoffeeRoasting_Numpy_Practice_TP.ipynb) for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c46dae",
   "metadata": {},
   "source": [
    "## Practice quiz: Neural network implementation in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2dd6c",
   "metadata": {},
   "source": [
    "Refer to [greyhat](greyhatguy007/Practice-Quiz-Neural-Networks-Implementation-in-python) screenshots for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d6b545",
   "metadata": {},
   "source": [
    "## Speculation on artificial general intelligence (AGI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910d130",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16b2e827",
   "metadata": {},
   "source": [
    "## Vectorization (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf12428",
   "metadata": {},
   "source": [
    "### How neural networks are implemented efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c848fc61",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c730a522",
   "metadata": {},
   "source": [
    "### Matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aaefe8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7be1eca6",
   "metadata": {},
   "source": [
    "### Matrix multiplication rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae29c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e51f0772",
   "metadata": {},
   "source": [
    "## Practice Lab: Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd5e5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8665e214",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "210ad021",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f81bd87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e00669e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e424801b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f60dbfd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ga]",
   "language": "python",
   "name": "conda-env-ga-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
